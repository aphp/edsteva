{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a tutorial for loading your data which is the first step in the [EDS-TeVa usage workflow][1-load-your-data]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 classes are available to facilitate data access:\n",
    "\n",
    "- `HiveData`: Getting data from a Hive cluster, returning `Koalas` DataFrames.\n",
    "- `LocalData`: Getting data from tables saved on disk, returning `Pandas` DataFrames.\n",
    "- `PostgresData`: Getting data from a PostGreSQL DB, returning `Pandas` DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsteva.io import HiveData, LocalData, PostgresData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from Hive: `HiveData`\n",
    "\n",
    "The `HiveData` class expects two parameters:  \n",
    "\n",
    "- A `SparkSession` variable\n",
    "- The name of the Database to connect to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! aphp \"Using **Spark** kernels\"\n",
    "     All kernels designed to use Spark are configured to expose 3 variables at startup:  \n",
    "     \n",
    "     - `spark`, the current SparkSession\n",
    "     - `sc`, the current SparkContext\n",
    "     - `sql`, a function to execute SQL code on the Hive Database.  \n",
    "\n",
    "     In this case you can just provide the `spark` variable to `HiveData` !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, the following snippet allows to create the necessary variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "sql = spark.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `HiveData` provides a convenient interface to OMOP data stored in Hive.  \n",
    "The OMOP tables can be accessed as attribute and they are represented as [Koalas DataFrames](https://koalas.readthedocs.io/en/latest/getting_started/10min.html#10-minutes-to-Koalas).\n",
    "You simply need to mention your Hive database name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HiveData(\n",
    "    spark, \n",
    "    DB_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, only a subset of tables are added as attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['care_site',\n",
       " 'concept',\n",
       " 'condition_occurrence',\n",
       " 'person',\n",
       " 'procedure_occurrence',\n",
       " 'visit_detail',\n",
       " 'visit_occurrence']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.available_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Koalas` DataFrames, like `Spark` DataFrames, rely on a *lazy* execution plan: As long as no data needs to be specifically collected, saved or displayed, no code is executed. It is simply saved for a later execution.  \n",
    "The main interest of Koalas DataFrames is that you can use (most of) the Pandas API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>year_of_birth</th>\n",
       "      <th>month_of_birth</th>\n",
       "      <th>day_of_birth</th>\n",
       "      <th>birth_datetime</th>\n",
       "      <th>death_datetime</th>\n",
       "      <th>gender_source_value</th>\n",
       "      <th>gender_source_concept_id</th>\n",
       "      <th>cdm_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3347087777</td>\n",
       "      <td>1949</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1949-08-02</td>\n",
       "      <td>NaT</td>\n",
       "      <td>f</td>\n",
       "      <td>2008119903</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9818741928</td>\n",
       "      <td>1975</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1975-07-06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>m</td>\n",
       "      <td>2008119900</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3345464435</td>\n",
       "      <td>1990</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1990-09-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>f</td>\n",
       "      <td>2008119903</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3346060919</td>\n",
       "      <td>1964</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1964-05-18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>f</td>\n",
       "      <td>2008119903</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3347197472</td>\n",
       "      <td>1990</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1990-02-02</td>\n",
       "      <td>NaT</td>\n",
       "      <td>m</td>\n",
       "      <td>2008119900</td>\n",
       "      <td>ORBIS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id  year_of_birth  month_of_birth  day_of_birth birth_datetime death_datetime gender_source_value  gender_source_concept_id cdm_source\n",
       "0   3347087777           1949               8             2     1949-08-02            NaT                   f                2008119903      ORBIS\n",
       "1   9818741928           1975               7             6     1975-07-06            NaT                   m                2008119900      ORBIS\n",
       "2   3345464435           1990               9             7     1990-09-07            NaT                   f                2008119903      ORBIS\n",
       "3   3346060919           1964               5            18     1964-05-18            NaT                   f                2008119903      ORBIS\n",
       "4   3347197472           1990               2             2     1990-02-02            NaT                   m                2008119900      ORBIS"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person = data.person\n",
    "person.drop(columns = ['person_id']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person['is_over_50'] = (person['birth_datetime'] >= datetime(1971,1,1))\n",
    "\n",
    "stats = (\n",
    "    person\n",
    "    .groupby('is_over_50')\n",
    "    .person_id\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data has been sufficiently aggregated, it can be converted back to Pandas, e.g. for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_pd = stats.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, if you want to work on the `Spark` DataFrame instead, a similar method is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_spark = person.to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting/Reading a sample to/from disk: `LocalData`\n",
    "\n",
    "Working with Pandas DataFrame is, when possible, more convenient.  \n",
    "You have the possibility to save your database or at least a subset of it.  \n",
    "Doing so allows you to work on it later without having to go through `Spark` again.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! warning \"Careful with cohort size\"\n",
    "      Do not save it if your cohort is **big**: This saves **all** available tables on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let us define a dummy subset of 1000 patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = data.visit_occurrence\n",
    "\n",
    "selected_visits = (\n",
    "    visits\n",
    "    .loc[visits[\"stay_source_value\"] == \"MCO\"]\n",
    ")\n",
    "\n",
    "sample_patients = (\n",
    "    selected_visits[\"person_id\"]\n",
    "    .drop_duplicates()\n",
    "    .head(1000)\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save every table restricted to this small cohort as a `parquet` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder= os.path.abspath(MY_FOLDER_PATH)\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "tables_to_save = [\"person\", \"visit_detail\", \"visit_occurrence\"]\n",
    "\n",
    "data.persist_tables_to_folder(folder, \n",
    "                              tables=tables_to_save,\n",
    "                              person_ids=sample_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you saved some data to disk, a dedicated class can be used to access it:  \n",
    "The class `LocalData` can be used to load OMOP data from a folder containing several parquet files. The tables\n",
    "are accessed as attributes and are returned as Pandas DataFrame.\n",
    "\n",
    "!!! warning\n",
    "    In this case, the whole table will be loaded into memory on a single jupyter server. Consequently it is advised to only use this for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LocalData(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visit_occurrence', 'visit_detail', 'person']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.available_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'pandas.core.frame.DataFrame'>\n",
      "shape: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "person = data.person\n",
    "print(f\"type: {type(person)}\")\n",
    "print(f\"shape: {person.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from PostGres: `PostgresData`\n",
    "\n",
    "OMOP data can be stored in a PostreSQL database. The `PostgresData` class provides a convinient interface to it.\n",
    "\n",
    "!!! note\n",
    "    This class relies on the file `~/.pgpass` that contains your identifiers for several databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12688670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count\n",
       "0  12688670"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PostgresData(dbname=DB, \n",
    "                    schema=\"omop\", \n",
    "                    user=USER)\n",
    "data.read_sql(\"select count(*) from person\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d9fd6df514111f70401340d2a1443f871d1ff64bf1df8d7ac5b8c221fed8d44"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
