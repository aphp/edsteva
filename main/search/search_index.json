{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DISCLAIMER EDS-TeVa is intended to be a module of EDS-Scikit EDS-TeVa Documentation : https://aphp.github.io/edsteva/latest/ Source Code : https://github.com/aphp/edsteva Getting Started EDS-TeVa provides a set of tools to characterize the temporal variability of data induced by the dynamics of the clinical IT system. Context Real world data is subject to important temporal drifts that may be caused by a variety of factors 1 . In particular, data availability fluctuates with the deployment of clinical softwares and their clinical use. The dynamics of software deployment and adoption is not trivial as it depends on the care site and on the category of data that are considered. Installation Requirements EDS-TeVa stands on the shoulders of Spark 2.4 which runs on Java 8 and Python ~3.7.1, it is essential to: Install a version of Python \\(\\geq 3.7.1\\) and \\(< 3.8\\) . Install OpenJDK 8 , an open-source reference implementation of Java 8 wit the following command lines: Linux (Debian, Ubunutu, etc.) Mac Windows $ sudo apt-get update $ sudo apt-get install openjdk-8-jdk ---> 100% For more details, check this installation guide $ brew tap AdoptOpenJDK/openjdk $ brew install --cask adoptopenjdk8 ---> 100% For more details, check this installation guide Follow this installation guide You can install EDS-TeVa through pip : $ pip install edsteva ---> 100% color:green Successfully installed edsteva We recommend pinning the library version in your projects, or use a strict package manager like Poetry . pip install edsteva==0.1.0 Working example: administrative records relative to visits Let's consider a basic category of data: administrative records relative to visits. Visits are characterized by a stay type (full hospitalisation, emergency, consultation, etc.). In this example, the objective is to estimate the availability of visits records with respect to time, care site and stay type. 1. Load your data As detailled in the dedicated section , EDS-TeVa is expecting to work with Pandas or Koalas DataFrames. We provide various connectors to facilitate data fetching, namely a Hive connector, a Postgres connector and a LocalData . Using a Hive DataBase Using a Postgres DataBase Using a Local DataBase from edsteva.io import HiveData db_name = \"my_db\" tables_to_load = [ \"visit_occurrence\" , \"visit_detail\" , \"care_site\" , \"fact_relationship\" , ] data = HiveData ( db_name , tables_to_load = tables_to_load ) data . visit_occurrence # (1) With this connector, visit_occurrence will be a Koalas DataFrame from edsteva.io import PostgresData db_name = \"my_db\" schema = \"my_schema\" user = \"my_username\" data = PostgresData ( db_name , schema = schema , user = user ) # (1) data . visit_occurrence # (2) This connector expects a .pgpass file storing the connection parameters With this connector, visit_occurrence will be a Pandas DataFrame import os from edsteva.io import LocalData folder = os . path . abspath ( MY_FOLDER_PATH ) data = LocalData ( folder ) # (1) data . visit_occurrence # (2) This connector expects a folder with a file per table to load. With this connector, visit_occurrence will be a Pandas DataFrame 2. Choose a Probe or create a new Probe Probe A Probe is a python class designed to compute a completeness predictor \\(c(t)\\) that characterizes data availability of a target variable over time \\(t\\) . In this example, \\(c(t)\\) predicts the availability of administrative records relative to visits. It is defined for each care site and stay type as the number of visits \\(n_{visit}(t)\\) per month \\(t\\) , normalized by the \\(99^{th}\\) percentile of visits \\(n_{99}\\) computed over the entire study period: \\[ c(t) = \\frac{n_{visit}(t)}{n_{99}} \\] If the \\(99^{th}\\) percentile of visits \\(n_{99}\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. The VisitProbe is already available by default in the library: 2.1 Compute your Probe The compute() method takes a Data object as input and stores the computed completeness predictor \\(c(t)\\) in the predictor attribute of a Probe : from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , # (1) }, care_site_levels = [ \"Hospital\" , \"Pole\" , \"UF\" ], # (2) ) visit . save ( path = probe_path ) # (3) visit . predictor . head () 1. The stay_types argument expects a python dictionary with labels as keys and regex as values. 2. The care sites are articulated into levels (cf. AP-HP's reference structure ). 3. Saving the Probe after computation saves you from having to compute it again. You just use VisitProbe.load(path=probe_path) . Saved to /my_path/visit.pkl care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.769 2.2 Filter your Probe In this example, we consider the poles of three hospitals over the period from 2009 to 2021. We consequently filter data before any further analysis. from edsteva.probes import VisitProbe start_date , end_date = ( \"2009-01-01\" , \"2021-01-01\" ) # (1) care_site_short_name = [ \"HOSPITAL 1\" , \"HOSPITAL 2\" , \"HOSPITAL 3\" ] bct_visit = VisitProbe () bct_visit . load () bct_visit . predictor = bct_visit . predictor [ # (2) ( bct_visit . predictor [ \"date\" ] >= start_date ) & ( bct_visit . predictor [ \"date\" ] <= end_date ) ] bct_visit . filter_care_site ( care_site_short_names = care_site_short_name ) # (3) This is the study period considered in the example. bct_visit.predictor is a Pandas.DataFrame , you can use Pandas'API to filter the Probe. To filter care sites there is a dedicated method that also includes all upper and lower levels care sites related to the selected care sites. 2.3 Visualize your Probe Interactive dashboard Interactive dashboards can be used to visualize the average completeness predictor \\(c(t)\\) of the selected care sites and stay types. from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = bct_visit , care_site_level = \"Pole\" , ) Interactive dashboard is available here Static plot If you need a static plot for a report, a paper or anything else, you can use the plot_probe() function. It returns the top plot of the dashboard without the interactive filters. Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_probe plot_path = \"my_path/visit.html\" stay_type = \"All\" plot_probe ( probe = bct_visit , care_site_level = \"Hospital\" , stay_type = stay_type , save_path = plot_path , # (1) ) If a save_path is specified, it'll save your plot in the specified path. { \"schema-url\": \"assets/charts/visit.json\" } 3. Choose a Model or create a new Model Model A Model is a python class designed to fit a function \\(f_\\Theta(t)\\) to each completeness predictor \\(c(t)\\) of a Probe . The fit process estimates the coefficients \\(\\Theta\\) with metrics to characterize the temporal variability of data availability. In this example, the model fits a step function \\(f_{t_0, c_0}(t)\\) to the completeness predictor \\(c(t)\\) with coefficients \\(\\Theta = (t_0, c_0)\\) : \\[ f_{t_0, c_0}(t) = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic value \\(c_0\\) estimates the stabilized routine completeness. It also computes the following \\(error\\) metric that estimates the stability of the data after \\(t_0\\) : \\[ \\begin{aligned} error & = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\\\ \\epsilon(t) & = f_{t_0, c_0}(t) - c(t) \\end{aligned} \\] This step function Model is available in the library. 3.1 Fit your Model The fit method takes a Probe as input, it estimates the coefficients, for example by minimizing a quadratic loss function and computes the metrics. Finally, it stores the estimated coefficients and the computed metrics in the estimates attribute of the Model . from edsteva.models.step_function import StepFunction model_path = \"my_path/fitted_visit.pkl\" step_function_model = StepFunction () step_function_model . fit ( probe = bct_visit ) step_function_model . save ( model_path ) # (1) step_function_model . estimates . head () Saving the Model after fitting saves you from having to fit it again. You just use StepFunction.load(path=model_path) . Saved to /my_path/fitted_visit.pkl care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Urg_Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Urg_Hospit' 2022-02-01 0.652 0.027 3.2 Visualize your fitted Probe Interactive dashboard Interactive dashboards can be used to visualize the average completeness predictor \\(c(t)\\) along with the fitted step function of the selected care sites and stay types. from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = bct_visit , fitted_model = step_function_model , care_site_level = \"Pole\" , ) Interactive dashboard is available here . Static plot If you need a static plot for a report, a paper or anything else, you can use the plot_probe() function. It returns the top plot of the dashboard without the interactive filters. Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_probe plot_path = \"my_path/fitted_visit.html\" stay_type = { \"All\" : \".*\" } plot_probe ( probe = bct_visit , fitted_model = step_function_model , care_site_level = \"Hospital\" , stay_type = stay_type , save_path = plot_path , # (1) ) 1. If a save_path is specified, it'll save your plot in the specified path. { \"schema-url\": \"assets/charts/fitted_visit.json\" } 4. Set the thresholds to fix the deployment bias Now, that we have estimated \\(t_0\\) , \\(c_0\\) and \\(error\\) for each care site and each stay type, one can set a threshold for each estimate in order to select only the care sites where the visits are available over the period of interest. 4.1 Visualize estimates distributions Visualizing the density plots and the medians of the estimates can help you setting the thresholds' values. from edsteva.viz import plot_estimates_densities plot_estimates_densities ( fitted_model = step_function_model , ) { \"schema-url\": \"assets/charts/distributions.json\" } 4.2 Set the thresholds The estimates dashboard provides a representation of the overall deviation from the Model on the top and interactive sliders on the bottom that allows you to vary the thresholds. The idea is to set the thresholds that keep the most care sites while having an acceptable overall deviation. from edsteva.viz.dashboards import estimates_dashboard estimates_dashboard ( probe = bct_visit , fitted_model = step_function_model , care_site_level = \"Pole\" , ) The threshold dashboard is available here . 4.3 Fix the deployment bias Once you set the thresholds, you can extract for each stay type the care sites for which data availability is estimated to be stable over the entire study period. t_0_max = \"2009-01-01\" # (1) c_0_min = 0.63 # (2) error_max = 0.03 # (3) estimates = step_function_model . estimates selected_care_site = estimates [ ( estimates [ \"t_0\" ] <= t_0_max ) & ( estimates [ \"c_0\" ] >= c_0_min ) & ( estimates [ \"error\" ] <= error_max ) ] print ( selected_care_site [ \"care_site_id\" ] . tolist ()) In this example the study period starts on January 1, 2009. The characteristic value \\(c_0\\) estimates the stabilized routine completeness. As we want the selected care sites to have a good completeness after \\(t_0\\) , one can for example set the threshold around the median (cf. distribution ) to keep half of the care sites with the highest completeness after \\(t_0\\) . \\(error\\) estimates the stability of the data after \\(t_0\\) . As we want the selected care sites to be stable after \\(t_0\\) , one can set the threshold around the median (cf. distribution ) to keep half of the care sites with the lowest error after \\(t_0\\) . [8312056386, 8457691845, 8745619784, 8314578956, 8314548764, 8542137845] In this example, \\(c_0\\) and \\(error\\) thresholds have been set around the median (cf. distribution ). However, this method is arbitrary and you have to find the appropriate method for your study with the help of the estimate dashboard . Limitations EDS-TeVa provides modelling tools to characterize the temporal variability of your data, it does not intend to provide direct methods to fix the deployment bias. As an open-source library, EDS-TeVa is also here to host a discussion in order to facilitate collective methodological convergence on flexible solutions. The default methods proposed in this example is intended to be reviewed and challenged by the user community. Make it your own The working example above describes the canonical usage workflow. However, you would probably need different Probes, Models, Visualizations and methods to set the thresholds for your projects. The components already available in the library are listed below but if it doesn't meet your requirements, you are encouraged to create your own. Contribution If you managed to implement your own component, or even if you just thought about a new component do not hesitate to share it with the community by following the contribution guidelines . Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Available components Probe Model Visualization VisitProbe NoteProbe The VisitProbe computes \\(c_{visit}(t)\\) the availability of administrative data related to visits for each care site and each stay type according to time: \\[ c_{visit}(t) = \\frac{n_{visit}(t)}{n_{99}} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{99}\\) is the \\(99^{th}\\) percentile of visits and \\(t\\) is the month. If the \\(99^{th}\\) percentile of visits \\(n_{99}\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import VisitProbe visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, ) visit . predictor . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg' 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Hospit' 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.769 The NoteProbe computes \\(c_{note}(t)\\) the availability of clinical documents linked to patients' visits for each care site, stay type and note type according to time: \\[ c_{note}(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{with\\,doc}\\) the number of visits having at least one document and \\(t\\) is the month. If the number of visits \\(n_{visit}(t)\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import NoteProbe note = Note () note . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, note_types = { \"All\" : \".*\" , \"CRH\" : \"crh\" , \"Ordonnance\" : \"ordo\" , \"CR Passage Urgences\" : \"urge\" , }, ) note . predictor . head () care_site_level care_site_id care_site_short_name stay_type note_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg' 'All' 2019-05-01 233.0 '0.841 Unit\u00e9 Fonctionnelle (UF) 8653815660 Care site 1 'All' 'CRH' 2011-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Hospit' 'CRH' 2021-03-01 204.0 0.497 P\u00f4le/DMU 8312056379 Care site 2 'All' 'Ordonnance' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 'CR Passage Urgences' 2022-02-01 9746.0 0.769 StepFunction RectangleFunction Coefficients Metrics Algos Example The StepFunction fits a step function \\(f_{t_0, c_0}(t)\\) with coefficients \\(\\Theta = (t_0, c_0)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0}(t) & = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\\\ c(t) & = f_{t_0, c_0}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic value \\(c_0\\) estimates the stabilized routine completeness. The default metric computed is the mean squared error after \\(t_0\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\] \\(error\\) estimates the stability of the data after \\(t_0\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algorithm if they don't meet your requirements. Loss minimization Quantile This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) and \\(\\hat{c_0}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{c_0}) & = \\underset{t_0, c_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0}(t)) = |c(t) - f_{t_0, c_0}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented with a dependency relation between \\(c_0\\) and \\(t_0\\) derived from the optimal estimates using the \\(l_2\\) loss function. For more informations, you can have a look on the source code . In this algorithm, \\(\\hat{c_0}\\) is directly estimated as the \\(x^{th}\\) quantile of the completeness predictor \\(c(t)\\) , where \\(x\\) is a number between 0 and 1. Then, \\(\\hat{t_0}\\) is the first time \\(c(t)\\) reaches \\(\\hat{c_0}\\) . \\[ \\begin{aligned} \\hat{c_0} & = x^{th} \\text{ quantile of } c(t) \\\\ \\hat{t_0} & = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) \\end{aligned} \\] Default quantile \\(x\\) The default quantile is \\(x = 0.8\\) . from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 0.027 Coefficients Metrics Algos Example The RectangleFunction fits a step function \\(f_{t_0, c_0, t_1}(t)\\) with coefficients \\(\\Theta = (t_0, c_0, t_1)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0, t_1}(t) & = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) \\\\ c(t) & = f_{t_0, c_0, t_1}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic time \\(t_1\\) estimates the time after which the data is not available anymore. the characteristic value \\(c_0\\) estimates the completeness between \\(t_0\\) and \\(t_1\\) . The default metric computed is the mean squared error between \\(t_0\\) and \\(t_1\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_1} \\epsilon(t)^2}{t_1 - t_0} \\] \\(error\\) estimates the stability of the data between \\(t_0\\) and \\(t_1\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algorithm if they don't meet your requirements. Loss minimization This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) , \\(\\hat{c_0}\\) and \\(\\hat{t_1}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0, t_1)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}, \\hat{c_0}) & = \\underset{t_0, c_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0, t_1)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t)) = |c(t) - f_{t_0, c_0, t_1}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented with a dependency relation between \\(c_0\\) and \\(t_0\\) derived from the optimal estimates using the \\(l_2\\) loss function. For more informations, you can have a look on the source code . from edsteva.models.rectangle_function import RectangleFunction rectangle_function_model = RectangleFunction () rectangle_function_model . fit ( probe ) rectangle_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 t_1 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 2020-05-01 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 2013-04-01 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 2022-03-01 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 2019-08-01 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 2022-08-01 0.027 Dashboard Plot The library provides interactive dashboards that let you set any combination of care sites, stay types and other columns if included in the Probe. You can only export a dashboard in HTML format. predictor_dashboard() estimates_dashboard() The predictor_dashboard() returns: On the top, the aggregated variable is the average completeness predictor \\(c(t)\\) over time \\(t\\) with the prediction \\(\\hat{c}(t)\\) if the fitted Model is specified. On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.). from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The estimates_dashboard() returns a representation of the overall deviation from the Model : On the top, the aggregated variable is a normalized completeness predictor \\(\\frac{c(t)}{c_0}\\) over normalized time \\(t - t_0\\) . On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.) with all the Model coefficients and metrics included in the Model . from edsteva.viz.dashboards import estimates_dashboard threshold_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The library provides static plots that you can export in png or svg. As it is less interactive, you may specify the filters in the inputs of the functions. plot_probe() plot_normalized_probe() plot_estimates_densities() The plot_probe() returns the top plot of the predictor_dashboard() : the normalized completeness predictor \\(\\frac{c(t)}{c_0}\\) over normalized time \\(t - t_0\\) . from edsteva.viz.plots import plot_probe plot_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"assets/charts/fitted_visit.json\" } The plot_normalized_probe() returns the top plot of the estimates_dashboard() . Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_normalized_probe plot_normalized_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"assets/charts/normalized_plot.json\" } The plot_estimates_densities() returns the density plot and the median of each estimate. It can help you to set the thresholds. from edsteva.viz.plots import plot_estimates_densities plot_estimates_densities ( fitted_model = step_function_model , ) { \"schema-url\": \"assets/charts/distributions.json\" } Samuel G Finlayson, Adarsh Subbaswamy, Karandeep Singh, John Bowers, Annabel Kupke, Jonathan Zittrain, Isaac S Kohane, and Suchi Saria. The clinician and dataset shift in artificial intelligence. The New England journal of medicine , 385(3):283, 2021. \u21a9","title":"Home"},{"location":"#getting-started","text":"EDS-TeVa provides a set of tools to characterize the temporal variability of data induced by the dynamics of the clinical IT system.","title":"Getting Started"},{"location":"#context","text":"Real world data is subject to important temporal drifts that may be caused by a variety of factors 1 . In particular, data availability fluctuates with the deployment of clinical softwares and their clinical use. The dynamics of software deployment and adoption is not trivial as it depends on the care site and on the category of data that are considered.","title":"Context"},{"location":"#installation","text":"Requirements EDS-TeVa stands on the shoulders of Spark 2.4 which runs on Java 8 and Python ~3.7.1, it is essential to: Install a version of Python \\(\\geq 3.7.1\\) and \\(< 3.8\\) . Install OpenJDK 8 , an open-source reference implementation of Java 8 wit the following command lines: Linux (Debian, Ubunutu, etc.) Mac Windows $ sudo apt-get update $ sudo apt-get install openjdk-8-jdk ---> 100% For more details, check this installation guide $ brew tap AdoptOpenJDK/openjdk $ brew install --cask adoptopenjdk8 ---> 100% For more details, check this installation guide Follow this installation guide You can install EDS-TeVa through pip : $ pip install edsteva ---> 100% color:green Successfully installed edsteva We recommend pinning the library version in your projects, or use a strict package manager like Poetry . pip install edsteva==0.1.0","title":"Installation"},{"location":"#working-example-administrative-records-relative-to-visits","text":"Let's consider a basic category of data: administrative records relative to visits. Visits are characterized by a stay type (full hospitalisation, emergency, consultation, etc.). In this example, the objective is to estimate the availability of visits records with respect to time, care site and stay type.","title":"Working example: administrative records relative to visits"},{"location":"#1-load-your-data","text":"As detailled in the dedicated section , EDS-TeVa is expecting to work with Pandas or Koalas DataFrames. We provide various connectors to facilitate data fetching, namely a Hive connector, a Postgres connector and a LocalData . Using a Hive DataBase Using a Postgres DataBase Using a Local DataBase from edsteva.io import HiveData db_name = \"my_db\" tables_to_load = [ \"visit_occurrence\" , \"visit_detail\" , \"care_site\" , \"fact_relationship\" , ] data = HiveData ( db_name , tables_to_load = tables_to_load ) data . visit_occurrence # (1) With this connector, visit_occurrence will be a Koalas DataFrame from edsteva.io import PostgresData db_name = \"my_db\" schema = \"my_schema\" user = \"my_username\" data = PostgresData ( db_name , schema = schema , user = user ) # (1) data . visit_occurrence # (2) This connector expects a .pgpass file storing the connection parameters With this connector, visit_occurrence will be a Pandas DataFrame import os from edsteva.io import LocalData folder = os . path . abspath ( MY_FOLDER_PATH ) data = LocalData ( folder ) # (1) data . visit_occurrence # (2) This connector expects a folder with a file per table to load. With this connector, visit_occurrence will be a Pandas DataFrame","title":"1. Load your data"},{"location":"#2-choose-a-probe-or-create-a-new-probe","text":"Probe A Probe is a python class designed to compute a completeness predictor \\(c(t)\\) that characterizes data availability of a target variable over time \\(t\\) . In this example, \\(c(t)\\) predicts the availability of administrative records relative to visits. It is defined for each care site and stay type as the number of visits \\(n_{visit}(t)\\) per month \\(t\\) , normalized by the \\(99^{th}\\) percentile of visits \\(n_{99}\\) computed over the entire study period: \\[ c(t) = \\frac{n_{visit}(t)}{n_{99}} \\] If the \\(99^{th}\\) percentile of visits \\(n_{99}\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. The VisitProbe is already available by default in the library:","title":"2. Choose a Probe or create a new Probe"},{"location":"#21-compute-your-probe","text":"The compute() method takes a Data object as input and stores the computed completeness predictor \\(c(t)\\) in the predictor attribute of a Probe : from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , # (1) }, care_site_levels = [ \"Hospital\" , \"Pole\" , \"UF\" ], # (2) ) visit . save ( path = probe_path ) # (3) visit . predictor . head () 1. The stay_types argument expects a python dictionary with labels as keys and regex as values. 2. The care sites are articulated into levels (cf. AP-HP's reference structure ). 3. Saving the Probe after computation saves you from having to compute it again. You just use VisitProbe.load(path=probe_path) . Saved to /my_path/visit.pkl care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.769","title":"2.1 Compute your Probe"},{"location":"#22-filter-your-probe","text":"In this example, we consider the poles of three hospitals over the period from 2009 to 2021. We consequently filter data before any further analysis. from edsteva.probes import VisitProbe start_date , end_date = ( \"2009-01-01\" , \"2021-01-01\" ) # (1) care_site_short_name = [ \"HOSPITAL 1\" , \"HOSPITAL 2\" , \"HOSPITAL 3\" ] bct_visit = VisitProbe () bct_visit . load () bct_visit . predictor = bct_visit . predictor [ # (2) ( bct_visit . predictor [ \"date\" ] >= start_date ) & ( bct_visit . predictor [ \"date\" ] <= end_date ) ] bct_visit . filter_care_site ( care_site_short_names = care_site_short_name ) # (3) This is the study period considered in the example. bct_visit.predictor is a Pandas.DataFrame , you can use Pandas'API to filter the Probe. To filter care sites there is a dedicated method that also includes all upper and lower levels care sites related to the selected care sites.","title":"2.2 Filter your Probe"},{"location":"#23-visualize-your-probe","text":"","title":"2.3 Visualize your Probe"},{"location":"#interactive-dashboard","text":"Interactive dashboards can be used to visualize the average completeness predictor \\(c(t)\\) of the selected care sites and stay types. from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = bct_visit , care_site_level = \"Pole\" , ) Interactive dashboard is available here","title":"Interactive dashboard"},{"location":"#static-plot","text":"If you need a static plot for a report, a paper or anything else, you can use the plot_probe() function. It returns the top plot of the dashboard without the interactive filters. Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_probe plot_path = \"my_path/visit.html\" stay_type = \"All\" plot_probe ( probe = bct_visit , care_site_level = \"Hospital\" , stay_type = stay_type , save_path = plot_path , # (1) ) If a save_path is specified, it'll save your plot in the specified path. { \"schema-url\": \"assets/charts/visit.json\" }","title":"Static plot"},{"location":"#3-choose-a-model-or-create-a-new-model","text":"Model A Model is a python class designed to fit a function \\(f_\\Theta(t)\\) to each completeness predictor \\(c(t)\\) of a Probe . The fit process estimates the coefficients \\(\\Theta\\) with metrics to characterize the temporal variability of data availability. In this example, the model fits a step function \\(f_{t_0, c_0}(t)\\) to the completeness predictor \\(c(t)\\) with coefficients \\(\\Theta = (t_0, c_0)\\) : \\[ f_{t_0, c_0}(t) = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic value \\(c_0\\) estimates the stabilized routine completeness. It also computes the following \\(error\\) metric that estimates the stability of the data after \\(t_0\\) : \\[ \\begin{aligned} error & = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\\\ \\epsilon(t) & = f_{t_0, c_0}(t) - c(t) \\end{aligned} \\] This step function Model is available in the library.","title":"3. Choose a Model or create a new Model"},{"location":"#31-fit-your-model","text":"The fit method takes a Probe as input, it estimates the coefficients, for example by minimizing a quadratic loss function and computes the metrics. Finally, it stores the estimated coefficients and the computed metrics in the estimates attribute of the Model . from edsteva.models.step_function import StepFunction model_path = \"my_path/fitted_visit.pkl\" step_function_model = StepFunction () step_function_model . fit ( probe = bct_visit ) step_function_model . save ( model_path ) # (1) step_function_model . estimates . head () Saving the Model after fitting saves you from having to fit it again. You just use StepFunction.load(path=model_path) . Saved to /my_path/fitted_visit.pkl care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Urg_Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Urg_Hospit' 2022-02-01 0.652 0.027","title":"3.1 Fit your Model"},{"location":"#32-visualize-your-fitted-probe","text":"","title":"3.2 Visualize your fitted Probe"},{"location":"#interactive-dashboard_1","text":"Interactive dashboards can be used to visualize the average completeness predictor \\(c(t)\\) along with the fitted step function of the selected care sites and stay types. from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = bct_visit , fitted_model = step_function_model , care_site_level = \"Pole\" , ) Interactive dashboard is available here .","title":"Interactive dashboard"},{"location":"#static-plot_1","text":"If you need a static plot for a report, a paper or anything else, you can use the plot_probe() function. It returns the top plot of the dashboard without the interactive filters. Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_probe plot_path = \"my_path/fitted_visit.html\" stay_type = { \"All\" : \".*\" } plot_probe ( probe = bct_visit , fitted_model = step_function_model , care_site_level = \"Hospital\" , stay_type = stay_type , save_path = plot_path , # (1) ) 1. If a save_path is specified, it'll save your plot in the specified path. { \"schema-url\": \"assets/charts/fitted_visit.json\" }","title":"Static plot"},{"location":"#4-set-the-thresholds-to-fix-the-deployment-bias","text":"Now, that we have estimated \\(t_0\\) , \\(c_0\\) and \\(error\\) for each care site and each stay type, one can set a threshold for each estimate in order to select only the care sites where the visits are available over the period of interest.","title":"4. Set the thresholds to fix the deployment bias"},{"location":"#41-visualize-estimates-distributions","text":"Visualizing the density plots and the medians of the estimates can help you setting the thresholds' values. from edsteva.viz import plot_estimates_densities plot_estimates_densities ( fitted_model = step_function_model , ) { \"schema-url\": \"assets/charts/distributions.json\" }","title":"4.1 Visualize estimates distributions"},{"location":"#42-set-the-thresholds","text":"The estimates dashboard provides a representation of the overall deviation from the Model on the top and interactive sliders on the bottom that allows you to vary the thresholds. The idea is to set the thresholds that keep the most care sites while having an acceptable overall deviation. from edsteva.viz.dashboards import estimates_dashboard estimates_dashboard ( probe = bct_visit , fitted_model = step_function_model , care_site_level = \"Pole\" , ) The threshold dashboard is available here .","title":"4.2 Set the thresholds"},{"location":"#43-fix-the-deployment-bias","text":"Once you set the thresholds, you can extract for each stay type the care sites for which data availability is estimated to be stable over the entire study period. t_0_max = \"2009-01-01\" # (1) c_0_min = 0.63 # (2) error_max = 0.03 # (3) estimates = step_function_model . estimates selected_care_site = estimates [ ( estimates [ \"t_0\" ] <= t_0_max ) & ( estimates [ \"c_0\" ] >= c_0_min ) & ( estimates [ \"error\" ] <= error_max ) ] print ( selected_care_site [ \"care_site_id\" ] . tolist ()) In this example the study period starts on January 1, 2009. The characteristic value \\(c_0\\) estimates the stabilized routine completeness. As we want the selected care sites to have a good completeness after \\(t_0\\) , one can for example set the threshold around the median (cf. distribution ) to keep half of the care sites with the highest completeness after \\(t_0\\) . \\(error\\) estimates the stability of the data after \\(t_0\\) . As we want the selected care sites to be stable after \\(t_0\\) , one can set the threshold around the median (cf. distribution ) to keep half of the care sites with the lowest error after \\(t_0\\) . [8312056386, 8457691845, 8745619784, 8314578956, 8314548764, 8542137845] In this example, \\(c_0\\) and \\(error\\) thresholds have been set around the median (cf. distribution ). However, this method is arbitrary and you have to find the appropriate method for your study with the help of the estimate dashboard . Limitations EDS-TeVa provides modelling tools to characterize the temporal variability of your data, it does not intend to provide direct methods to fix the deployment bias. As an open-source library, EDS-TeVa is also here to host a discussion in order to facilitate collective methodological convergence on flexible solutions. The default methods proposed in this example is intended to be reviewed and challenged by the user community.","title":"4.3 Fix the deployment bias"},{"location":"#make-it-your-own","text":"The working example above describes the canonical usage workflow. However, you would probably need different Probes, Models, Visualizations and methods to set the thresholds for your projects. The components already available in the library are listed below but if it doesn't meet your requirements, you are encouraged to create your own. Contribution If you managed to implement your own component, or even if you just thought about a new component do not hesitate to share it with the community by following the contribution guidelines . Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Make it your own"},{"location":"#available-components","text":"Probe Model Visualization VisitProbe NoteProbe The VisitProbe computes \\(c_{visit}(t)\\) the availability of administrative data related to visits for each care site and each stay type according to time: \\[ c_{visit}(t) = \\frac{n_{visit}(t)}{n_{99}} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{99}\\) is the \\(99^{th}\\) percentile of visits and \\(t\\) is the month. If the \\(99^{th}\\) percentile of visits \\(n_{99}\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import VisitProbe visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, ) visit . predictor . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg' 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Hospit' 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.769 The NoteProbe computes \\(c_{note}(t)\\) the availability of clinical documents linked to patients' visits for each care site, stay type and note type according to time: \\[ c_{note}(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{with\\,doc}\\) the number of visits having at least one document and \\(t\\) is the month. If the number of visits \\(n_{visit}(t)\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import NoteProbe note = Note () note . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, note_types = { \"All\" : \".*\" , \"CRH\" : \"crh\" , \"Ordonnance\" : \"ordo\" , \"CR Passage Urgences\" : \"urge\" , }, ) note . predictor . head () care_site_level care_site_id care_site_short_name stay_type note_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg' 'All' 2019-05-01 233.0 '0.841 Unit\u00e9 Fonctionnelle (UF) 8653815660 Care site 1 'All' 'CRH' 2011-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Hospit' 'CRH' 2021-03-01 204.0 0.497 P\u00f4le/DMU 8312056379 Care site 2 'All' 'Ordonnance' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 'CR Passage Urgences' 2022-02-01 9746.0 0.769 StepFunction RectangleFunction Coefficients Metrics Algos Example The StepFunction fits a step function \\(f_{t_0, c_0}(t)\\) with coefficients \\(\\Theta = (t_0, c_0)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0}(t) & = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\\\ c(t) & = f_{t_0, c_0}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic value \\(c_0\\) estimates the stabilized routine completeness. The default metric computed is the mean squared error after \\(t_0\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\] \\(error\\) estimates the stability of the data after \\(t_0\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algorithm if they don't meet your requirements. Loss minimization Quantile This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) and \\(\\hat{c_0}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{c_0}) & = \\underset{t_0, c_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0}(t)) = |c(t) - f_{t_0, c_0}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented with a dependency relation between \\(c_0\\) and \\(t_0\\) derived from the optimal estimates using the \\(l_2\\) loss function. For more informations, you can have a look on the source code . In this algorithm, \\(\\hat{c_0}\\) is directly estimated as the \\(x^{th}\\) quantile of the completeness predictor \\(c(t)\\) , where \\(x\\) is a number between 0 and 1. Then, \\(\\hat{t_0}\\) is the first time \\(c(t)\\) reaches \\(\\hat{c_0}\\) . \\[ \\begin{aligned} \\hat{c_0} & = x^{th} \\text{ quantile of } c(t) \\\\ \\hat{t_0} & = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) \\end{aligned} \\] Default quantile \\(x\\) The default quantile is \\(x = 0.8\\) . from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 0.027 Coefficients Metrics Algos Example The RectangleFunction fits a step function \\(f_{t_0, c_0, t_1}(t)\\) with coefficients \\(\\Theta = (t_0, c_0, t_1)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0, t_1}(t) & = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) \\\\ c(t) & = f_{t_0, c_0, t_1}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic time \\(t_1\\) estimates the time after which the data is not available anymore. the characteristic value \\(c_0\\) estimates the completeness between \\(t_0\\) and \\(t_1\\) . The default metric computed is the mean squared error between \\(t_0\\) and \\(t_1\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_1} \\epsilon(t)^2}{t_1 - t_0} \\] \\(error\\) estimates the stability of the data between \\(t_0\\) and \\(t_1\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algorithm if they don't meet your requirements. Loss minimization This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) , \\(\\hat{c_0}\\) and \\(\\hat{t_1}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0, t_1)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}, \\hat{c_0}) & = \\underset{t_0, c_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0, t_1)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t)) = |c(t) - f_{t_0, c_0, t_1}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented with a dependency relation between \\(c_0\\) and \\(t_0\\) derived from the optimal estimates using the \\(l_2\\) loss function. For more informations, you can have a look on the source code . from edsteva.models.rectangle_function import RectangleFunction rectangle_function_model = RectangleFunction () rectangle_function_model . fit ( probe ) rectangle_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 t_1 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 2020-05-01 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 2013-04-01 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 2022-03-01 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 2019-08-01 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 2022-08-01 0.027 Dashboard Plot The library provides interactive dashboards that let you set any combination of care sites, stay types and other columns if included in the Probe. You can only export a dashboard in HTML format. predictor_dashboard() estimates_dashboard() The predictor_dashboard() returns: On the top, the aggregated variable is the average completeness predictor \\(c(t)\\) over time \\(t\\) with the prediction \\(\\hat{c}(t)\\) if the fitted Model is specified. On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.). from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The estimates_dashboard() returns a representation of the overall deviation from the Model : On the top, the aggregated variable is a normalized completeness predictor \\(\\frac{c(t)}{c_0}\\) over normalized time \\(t - t_0\\) . On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.) with all the Model coefficients and metrics included in the Model . from edsteva.viz.dashboards import estimates_dashboard threshold_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The library provides static plots that you can export in png or svg. As it is less interactive, you may specify the filters in the inputs of the functions. plot_probe() plot_normalized_probe() plot_estimates_densities() The plot_probe() returns the top plot of the predictor_dashboard() : the normalized completeness predictor \\(\\frac{c(t)}{c_0}\\) over normalized time \\(t - t_0\\) . from edsteva.viz.plots import plot_probe plot_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"assets/charts/fitted_visit.json\" } The plot_normalized_probe() returns the top plot of the estimates_dashboard() . Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_normalized_probe plot_normalized_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"assets/charts/normalized_plot.json\" } The plot_estimates_densities() returns the density plot and the median of each estimate. It can help you to set the thresholds. from edsteva.viz.plots import plot_estimates_densities plot_estimates_densities ( fitted_model = step_function_model , ) { \"schema-url\": \"assets/charts/distributions.json\" } Samuel G Finlayson, Adarsh Subbaswamy, Karandeep Singh, John Bowers, Annabel Kupke, Jonathan Zittrain, Isaac S Kohane, and Suchi Saria. The clinician and dataset shift in artificial intelligence. The New England journal of medicine , 385(3):283, 2021. \u21a9","title":"Available components"},{"location":"changelog/","text":"Changelog v1.0.0 - 29-11-2022 Initial release","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v100-29-11-2022","text":"Initial release","title":"v1.0.0 - 29-11-2022"},{"location":"contributing/","text":"Contributing We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues. Suggest and help prioritize new functionalities. Develop a new Probe or a new Model ! Fork the project and propose a new functionality through a pull request. Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you. Guidelines 1. Development installation Ready to contribute? Here's how to set up edsteva for local development. Fork the edsteva repo. Clone your fork locally: $ git clone git@github.com:your_name_here/edsteva.git ---> 100% Optional, create a virtual environment: $ cd edsteva $ python -m venv .venv $ source .venv/bin/activate Install Poetry (a tool for dependency management and packaging in Python): Linux, macOS, Windows (WSL) Windows (Powershell) $ curl -sSL https://install.python-poetry.org | python3 - ---> 100% $ ( Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing ) .Content | py - ---> 100% For more details, check the installation guide Install dependencies: $ poetry config experimental.new-installer false $ poetry install color:lightblue Updating dependencies color:lightblue Resolving dependencies... (25.3s) color:lightblue Writing lock file Package operations: 126 installs, 0 updates, 0 removals \u2022 Installing .... \u2022 Installing .... Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature 2. Style guide We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. Tip Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% All good ! 3. Testing your code We use the Pytest test suite. Writing your own tests is encouraged ! The following command will run the test suite: $ poetry run pytest tests --cov edsteva --junitxml = report.xml collected X items tests/test_a.py tests/test_b.py tests/test_your_bug.py ---> 100% Should your contribution propose a bug fix, we require the bug be thoroughly tested. 4. Documentation Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-TeVa's documentation. You can checkout the changes you make with: $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page. 5. Proposing a merge request At the very least, if your changes are well-documented, pass every tests, and follow the style guide, you can: Commit your changes and push your branch: $ git add * $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request.","title":"Contributing"},{"location":"contributing/#contributing","text":"We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues. Suggest and help prioritize new functionalities. Develop a new Probe or a new Model ! Fork the project and propose a new functionality through a pull request. Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.","title":"Contributing"},{"location":"contributing/#guidelines","text":"","title":"Guidelines"},{"location":"contributing/#1-development-installation","text":"Ready to contribute? Here's how to set up edsteva for local development. Fork the edsteva repo. Clone your fork locally: $ git clone git@github.com:your_name_here/edsteva.git ---> 100% Optional, create a virtual environment: $ cd edsteva $ python -m venv .venv $ source .venv/bin/activate Install Poetry (a tool for dependency management and packaging in Python): Linux, macOS, Windows (WSL) Windows (Powershell) $ curl -sSL https://install.python-poetry.org | python3 - ---> 100% $ ( Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing ) .Content | py - ---> 100% For more details, check the installation guide Install dependencies: $ poetry config experimental.new-installer false $ poetry install color:lightblue Updating dependencies color:lightblue Resolving dependencies... (25.3s) color:lightblue Writing lock file Package operations: 126 installs, 0 updates, 0 removals \u2022 Installing .... \u2022 Installing .... Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature","title":"1. Development installation"},{"location":"contributing/#2-style-guide","text":"We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. Tip Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% All good !","title":"2. Style guide"},{"location":"contributing/#3-testing-your-code","text":"We use the Pytest test suite. Writing your own tests is encouraged ! The following command will run the test suite: $ poetry run pytest tests --cov edsteva --junitxml = report.xml collected X items tests/test_a.py tests/test_b.py tests/test_your_bug.py ---> 100% Should your contribution propose a bug fix, we require the bug be thoroughly tested.","title":"3. Testing your code"},{"location":"contributing/#4-documentation","text":"Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-TeVa's documentation. You can checkout the changes you make with: $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.","title":"4. Documentation"},{"location":"contributing/#5-proposing-a-merge-request","text":"At the very least, if your changes are well-documented, pass every tests, and follow the style guide, you can: Commit your changes and push your branch: $ git add * $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request.","title":"5. Proposing a merge request"},{"location":"components/loading_data/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Loading Data Here is a tutorial for loading your data which is the first step in the EDS-TeVa usage workflow . % config Completer . use_jedi = False % load_ext autoreload % autoreload 2 import os from datetime import datetime 3 classes are available to facilitate data access: HiveData : Getting data from a Hive cluster, returning Koalas DataFrames. LocalData : Getting data from tables saved on disk, returning Pandas DataFrames. PostgresData : Getting data from a PostGreSQL DB, returning Pandas DataFrames. from edsteva.io import HiveData , LocalData , PostgresData Loading from Hive: HiveData The HiveData class expects two parameters: A SparkSession variable The name of the Database to connect to Using Spark kernels All kernels designed to use Spark are configured to expose 3 variables at startup: spark , the current SparkSession sc , the current SparkContext sql , a function to execute SQL code on the Hive Database. In this case you can just provide the spark variable to HiveData ! If needed, the following snippet allows to create the necessary variables: from pyspark import SparkConf , SparkContext from pyspark.sql.session import SparkSession conf = SparkConf () sc = SparkContext ( conf = conf ) spark = SparkSession . builder \\ . enableHiveSupport () \\ . getOrCreate () sql = spark . sql The class HiveData provides a convenient interface to OMOP data stored in Hive. The OMOP tables can be accessed as attribute and they are represented as Koalas DataFrames . You simply need to mention your Hive database name. data = HiveData ( spark , DB_NAME ) By default, only a subset of tables are added as attributes: data . available_tables ['care_site', 'concept', 'condition_occurrence', 'person', 'procedure_occurrence', 'visit_detail', 'visit_occurrence'] Koalas DataFrames, like Spark DataFrames, rely on a lazy execution plan: As long as no data needs to be specifically collected, saved or displayed, no code is executed. It is simply saved for a later execution. The main interest of Koalas DataFrames is that you can use (most of) the Pandas API: person = data . person person . drop ( columns = [ 'person_id' ]) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } location_id year_of_birth month_of_birth day_of_birth birth_datetime death_datetime gender_source_value gender_source_concept_id cdm_source 0 3347087777 1949 8 2 1949-08-02 NaT f 2008119903 ORBIS 1 9818741928 1975 7 6 1975-07-06 NaT m 2008119900 ORBIS 2 3345464435 1990 9 7 1990-09-07 NaT f 2008119903 ORBIS 3 3346060919 1964 5 18 1964-05-18 NaT f 2008119903 ORBIS 4 3347197472 1990 2 2 1990-02-02 NaT m 2008119900 ORBIS person [ 'is_over_50' ] = ( person [ 'birth_datetime' ] >= datetime ( 1971 , 1 , 1 )) stats = ( person . groupby ( 'is_over_50' ) . person_id . count () ) Once data has been sufficiently aggregated, it can be converted back to Pandas, e.g. for plotting. stats_pd = stats . to_pandas () Similarily, if you want to work on the Spark DataFrame instead, a similar method is available: person_spark = person . to_spark () Persisting/Reading a sample to/from disk: LocalData Working with Pandas DataFrame is, when possible, more convenient. You have the possibility to save your database or at least a subset of it. Doing so allows you to work on it later without having to go through Spark again. Careful with cohort size Do not save it if your cohort is big : This saves all available tables on disk. For instance, let us define a dummy subset of 1000 patients: visits = data . visit_occurrence selected_visits = ( visits . loc [ visits [ \"stay_source_value\" ] == \"MCO\" ] ) sample_patients = ( selected_visits [ \"person_id\" ] . drop_duplicates () . head ( 1000 ) . to_list () ) And save every table restricted to this small cohort as a parquet file: folder = os . path . abspath ( MY_FOLDER_PATH ) os . makedirs ( folder , exist_ok = True ) tables_to_save = [ \"person\" , \"visit_detail\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( folder , tables = tables_to_save , person_ids = sample_patients ) Once you saved some data to disk, a dedicated class can be used to access it: The class LocalData can be used to load OMOP data from a folder containing several parquet files. The tables are accessed as attributes and are returned as Pandas DataFrame. Warning In this case, the whole table will be loaded into memory on a single jupyter server. Consequently it is advised to only use this for small datasets. data = LocalData ( folder ) data . available_tables ['visit_occurrence', 'visit_detail', 'person'] person = data . person print ( f \"type: { type ( person ) } \" ) print ( f \"shape: { person . shape } \" ) type: <class 'pandas.core.frame.DataFrame'> shape: (1000, 10) Loading from PostGres: PostgresData OMOP data can be stored in a PostreSQL database. The PostgresData class provides a convinient interface to it. Note This class relies on the file ~/.pgpass that contains your identifiers for several databases. data = PostgresData ( dbname = DB , schema = \"omop\" , user = USER ) data . read_sql ( \"select count(*) from person\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count 0 12688670 {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"Loading data"},{"location":"components/loading_data/#loading-data","text":"Here is a tutorial for loading your data which is the first step in the EDS-TeVa usage workflow . % config Completer . use_jedi = False % load_ext autoreload % autoreload 2 import os from datetime import datetime 3 classes are available to facilitate data access: HiveData : Getting data from a Hive cluster, returning Koalas DataFrames. LocalData : Getting data from tables saved on disk, returning Pandas DataFrames. PostgresData : Getting data from a PostGreSQL DB, returning Pandas DataFrames. from edsteva.io import HiveData , LocalData , PostgresData","title":"Loading Data"},{"location":"components/loading_data/#loading-from-hive-hivedata","text":"The HiveData class expects two parameters: A SparkSession variable The name of the Database to connect to Using Spark kernels All kernels designed to use Spark are configured to expose 3 variables at startup: spark , the current SparkSession sc , the current SparkContext sql , a function to execute SQL code on the Hive Database. In this case you can just provide the spark variable to HiveData ! If needed, the following snippet allows to create the necessary variables: from pyspark import SparkConf , SparkContext from pyspark.sql.session import SparkSession conf = SparkConf () sc = SparkContext ( conf = conf ) spark = SparkSession . builder \\ . enableHiveSupport () \\ . getOrCreate () sql = spark . sql The class HiveData provides a convenient interface to OMOP data stored in Hive. The OMOP tables can be accessed as attribute and they are represented as Koalas DataFrames . You simply need to mention your Hive database name. data = HiveData ( spark , DB_NAME ) By default, only a subset of tables are added as attributes: data . available_tables ['care_site', 'concept', 'condition_occurrence', 'person', 'procedure_occurrence', 'visit_detail', 'visit_occurrence'] Koalas DataFrames, like Spark DataFrames, rely on a lazy execution plan: As long as no data needs to be specifically collected, saved or displayed, no code is executed. It is simply saved for a later execution. The main interest of Koalas DataFrames is that you can use (most of) the Pandas API: person = data . person person . drop ( columns = [ 'person_id' ]) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } location_id year_of_birth month_of_birth day_of_birth birth_datetime death_datetime gender_source_value gender_source_concept_id cdm_source 0 3347087777 1949 8 2 1949-08-02 NaT f 2008119903 ORBIS 1 9818741928 1975 7 6 1975-07-06 NaT m 2008119900 ORBIS 2 3345464435 1990 9 7 1990-09-07 NaT f 2008119903 ORBIS 3 3346060919 1964 5 18 1964-05-18 NaT f 2008119903 ORBIS 4 3347197472 1990 2 2 1990-02-02 NaT m 2008119900 ORBIS person [ 'is_over_50' ] = ( person [ 'birth_datetime' ] >= datetime ( 1971 , 1 , 1 )) stats = ( person . groupby ( 'is_over_50' ) . person_id . count () ) Once data has been sufficiently aggregated, it can be converted back to Pandas, e.g. for plotting. stats_pd = stats . to_pandas () Similarily, if you want to work on the Spark DataFrame instead, a similar method is available: person_spark = person . to_spark ()","title":"Loading from Hive: HiveData"},{"location":"components/loading_data/#persistingreading-a-sample-tofrom-disk-localdata","text":"Working with Pandas DataFrame is, when possible, more convenient. You have the possibility to save your database or at least a subset of it. Doing so allows you to work on it later without having to go through Spark again. Careful with cohort size Do not save it if your cohort is big : This saves all available tables on disk. For instance, let us define a dummy subset of 1000 patients: visits = data . visit_occurrence selected_visits = ( visits . loc [ visits [ \"stay_source_value\" ] == \"MCO\" ] ) sample_patients = ( selected_visits [ \"person_id\" ] . drop_duplicates () . head ( 1000 ) . to_list () ) And save every table restricted to this small cohort as a parquet file: folder = os . path . abspath ( MY_FOLDER_PATH ) os . makedirs ( folder , exist_ok = True ) tables_to_save = [ \"person\" , \"visit_detail\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( folder , tables = tables_to_save , person_ids = sample_patients ) Once you saved some data to disk, a dedicated class can be used to access it: The class LocalData can be used to load OMOP data from a folder containing several parquet files. The tables are accessed as attributes and are returned as Pandas DataFrame. Warning In this case, the whole table will be loaded into memory on a single jupyter server. Consequently it is advised to only use this for small datasets. data = LocalData ( folder ) data . available_tables ['visit_occurrence', 'visit_detail', 'person'] person = data . person print ( f \"type: { type ( person ) } \" ) print ( f \"shape: { person . shape } \" ) type: <class 'pandas.core.frame.DataFrame'> shape: (1000, 10)","title":"Persisting/Reading a sample to/from disk: LocalData"},{"location":"components/loading_data/#loading-from-postgres-postgresdata","text":"OMOP data can be stored in a PostreSQL database. The PostgresData class provides a convinient interface to it. Note This class relies on the file ~/.pgpass that contains your identifiers for several databases. data = PostgresData ( dbname = DB , schema = \"omop\" , user = USER ) data . read_sql ( \"select count(*) from person\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count 0 12688670 {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"Loading from PostGres: PostgresData"},{"location":"components/model/","text":"Model Choosing or customizing a Model is the third step in the EDS-TeVa usage workflow . Definition A Model is a python class designed to characterize the temporal variability of data availability. It estimates the coefficients \\(\\Theta\\) and the metrics from a Probe . Model class diagram Input The Model class is expecting a Probe object in order to estimate the Model coefficients \\(\\Theta\\) and some metrics if desired. Attributes estimates is a Pandas.DataFrame computed by the fit() method. It contains the estimated coefficients \\(\\Theta\\) and metrics for each column given by the Probe._index (e.g. care site, stay type, etc.). _coefs is the list of the Model coefficients \\(\\Theta\\) that are estimated by the fit() method. Methods fit() method calls the fit_process() method to compute the estimated coefficients \\(\\Theta\\) and metrics and store them in the estimates attribute. fit_process() method computes the estimated coefficients \\(\\Theta\\) and metrics from a Probe.predictor DataFrame. predict() method applies the predict_process() on a Probe.predictor DataFrame and returns a Pandas.DataFrame of the estimated prediction \\(\\hat{c}(t)\\) for each columns given by Probe._index . predict_process() method computes the estimated completeness predictor \\(\\hat{c}(t)\\) for each column given by Probe._index . save() method saves the Model in the desired path. By default it is saved in the cache directory (~/.cache/edsteva/models). load() method loads the Model from the desired path. By default it is loaded from the cache directory (~/.cache/edsteva/models). Prediction predict() method must be called on a fitted Model. Estimates schema Data stored in the estimates attribute follows a specific schema: Indexes The estimates are computed for each column given by the Probe._index . For example, if you fit your Model on the VisitProbe , the estimates will be computed for each: care_site_level : care site hierarchic level ( uf , pole , hospital ). care_site_id : care site unique identifier. stay_type : type of stay ( hospitalis\u00e9s , urgence , hospitalisation incompl\u00e8te , consultation externe ). Model coefficients It depends on the Model used, for instance the step function Model has 2 coefficients: \\(t_0\\) the characteristic time that estimates the time the after which the data is available. \\(c_0\\) the characteristic completeness that estimates the stabilized routine completeness after \\(t_0\\) . Metrics It depends on the metrics you specify in the fit() method. For instance, you can specify an \\(error\\) metric: \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\] \\(error\\) estimates the stability of the data after \\(t_0\\) . Example When considering the StepFunction.estimates fitted on a VisitProbe , it may for instance look like this: care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 0.040 P\u00f4le/DMU 8653815660 'All' 2011-04-01 0.583 0.028 Unit\u00e9 Fonctionnelle (UF) 8312027648 'Hospit' 2021-03-01 0.677 0.022 Unit\u00e9 Fonctionnelle (UF) 8312056379 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 0.027 Saving and loading a fitted Model In order to ease the future loading of a Model that has been fitted with the fit() method, one can pickle it using the save() method. This enables a rapid loading of the Model from local disk using the load() method. from edsteva.models import StepFunction model = StepFunction () model . fit ( probe ) # (1) model . save () # (2) model_2 = StepFunction () model_2 . load () # (3) Computation of the estimates (long). Saving of the fitted Model on the local disk. Rapid loading of the fitted Model fom the local disk. Defining a custom Model If none of the available Models meets your requirements, you may want to create your own. To define a custom Model class CustomModel that inherits from the abstract class BaseModel you'll have to implement the fit_process() and predict_process() methods (these methods are respectively called by the fit() method and the predict() method inherited by the BaseModel class). You'll also have to define the _coefs attribute which is the list of the Model coefficients. from edsteva.models import BaseModel from edsteva.probes import BaseProbe # Definition of a new Model class class CustomProbe ( BaseModel ): _coefs = [ \"my_model_coefficient_1\" , \"my_model_coefficient_2\" ] def fit_process ( self , probe : BaseProbe ): # fit process return custom_predictor def predict_process ( self , probe : BaseProbe ): # predict process return custom_predictor fit_process() and predict_process() methods take a Probe as the first argument. All other parameters must be keyword arguments. For a detailed example of the implementation of a Model, please have a look on the implemented StepFunction Model. Contributions If you managed to create your own Model do not hesitate to share it with the community by following the contribution guidelines . Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Available Models We detail hereafter the step function Model that has already been implemented in the library. StepFunction RectangleFunction Coefficients Metrics Algos Example The StepFunction fits a step function \\(f_{t_0, c_0}(t)\\) with coefficients \\(\\Theta = (t_0, c_0)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0}(t) & = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\\\ c(t) & = f_{t_0, c_0}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic value \\(c_0\\) estimates the stabilized routine completeness. The default metric computed is the mean squared error after \\(t_0\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\] \\(error\\) estimates the stability of the data after \\(t_0\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algo if they don't meet your requirements. Loss minimization Quantile This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) and \\(\\hat{c_0}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{c_0}) & = \\underset{t_0, c_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0}(t)) = |c(t) - f_{t_0, c_0}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented to compute the optimal estimates only with the \\(l_2\\) loss function. For more informations, you can have a look on the source code . In this algorithm, \\(\\hat{c_0}\\) is directly estimated as the \\(x^{th}\\) quantile of the completeness predictor \\(c(t)\\) , where \\(x\\) is a number between 0 and 1. Then, \\(\\hat{t_0}\\) is the first time \\(c(t)\\) reaches \\(\\hat{c_0}\\) . \\[ \\begin{aligned} \\hat{c_0} & = x^{th} \\text{ quantile of } c(t) \\\\ \\hat{t_0} & = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) \\end{aligned} \\] Default quantile \\(x\\) The default quantile is \\(x = 0.8\\) . from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 0.027 Coefficients Metrics Algos Example The RectangleFunction fits a step function \\(f_{t_0, c_0, t_1}(t)\\) with coefficients \\(\\Theta = (t_0, c_0, t_1)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0, t_1}(t) & = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) \\\\ c(t) & = f_{t_0, c_0, t_1}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic time \\(t_1\\) estimates the time after which the data is not available anymore. the characteristic value \\(c_0\\) estimates the completeness between \\(t_0\\) and \\(t_1\\) . The default metric computed is the mean squared error between \\(t_0\\) and \\(t_1\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_1} \\epsilon(t)^2}{t_1 - t_0} \\] \\(error\\) estimates the stability of the data between \\(t_0\\) and \\(t_1\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algorithm if they don't meet your requirements. Loss minimization This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) , \\(\\hat{c_0}\\) and \\(\\hat{t_1}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0, t_1)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}, \\hat{c_0}) & = \\underset{t_0, c_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0, t_1)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t)) = |c(t) - f_{t_0, c_0, t_1}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented with a dependency relation between \\(c_0\\) and \\(t_0\\) derived from the optimal estimates using the \\(l_2\\) loss function. For more informations, you can have a look on the source code . from edsteva.models.rectangle_function import RectangleFunction rectangle_function_model = RectangleFunction () rectangle_function_model . fit ( probe ) rectangle_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 t_1 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 2020-05-01 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 2013-04-01 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 2022-03-01 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 2019-08-01 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 2022-08-01 0.027","title":"Model"},{"location":"components/model/#model","text":"Choosing or customizing a Model is the third step in the EDS-TeVa usage workflow .","title":"Model"},{"location":"components/model/#definition","text":"A Model is a python class designed to characterize the temporal variability of data availability. It estimates the coefficients \\(\\Theta\\) and the metrics from a Probe . Model class diagram","title":"Definition"},{"location":"components/model/#input","text":"The Model class is expecting a Probe object in order to estimate the Model coefficients \\(\\Theta\\) and some metrics if desired.","title":"Input"},{"location":"components/model/#attributes","text":"estimates is a Pandas.DataFrame computed by the fit() method. It contains the estimated coefficients \\(\\Theta\\) and metrics for each column given by the Probe._index (e.g. care site, stay type, etc.). _coefs is the list of the Model coefficients \\(\\Theta\\) that are estimated by the fit() method.","title":"Attributes"},{"location":"components/model/#methods","text":"fit() method calls the fit_process() method to compute the estimated coefficients \\(\\Theta\\) and metrics and store them in the estimates attribute. fit_process() method computes the estimated coefficients \\(\\Theta\\) and metrics from a Probe.predictor DataFrame. predict() method applies the predict_process() on a Probe.predictor DataFrame and returns a Pandas.DataFrame of the estimated prediction \\(\\hat{c}(t)\\) for each columns given by Probe._index . predict_process() method computes the estimated completeness predictor \\(\\hat{c}(t)\\) for each column given by Probe._index . save() method saves the Model in the desired path. By default it is saved in the cache directory (~/.cache/edsteva/models). load() method loads the Model from the desired path. By default it is loaded from the cache directory (~/.cache/edsteva/models). Prediction predict() method must be called on a fitted Model.","title":"Methods"},{"location":"components/model/#estimates-schema","text":"Data stored in the estimates attribute follows a specific schema:","title":"Estimates schema"},{"location":"components/model/#indexes","text":"The estimates are computed for each column given by the Probe._index . For example, if you fit your Model on the VisitProbe , the estimates will be computed for each: care_site_level : care site hierarchic level ( uf , pole , hospital ). care_site_id : care site unique identifier. stay_type : type of stay ( hospitalis\u00e9s , urgence , hospitalisation incompl\u00e8te , consultation externe ).","title":"Indexes"},{"location":"components/model/#model-coefficients","text":"It depends on the Model used, for instance the step function Model has 2 coefficients: \\(t_0\\) the characteristic time that estimates the time the after which the data is available. \\(c_0\\) the characteristic completeness that estimates the stabilized routine completeness after \\(t_0\\) .","title":"Model coefficients"},{"location":"components/model/#metrics","text":"It depends on the metrics you specify in the fit() method. For instance, you can specify an \\(error\\) metric: \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\] \\(error\\) estimates the stability of the data after \\(t_0\\) .","title":"Metrics"},{"location":"components/model/#example","text":"When considering the StepFunction.estimates fitted on a VisitProbe , it may for instance look like this: care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 0.040 P\u00f4le/DMU 8653815660 'All' 2011-04-01 0.583 0.028 Unit\u00e9 Fonctionnelle (UF) 8312027648 'Hospit' 2021-03-01 0.677 0.022 Unit\u00e9 Fonctionnelle (UF) 8312056379 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 0.027","title":"Example"},{"location":"components/model/#saving-and-loading-a-fitted-model","text":"In order to ease the future loading of a Model that has been fitted with the fit() method, one can pickle it using the save() method. This enables a rapid loading of the Model from local disk using the load() method. from edsteva.models import StepFunction model = StepFunction () model . fit ( probe ) # (1) model . save () # (2) model_2 = StepFunction () model_2 . load () # (3) Computation of the estimates (long). Saving of the fitted Model on the local disk. Rapid loading of the fitted Model fom the local disk.","title":"Saving and loading a fitted Model"},{"location":"components/model/#defining-a-custom-model","text":"If none of the available Models meets your requirements, you may want to create your own. To define a custom Model class CustomModel that inherits from the abstract class BaseModel you'll have to implement the fit_process() and predict_process() methods (these methods are respectively called by the fit() method and the predict() method inherited by the BaseModel class). You'll also have to define the _coefs attribute which is the list of the Model coefficients. from edsteva.models import BaseModel from edsteva.probes import BaseProbe # Definition of a new Model class class CustomProbe ( BaseModel ): _coefs = [ \"my_model_coefficient_1\" , \"my_model_coefficient_2\" ] def fit_process ( self , probe : BaseProbe ): # fit process return custom_predictor def predict_process ( self , probe : BaseProbe ): # predict process return custom_predictor fit_process() and predict_process() methods take a Probe as the first argument. All other parameters must be keyword arguments. For a detailed example of the implementation of a Model, please have a look on the implemented StepFunction Model. Contributions If you managed to create your own Model do not hesitate to share it with the community by following the contribution guidelines . Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Defining a custom Model"},{"location":"components/model/#available-models","text":"We detail hereafter the step function Model that has already been implemented in the library. StepFunction RectangleFunction Coefficients Metrics Algos Example The StepFunction fits a step function \\(f_{t_0, c_0}(t)\\) with coefficients \\(\\Theta = (t_0, c_0)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0}(t) & = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\\\ c(t) & = f_{t_0, c_0}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic value \\(c_0\\) estimates the stabilized routine completeness. The default metric computed is the mean squared error after \\(t_0\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\epsilon(t)^2}{t_{max} - t_0} \\] \\(error\\) estimates the stability of the data after \\(t_0\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algo if they don't meet your requirements. Loss minimization Quantile This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) and \\(\\hat{c_0}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{c_0}) & = \\underset{t_0, c_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0}(t)) = |c(t) - f_{t_0, c_0}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented to compute the optimal estimates only with the \\(l_2\\) loss function. For more informations, you can have a look on the source code . In this algorithm, \\(\\hat{c_0}\\) is directly estimated as the \\(x^{th}\\) quantile of the completeness predictor \\(c(t)\\) , where \\(x\\) is a number between 0 and 1. Then, \\(\\hat{t_0}\\) is the first time \\(c(t)\\) reaches \\(\\hat{c_0}\\) . \\[ \\begin{aligned} \\hat{c_0} & = x^{th} \\text{ quantile of } c(t) \\\\ \\hat{t_0} & = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) \\end{aligned} \\] Default quantile \\(x\\) The default quantile is \\(x = 0.8\\) . from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 0.027 Coefficients Metrics Algos Example The RectangleFunction fits a step function \\(f_{t_0, c_0, t_1}(t)\\) with coefficients \\(\\Theta = (t_0, c_0, t_1)\\) on a completeness predictor \\(c(t)\\) : \\[ \\begin{aligned} f_{t_0, c_0, t_1}(t) & = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) \\\\ c(t) & = f_{t_0, c_0, t_1}(t) + \\epsilon(t) \\end{aligned} \\] the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic time \\(t_1\\) estimates the time after which the data is not available anymore. the characteristic value \\(c_0\\) estimates the completeness between \\(t_0\\) and \\(t_1\\) . The default metric computed is the mean squared error between \\(t_0\\) and \\(t_1\\) : \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_1} \\epsilon(t)^2}{t_1 - t_0} \\] \\(error\\) estimates the stability of the data between \\(t_0\\) and \\(t_1\\) . Custom metric You can define your own metric if this one doesn't meet your requirements. The available algorithms used to fit the step function are listed below: Custom algo You can define your own algorithm if they don't meet your requirements. Loss minimization This algorithm computes the estimated coefficients \\(\\hat{t_0}\\) , \\(\\hat{c_0}\\) and \\(\\hat{t_1}\\) by minimizing the loss function \\(\\mathcal{L}(t_0, c_0, t_1)\\) : \\[ \\begin{aligned} \\mathcal{L}(t_0, c_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}, \\hat{c_0}) & = \\underset{t_0, c_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, c_0, t_1)) \\\\ \\end{aligned} \\] Default loss function \\(\\mathcal{l}\\) The loss function is \\(l_2\\) by default: $$ \\mathcal{l}(c(t), f_{t_0, c_0, t_1}(t)) = |c(t) - f_{t_0, c_0, t_1}(t)|^2 $$ Optimal estimates For complexity purposes, this algorithm has been implemented with a dependency relation between \\(c_0\\) and \\(t_0\\) derived from the optimal estimates using the \\(l_2\\) loss function. For more informations, you can have a look on the source code . from edsteva.models.rectangle_function import RectangleFunction rectangle_function_model = RectangleFunction () rectangle_function_model . fit ( probe ) rectangle_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 t_1 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 2020-05-01 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 2013-04-01 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 2022-03-01 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 2019-08-01 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 2022-08-01 0.027","title":"Available Models"},{"location":"components/probe/","text":"Probe Choosing or customizing a Probe is the second step in the EDS-TeVa usage workflow . Definition A Probe is a python class designed to characterize data availability of a target variable over time \\(t\\) . It aggregates the loaded data to obtain a completeness predictor \\(c(t)\\) . Probe class diagram Input As detailled in the dedicated section , the Probe class is expecting a Data object with Pandas or Koalas DataFrames. We provide various connectors to facilitate data fetching, namely a Hive connector, a Postgres connector and a LocalData . Attributes predictor is a Pandas.DataFrame computed by the compute() method. It contains the desired completeness predictor \\(c(t)\\) for each column in the _index attribute (care site, stay type and any other needed column). _index is the list of columns that are used to aggregate the data in the compute() method. Methods compute() method calls the compute_process() method to compute the completeness predictors \\(c(t)\\) and store them in the predictor attribute. compute_process() method aggregates the input data to compute the completeness predictors \\(c(t)\\) . filter_care_site() method filters predictor attribute on the selected care sites including upper and lower levels care sites. save() method saves the Probe in the desired path. By default it is saved in the the cache directory (~/.cache/edsteva/probes). load() method loads the Probe from the desired path. By default it is loaded from the the cache directory (~/.cache/edsteva/probes). Predictor schema Data stored in predictor attribute follows a specific schema: Predictors It must include a completeness predictor \\(c(t)\\) : c : value of the completeness predictor \\(c(t)\\) . Then, it can have any other extra predictor you find useful such as: n_visit : the number of visits. Extra predictor The extra predictors must be additive to be aggregated properly in the dashboards. For instance, the number of visits is additive but the \\(99^{th}\\) percentile is not. Indexes It must include one and only one time related column: date : date of the event associated with the target variable (by default, the dates are truncated to the month in which the event occurs). It must include the following string type column : care_site_level : care site hierarchic level ( uf , pole , hospital ). care_site_id : care site unique identifier. care_site_short_name : care site short name used for visualization. Then, it can have any other string type column such as: stay_type : type of stay ( hospitalis\u00e9s , urgence , hospitalisation incompl\u00e8te , consultation externe ). note_type : type of note ( CRH , Ordonnance , CR Passage Urgences ). Example When considering the availability of clinical notes, a NoteProbe.predictor may for instance look like this: care_site_level care_site_id care_site_short_name stay_type note_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 'All' 2019-05-01 233.0 '0.841 Unit\u00e9 Fonctionnelle (UF) 8653815660 Care site 1 'All' 'CRH' 2011-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 'CRH' 2021-03-01 204.0 0.497 P\u00f4le/DMU 8312056379 Care site 2 'All' 'Ordonnance' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 'CR Passage Urgences' 2022-02-01 9746.0 0.769 Saving and loading a computed Probe In order to ease the future loading of a Probe that has been computed with the compute() method, one can pickle it using the save() method. This enables a rapid loading of the Probe from local disk using the load() method. from edsteva.probes import NoteProbe note = NoteProbe () note . compute ( data ) # (1) note . save () # (2) note_2 = NoteProbe () note_2 . load () # (3) Computation of the Probe querying the database (long). Saving of the Probe on the local disk. Rapid loading of the Probe fom the local disk. Defining a custom Probe If none of the available Probes meets your requirements, you may want to create your own. To define a custom Probe class CustomProbe that inherits from the abstract class BaseProbe you'll have to implement the compute_process() method (this method is natively called by the compute() method inherited by the BaseProbe class). You'll also have to define the _index attribute which is the list of columns that are used to aggregate the data in the compute_process() method. from edsteva.probes import BaseProbe # Definition of a new Probe class class CustomProbe ( BaseProbe ): _index = [ \"my_custom_column_1\" , \"my_custom_column_2\" ] def compute_process ( self , data : Data ): # query using Pandas API return custom_predictor compute_process() can take as much as argument as you need but it must include a data argument and must return a Pandas.DataFrame which contains at least the columns of the standard schema of a predictor . For a detailed example of the implementation of a Probe, please have a look on the implemented Probes such as VisitProbe or NoteProbe . Contributions If you managed to create your own Probe do not hesitate to share it with the community by following the contribution guidelines . Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Available Probes We list hereafter the Probes that have already been implemented in the library. VisitProbe NoteProbe The VisitProbe computes \\(c_{visit}(t)\\) the availability of administrative data related to visits for each care site according to time: \\[ c_{visit}(t) = \\frac{n_{visit}(t)}{n_{99}} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{99}\\) is the \\(99^{th}\\) percentile of visits and \\(t\\) is the month. If the \\(99^{th}\\) percentile of visits \\(n_{99}\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import VisitProbe visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, ) visit . predictor . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg' 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Hospit' 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.769 The NoteProbe computes \\(c_{note}(t)\\) the availability of clinical documents linked to patients' visits: \\[ c_{note}(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{with\\,doc}\\) the number of visits having at least one document and \\(t\\) is the month. If the number of visits \\(n_{visit}(t)\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import NoteProbe note = Note () note . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, note_types = { \"All\" : \".*\" , \"CRH\" : \"crh\" , \"Ordonnance\" : \"ordo\" , \"CR Passage Urgences\" : \"urge\" , }, ) note . predictor . head () care_site_level care_site_id care_site_short_name stay_type note_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 'All' 2019-05-01 233.0 '0.841 Unit\u00e9 Fonctionnelle (UF) 8653815660 Care site 1 'All' 'CRH' 2011-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Urg' 'CRH' 2021-03-01 204.0 0.497 P\u00f4le/DMU 8312056379 Care site 2 'All' 'Ordonnance' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Hospit' 'CR Passage Urgences' 2022-02-01 9746.0 0.769","title":"Probe"},{"location":"components/probe/#probe","text":"Choosing or customizing a Probe is the second step in the EDS-TeVa usage workflow .","title":"Probe"},{"location":"components/probe/#definition","text":"A Probe is a python class designed to characterize data availability of a target variable over time \\(t\\) . It aggregates the loaded data to obtain a completeness predictor \\(c(t)\\) . Probe class diagram","title":"Definition"},{"location":"components/probe/#input","text":"As detailled in the dedicated section , the Probe class is expecting a Data object with Pandas or Koalas DataFrames. We provide various connectors to facilitate data fetching, namely a Hive connector, a Postgres connector and a LocalData .","title":"Input"},{"location":"components/probe/#attributes","text":"predictor is a Pandas.DataFrame computed by the compute() method. It contains the desired completeness predictor \\(c(t)\\) for each column in the _index attribute (care site, stay type and any other needed column). _index is the list of columns that are used to aggregate the data in the compute() method.","title":"Attributes"},{"location":"components/probe/#methods","text":"compute() method calls the compute_process() method to compute the completeness predictors \\(c(t)\\) and store them in the predictor attribute. compute_process() method aggregates the input data to compute the completeness predictors \\(c(t)\\) . filter_care_site() method filters predictor attribute on the selected care sites including upper and lower levels care sites. save() method saves the Probe in the desired path. By default it is saved in the the cache directory (~/.cache/edsteva/probes). load() method loads the Probe from the desired path. By default it is loaded from the the cache directory (~/.cache/edsteva/probes).","title":"Methods"},{"location":"components/probe/#predictor-schema","text":"Data stored in predictor attribute follows a specific schema:","title":"Predictor schema"},{"location":"components/probe/#predictors","text":"It must include a completeness predictor \\(c(t)\\) : c : value of the completeness predictor \\(c(t)\\) . Then, it can have any other extra predictor you find useful such as: n_visit : the number of visits. Extra predictor The extra predictors must be additive to be aggregated properly in the dashboards. For instance, the number of visits is additive but the \\(99^{th}\\) percentile is not.","title":"Predictors"},{"location":"components/probe/#indexes","text":"It must include one and only one time related column: date : date of the event associated with the target variable (by default, the dates are truncated to the month in which the event occurs). It must include the following string type column : care_site_level : care site hierarchic level ( uf , pole , hospital ). care_site_id : care site unique identifier. care_site_short_name : care site short name used for visualization. Then, it can have any other string type column such as: stay_type : type of stay ( hospitalis\u00e9s , urgence , hospitalisation incompl\u00e8te , consultation externe ). note_type : type of note ( CRH , Ordonnance , CR Passage Urgences ).","title":"Indexes"},{"location":"components/probe/#example","text":"When considering the availability of clinical notes, a NoteProbe.predictor may for instance look like this: care_site_level care_site_id care_site_short_name stay_type note_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 'All' 2019-05-01 233.0 '0.841 Unit\u00e9 Fonctionnelle (UF) 8653815660 Care site 1 'All' 'CRH' 2011-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 'CRH' 2021-03-01 204.0 0.497 P\u00f4le/DMU 8312056379 Care site 2 'All' 'Ordonnance' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 'CR Passage Urgences' 2022-02-01 9746.0 0.769","title":"Example"},{"location":"components/probe/#saving-and-loading-a-computed-probe","text":"In order to ease the future loading of a Probe that has been computed with the compute() method, one can pickle it using the save() method. This enables a rapid loading of the Probe from local disk using the load() method. from edsteva.probes import NoteProbe note = NoteProbe () note . compute ( data ) # (1) note . save () # (2) note_2 = NoteProbe () note_2 . load () # (3) Computation of the Probe querying the database (long). Saving of the Probe on the local disk. Rapid loading of the Probe fom the local disk.","title":"Saving and loading a computed Probe"},{"location":"components/probe/#defining-a-custom-probe","text":"If none of the available Probes meets your requirements, you may want to create your own. To define a custom Probe class CustomProbe that inherits from the abstract class BaseProbe you'll have to implement the compute_process() method (this method is natively called by the compute() method inherited by the BaseProbe class). You'll also have to define the _index attribute which is the list of columns that are used to aggregate the data in the compute_process() method. from edsteva.probes import BaseProbe # Definition of a new Probe class class CustomProbe ( BaseProbe ): _index = [ \"my_custom_column_1\" , \"my_custom_column_2\" ] def compute_process ( self , data : Data ): # query using Pandas API return custom_predictor compute_process() can take as much as argument as you need but it must include a data argument and must return a Pandas.DataFrame which contains at least the columns of the standard schema of a predictor . For a detailed example of the implementation of a Probe, please have a look on the implemented Probes such as VisitProbe or NoteProbe . Contributions If you managed to create your own Probe do not hesitate to share it with the community by following the contribution guidelines . Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Defining a custom Probe"},{"location":"components/probe/#available-probes","text":"We list hereafter the Probes that have already been implemented in the library. VisitProbe NoteProbe The VisitProbe computes \\(c_{visit}(t)\\) the availability of administrative data related to visits for each care site according to time: \\[ c_{visit}(t) = \\frac{n_{visit}(t)}{n_{99}} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{99}\\) is the \\(99^{th}\\) percentile of visits and \\(t\\) is the month. If the \\(99^{th}\\) percentile of visits \\(n_{99}\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import VisitProbe visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, ) visit . predictor . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg' 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Hospit' 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.769 The NoteProbe computes \\(c_{note}(t)\\) the availability of clinical documents linked to patients' visits: \\[ c_{note}(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{with\\,doc}\\) the number of visits having at least one document and \\(t\\) is the month. If the number of visits \\(n_{visit}(t)\\) is equal to 0, we consider that the completeness predictor \\(c(t)\\) is also equal to 0. from edsteva.probes import NoteProbe note = Note () note . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg\" : \"urgence\" , \"Hospit\" : \"hospitalis\u00e9s\" , \"Urg_Hospit\" : \"urgence|hospitalis\u00e9s\" , }, note_types = { \"All\" : \".*\" , \"CRH\" : \"crh\" , \"Ordonnance\" : \"ordo\" , \"CR Passage Urgences\" : \"urge\" , }, ) note . predictor . head () care_site_level care_site_id care_site_short_name stay_type note_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 'All' 2019-05-01 233.0 '0.841 Unit\u00e9 Fonctionnelle (UF) 8653815660 Care site 1 'All' 'CRH' 2011-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 'Urg' 'CRH' 2021-03-01 204.0 0.497 P\u00f4le/DMU 8312056379 Care site 2 'All' 'Ordonnance' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 'Hospit' 'CR Passage Urgences' 2022-02-01 9746.0 0.769","title":"Available Probes"},{"location":"components/visualization/","text":"Visualization The fourth (and last) step in the EDS-TeVa usage workflow is setting the thresholds associated with the coefficients and the metrics of the Model fitted on the Probes . Definition The EDS-TeVa library provides dashboards and plots to visualize the temporal evolution of Probes along with fitted Models . Visualization functionalities can be used to explore the database and set thresholds relative to selection criteria. Visualization diagram Dashboard A Dashboard is an interactive Altair chart that lets you visualize variables aggregated by any combination of columns included in the Probe . In the library, the dashboards are divided into two parts: On the top, there is the plot of the aggregated variable of interest. On the bottom, there are interactive filters to set. Only the selected data is aggregated to produce the plot on the top. Plot A Plot is exportable in png or svg format and easy to integrate into a report. However, as it is less interactive it is preferred to specify the filters in the inputs of the functions. Available Visualizations Dashboard Plot The library provides interactive dashboards that let you set any combination of care sites, stay types and other columns if included in the Probe. You can only export a dashboard in HTML format. predictor_dashboard() estimates_dashboard() The predictor_dashboard() returns: On the top, the aggregated variable is the average completeness predictor \\(c(t)\\) over time \\(t\\) with the prediction \\(\\hat{c}(t)\\) if the fitted Model is specified. On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.). from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The estimates_dashboard() returns a representation of the overall deviation from the Model : On the top, the aggregated variable is a normalized completeness predictor \\(\\frac{c(t)}{c_0}\\) over normalized time \\(t - t_0\\) . On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.) with all the Model coefficients and metrics included in the Model . from edsteva.viz.dashboards import estimates_dashboard threshold_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The library provides static plots that you can export in png or svg. As it is less interactive, you may specify the filters in the inputs of the functions. plot_probe() plot_normalized_probe() plot_estimates_densities() The plot_probe() returns the top plot of the predictor_dashboard() without the interactive filters. Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_probe plot_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"../../assets/charts/fitted_visit.json\" } The plot_normalized_probe() returns the top plot of the estimates_dashboard() . Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_normalized_probe plot_normalized_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"../../assets/charts/normalized_plot.json\" } The plot_estimates_densities() returns the density plot and the median of each estimate. It can help you to set the thresholds. from edsteva.viz.plots import plot_estimates_densities plot_estimates_densities ( fitted_model = step_function_model , ) { \"schema-url\": \"../../assets/charts/distributions.json\" }","title":"Visualization"},{"location":"components/visualization/#visualization","text":"The fourth (and last) step in the EDS-TeVa usage workflow is setting the thresholds associated with the coefficients and the metrics of the Model fitted on the Probes .","title":"Visualization"},{"location":"components/visualization/#definition","text":"The EDS-TeVa library provides dashboards and plots to visualize the temporal evolution of Probes along with fitted Models . Visualization functionalities can be used to explore the database and set thresholds relative to selection criteria. Visualization diagram","title":"Definition"},{"location":"components/visualization/#dashboard","text":"A Dashboard is an interactive Altair chart that lets you visualize variables aggregated by any combination of columns included in the Probe . In the library, the dashboards are divided into two parts: On the top, there is the plot of the aggregated variable of interest. On the bottom, there are interactive filters to set. Only the selected data is aggregated to produce the plot on the top.","title":"Dashboard"},{"location":"components/visualization/#plot","text":"A Plot is exportable in png or svg format and easy to integrate into a report. However, as it is less interactive it is preferred to specify the filters in the inputs of the functions.","title":"Plot"},{"location":"components/visualization/#available-visualizations","text":"Dashboard Plot The library provides interactive dashboards that let you set any combination of care sites, stay types and other columns if included in the Probe. You can only export a dashboard in HTML format. predictor_dashboard() estimates_dashboard() The predictor_dashboard() returns: On the top, the aggregated variable is the average completeness predictor \\(c(t)\\) over time \\(t\\) with the prediction \\(\\hat{c}(t)\\) if the fitted Model is specified. On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.). from edsteva.viz.dashboards import predictor_dashboard predictor_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The estimates_dashboard() returns a representation of the overall deviation from the Model : On the top, the aggregated variable is a normalized completeness predictor \\(\\frac{c(t)}{c_0}\\) over normalized time \\(t - t_0\\) . On the bottom, the interactive filters are all the columns included in the Probe (such as time, care site, number of visits...etc.) with all the Model coefficients and metrics included in the Model . from edsteva.viz.dashboards import estimates_dashboard threshold_dashboard ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , ) An example is available here . The library provides static plots that you can export in png or svg. As it is less interactive, you may specify the filters in the inputs of the functions. plot_probe() plot_normalized_probe() plot_estimates_densities() The plot_probe() returns the top plot of the predictor_dashboard() without the interactive filters. Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_probe plot_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"../../assets/charts/fitted_visit.json\" } The plot_normalized_probe() returns the top plot of the estimates_dashboard() . Consequently, you have to specify the filters in the inputs of the function. from edsteva.viz.plots import plot_normalized_probe plot_normalized_probe ( probe = probe , fitted_model = step_function_model , care_site_level = care_site_level , stay_type = stay_type , save_path = plot_path , ) { \"schema-url\": \"../../assets/charts/normalized_plot.json\" } The plot_estimates_densities() returns the density plot and the median of each estimate. It can help you to set the thresholds. from edsteva.viz.plots import plot_estimates_densities plot_estimates_densities ( fitted_model = step_function_model , ) { \"schema-url\": \"../../assets/charts/distributions.json\" }","title":"Available Visualizations"},{"location":"mkdocs_theme/announce_bar/","text":"New features: Check out the new RectangleModel !","title":"Announce bar"},{"location":"reference/","text":"edsteva koalas_options koalas_options () -> None Set necessary options to optimise Koalas Source code in edsteva/__init__.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def koalas_options () -> None : \"\"\" Set necessary options to optimise Koalas \"\"\" # Reloading Koalas to use the new configuration ks = sys . modules . get ( \"databricks.koalas\" , None ) if ks is not None : importlib . reload ( ks ) else : import databricks.koalas as ks ks . set_option ( \"compute.default_index_type\" , \"distributed\" ) ks . set_option ( \"compute.ops_on_diff_frames\" , True ) improve_performances improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ] (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one RETURNS DESCRIPTION Tuple of - A SparkSession - The associated SparkContext - The associated Source code in edsteva/__init__.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ]: \"\"\" (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one Returns ------- Tuple of - A SparkSession - The associated SparkContext - The associated ``sql`` object to run SQL queries \"\"\" # Check if a spark Session is up global spark , sc , sql spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) conf = sc . getConf () # Synchronizing TimeZone tz = os . environ . get ( \"TZ\" , \"UTC\" ) os . environ [ \"TZ\" ] = tz time . tzset () to_add_conf . extend ( [ ( \"spark.app.name\" , f \" { os . environ . get ( 'USER' ) } _scikit\" ), ( \"spark.sql.session.timeZone\" , tz ), ( \"spark.sql.execution.arrow.enabled\" , \"true\" ), ( \"spark.sql.execution.arrow.pyspark.enabled\" , \"true\" ), ] ) for key , value in to_add_conf : conf . set ( key , value ) # Stopping context to add necessary env variables sc . stop () spark . stop () set_env_variables () spark = SparkSession . builder . enableHiveSupport () . config ( conf = conf ) . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) sql = spark . sql koalas_options () return spark , sc , sql","title":"`edsteva`"},{"location":"reference/#edsteva_1","text":"","title":"edsteva"},{"location":"reference/#edsteva.koalas_options","text":"koalas_options () -> None Set necessary options to optimise Koalas Source code in edsteva/__init__.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def koalas_options () -> None : \"\"\" Set necessary options to optimise Koalas \"\"\" # Reloading Koalas to use the new configuration ks = sys . modules . get ( \"databricks.koalas\" , None ) if ks is not None : importlib . reload ( ks ) else : import databricks.koalas as ks ks . set_option ( \"compute.default_index_type\" , \"distributed\" ) ks . set_option ( \"compute.ops_on_diff_frames\" , True )","title":"koalas_options()"},{"location":"reference/#edsteva.improve_performances","text":"improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ] (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one RETURNS DESCRIPTION Tuple of - A SparkSession - The associated SparkContext - The associated Source code in edsteva/__init__.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ]: \"\"\" (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one Returns ------- Tuple of - A SparkSession - The associated SparkContext - The associated ``sql`` object to run SQL queries \"\"\" # Check if a spark Session is up global spark , sc , sql spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) conf = sc . getConf () # Synchronizing TimeZone tz = os . environ . get ( \"TZ\" , \"UTC\" ) os . environ [ \"TZ\" ] = tz time . tzset () to_add_conf . extend ( [ ( \"spark.app.name\" , f \" { os . environ . get ( 'USER' ) } _scikit\" ), ( \"spark.sql.session.timeZone\" , tz ), ( \"spark.sql.execution.arrow.enabled\" , \"true\" ), ( \"spark.sql.execution.arrow.pyspark.enabled\" , \"true\" ), ] ) for key , value in to_add_conf : conf . set ( key , value ) # Stopping context to add necessary env variables sc . stop () spark . stop () set_env_variables () spark = SparkSession . builder . enableHiveSupport () . config ( conf = conf ) . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) sql = spark . sql koalas_options () return spark , sc , sql","title":"improve_performances()"},{"location":"reference/SUMMARY/","text":"edsteva io files hive i2b2_mapping postgres settings synthetic care_site synthetic utils visit metrics error error_after_t0 error_between_t0_t1 models base rectangle_function algos loss_minimization rectangle_function step_function algos loss_minimization quantile step_function probes base note utils visit utils checks framework loss_functions typing viz dashboards estimates_dashboard predictor_dashboard fitted_probe probe wrapper plots estimates_densities normalized_probe plot_probe fitted_probe probe wrapper utils","title":"SUMMARY"},{"location":"reference/io/","text":"edsteva.io","title":"`edsteva.io`"},{"location":"reference/io/#edstevaio","text":"","title":"edsteva.io"},{"location":"reference/io/files/","text":"edsteva.io.files LocalData Pandas interface to OMOP data stored as local parquet files/folders. PARAMETER DESCRIPTION folder absolute path to a folder containing several parquet files with omop data TYPE: str Examples: >>> data = LocalData ( folder = \"/export/home/USER/my_data/\" ) >>> person = data . person >>> person . shape (100, 10) ATTRIBUTE DESCRIPTION person Pandas dataframe person . All dataframe attributes are dynamically generated to match the content of the selected folder. TYPE: pd . DataFrame Source code in edsteva/io/files.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LocalData : # pragma: no cover \"\"\"Pandas interface to OMOP data stored as local parquet files/folders. Parameters ---------- folder: str absolute path to a folder containing several parquet files with omop data Examples -------- >>> data = LocalData(folder=\"/export/home/USER/my_data/\") >>> person = data.person >>> person.shape (100, 10) Attributes ---------- person: pd.DataFrame Pandas dataframe `person`. All dataframe attributes are dynamically generated to match the content of the selected folder. \"\"\" def __init__ ( self , folder : str , ): ( self . available_tables , self . tables_paths , self . available_omop_tables , ) = self . list_available_tables ( folder ) if not self . available_omop_tables : raise ValueError ( f \"Folder { folder } does not contain any parquet omop data.\" ) @staticmethod def list_available_tables ( folder : str ) -> Tuple [ List [ str ], List [ str ]]: available_tables = [] available_omop_tables = [] tables_paths = {} known_omop_tables = settings . tables_to_load . keys () for filename in os . listdir ( folder ): table_name , extension = os . path . splitext ( filename ) if extension == \".parquet\" : abspath = os . path . abspath ( os . path . join ( folder , filename )) tables_paths [ table_name ] = abspath available_tables . append ( table_name ) if table_name in known_omop_tables : available_omop_tables . append ( table_name ) return available_tables , tables_paths , available_omop_tables def _read_table ( self , table_name : str ) -> pd . DataFrame : path = self . tables_paths [ table_name ] return pd . read_parquet ( path ) def __getattr__ ( self , table_name : str ) -> pd . DataFrame : if table_name in self . available_tables : return self . _read_table ( table_name ) else : raise AttributeError ( f \"Table ' { table_name } ' does is not available in chosen folder.\" ) def __dir__ ( self ) -> List [ str ]: return list ( super () . __dir__ ()) + list ( self . available_tables )","title":"files"},{"location":"reference/io/files/#edstevaiofiles","text":"","title":"edsteva.io.files"},{"location":"reference/io/files/#edsteva.io.files.LocalData","text":"Pandas interface to OMOP data stored as local parquet files/folders. PARAMETER DESCRIPTION folder absolute path to a folder containing several parquet files with omop data TYPE: str Examples: >>> data = LocalData ( folder = \"/export/home/USER/my_data/\" ) >>> person = data . person >>> person . shape (100, 10) ATTRIBUTE DESCRIPTION person Pandas dataframe person . All dataframe attributes are dynamically generated to match the content of the selected folder. TYPE: pd . DataFrame Source code in edsteva/io/files.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LocalData : # pragma: no cover \"\"\"Pandas interface to OMOP data stored as local parquet files/folders. Parameters ---------- folder: str absolute path to a folder containing several parquet files with omop data Examples -------- >>> data = LocalData(folder=\"/export/home/USER/my_data/\") >>> person = data.person >>> person.shape (100, 10) Attributes ---------- person: pd.DataFrame Pandas dataframe `person`. All dataframe attributes are dynamically generated to match the content of the selected folder. \"\"\" def __init__ ( self , folder : str , ): ( self . available_tables , self . tables_paths , self . available_omop_tables , ) = self . list_available_tables ( folder ) if not self . available_omop_tables : raise ValueError ( f \"Folder { folder } does not contain any parquet omop data.\" ) @staticmethod def list_available_tables ( folder : str ) -> Tuple [ List [ str ], List [ str ]]: available_tables = [] available_omop_tables = [] tables_paths = {} known_omop_tables = settings . tables_to_load . keys () for filename in os . listdir ( folder ): table_name , extension = os . path . splitext ( filename ) if extension == \".parquet\" : abspath = os . path . abspath ( os . path . join ( folder , filename )) tables_paths [ table_name ] = abspath available_tables . append ( table_name ) if table_name in known_omop_tables : available_omop_tables . append ( table_name ) return available_tables , tables_paths , available_omop_tables def _read_table ( self , table_name : str ) -> pd . DataFrame : path = self . tables_paths [ table_name ] return pd . read_parquet ( path ) def __getattr__ ( self , table_name : str ) -> pd . DataFrame : if table_name in self . available_tables : return self . _read_table ( table_name ) else : raise AttributeError ( f \"Table ' { table_name } ' does is not available in chosen folder.\" ) def __dir__ ( self ) -> List [ str ]: return list ( super () . __dir__ ()) + list ( self . available_tables )","title":"LocalData"},{"location":"reference/io/hive/","text":"edsteva.io.hive HiveData Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. PARAMETER DESCRIPTION database_name The name of you database in Hive. Ex: \"cse_82727572\" TYPE: str database_type The type of your database. Must be \"OMOP\" or \"I2B2\" TYPE: str DEFAULT: 'OMOP' spark_session If None, a SparkSession will be retrieved or created via SparkSession.builder.enableHiveSupport().getOrCreate() TYPE: pyspark . sql . SparkSession , optional DEFAULT: None person_ids An iterable of person_id that is used to define a subset of the database. TYPE: Optional [ Iterable [ int ]], default DEFAULT: None tables_to_load By default (i.e. if tables_to_load is None ), loaded tables and columns loaded in each table are those listed in :py:data: ~edsteva.io.settings.tables_to_load . A dictionnary can be provided to complement those default settings. Keys should be table names to load, and values should be: - None to load all columns - A list of columns to load (or to add to the default loaded columns if the table is already loaded by default) TYPE: Optional [ Dict [ str , Optional [ List [ str ]]]], default DEFAULT: None ATTRIBUTE DESCRIPTION person Hive data for table person as a koalas dataframe. Other OMOP tables can also be accessed as attributes TYPE: koalas dataframe available_tables names of OMOP tables that can be accessed as attributes with this HiveData object. TYPE: list of str Examples: data = HiveData ( database_name = \"edsomop_prod_a\" ) data . available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data . person type ( person ) # Out: databricks.koalas.frame.DataFrame person [ \"person_id\" ] . count () # Out: 12670874 This class can be used to create a subset of data for a given list of person_id . This is useful because the smaller dataset can then be used to prototype more rapidly. my_person_ids = [ 9226726 , 2092082 , 5097816 ] data = HiveData ( spark_session = spark , database_name = \"edsomop_prod_a\" , person_ids = my_person_ids ) data . person [ \"person_id\" ] . count () # Out: 1000 tables_to_save = [ \"person\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( \"./cohort_sample_1000\" , table_names = tables_to_save ) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... Source code in edsteva/io/hive.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class HiveData : # pragma: no cover \"\"\"Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. Parameters ---------- database_name : str The name of you database in Hive. Ex: \"cse_82727572\" database_type : str The type of your database. Must be \"OMOP\" or \"I2B2\" spark_session : pyspark.sql.SparkSession, optional If None, a SparkSession will be retrieved or created via `SparkSession.builder.enableHiveSupport().getOrCreate()` person_ids : Optional[Iterable[int]], default: None An iterable of `person_id` that is used to define a subset of the database. tables_to_load : Optional[Dict[str, Optional[List[str]]]], default: None By default (i.e. if ``tables_to_load is None``), loaded tables and columns loaded in each table are those listed in :py:data:`~edsteva.io.settings.tables_to_load`. A dictionnary can be provided to complement those default settings. Keys should be table names to load, and values should be: - ``None`` to load all columns - A list of columns to load (or to add to the default loaded columns if the table is already loaded by default) Attributes ---------- person : koalas dataframe Hive data for table `person` as a koalas dataframe. Other OMOP tables can also be accessed as attributes available_tables : list of str names of OMOP tables that can be accessed as attributes with this HiveData object. Examples -------- ```python data = HiveData(database_name=\"edsomop_prod_a\") data.available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data.person type(person) # Out: databricks.koalas.frame.DataFrame person[\"person_id\"].count() # Out: 12670874 ``` This class can be used to create a subset of data for a given list of `person_id`. This is useful because the smaller dataset can then be used to prototype more rapidly. ```python my_person_ids = [9226726, 2092082, 5097816] data = HiveData( spark_session=spark, database_name=\"edsomop_prod_a\", person_ids=my_person_ids ) data.person[\"person_id\"].count() # Out: 1000 tables_to_save = [\"person\", \"visit_occurrence\"] data.persist_tables_to_folder(\"./cohort_sample_1000\", table_names=tables_to_save) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... ``` \"\"\" def __init__ ( self , database_name : str , database_type : str = \"OMOP\" , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , ): self . spark_session = ( spark_session if spark_session is not None else SparkSession . builder . enableHiveSupport () . getOrCreate () ) self . database_name = database_name self . database_type = database_type if self . database_type == \"I2B2\" : self . database_source = \"cse\" if \"cse\" in self . database_name else \"edsprod\" self . omop_to_i2b2 = settings . i2b2_tables [ self . database_source ] self . i2b2_to_omop = {} for omop_col , i2b2_col in self . omop_to_i2b2 . items (): if i2b2_col in self . i2b2_to_omop . keys (): self . i2b2_to_omop [ i2b2_col ] . append ( omop_col ) else : self . i2b2_to_omop [ i2b2_col ] = [ omop_col ] self . person_ids = self . _prepare_person_ids ( person_ids ) tmp_tables_to_load = settings . tables_to_load if isinstance ( tables_to_load , dict ): for table_name , columns in tables_to_load . items (): if columns is None : tmp_tables_to_load [ table_name ] = None else : tmp_tables_to_load [ table_name ] = list ( set ( tmp_tables_to_load . get ( table_name , []) + columns ) ) elif isinstance ( tables_to_load , list ): for table_name in tables_to_load : tmp_tables_to_load [ table_name ] = None self . tables_to_load = tmp_tables_to_load self . available_tables = self . list_available_tables () def list_available_tables ( self ) -> List [ str ]: tables_df = self . spark_session . sql ( f \"SHOW TABLES IN { self . database_name } \" ) . toPandas () available_tables = [] for table_name in tables_df [ \"tableName\" ] . drop_duplicates () . to_list (): if ( self . database_type == \"OMOP\" and table_name in self . tables_to_load . keys () ): available_tables . append ( table_name ) elif ( self . database_type == \"I2B2\" and table_name in self . i2b2_to_omop . keys () ): for omop_table in self . i2b2_to_omop [ table_name ]: if omop_table in self . tables_to_load . keys (): available_tables . append ( omop_table ) available_tables = list ( set ( available_tables )) return available_tables def rename_table ( self , old_table_name : str , new_table_name : str ) -> None : if old_table_name in self . available_tables : setattr ( self , new_table_name , getattr ( self , old_table_name )) self . available_tables . remove ( old_table_name ) self . available_tables . append ( new_table_name ) logger . info ( \"Table {} has been renamed {} \" , old_table_name , new_table_name ) else : logger . info ( \"Table {} is not available\" , old_table_name ) def add_table ( self , table_name : str , columns : List [ str ]) -> None : tables_df = self . spark_session . sql ( f \"SHOW TABLES IN { self . database_name } \" ) . toPandas () if table_name in tables_df [ \"tableName\" ] . drop_duplicates () . to_list (): self . tables_to_load [ table_name ] = list ( set ( self . tables_to_load . get ( table_name , []) + columns ) ) self . available_tables = self . list_available_tables () logger . info ( \"Table {} has been added\" , table_name ) else : raise AttributeError ( f \"Table ' { table_name } ' is not in the database ' { self . database_name } '\" ) def delete_table ( self , table_name : str ) -> None : self . tables_to_load . pop ( table_name , None ) self . available_tables = self . list_available_tables () logger . info ( \"Table {} has been deleted\" , table_name ) def _prepare_person_ids ( self , list_of_person_ids ) -> Optional [ SparkDataFrame ]: if list_of_person_ids is None : return None elif hasattr ( list_of_person_ids , \"to_list\" ): # Useful when list_of_person_ids are Koalas (or Pandas) Series unique_ids = set ( list_of_person_ids . to_list ()) else : unique_ids = set ( list_of_person_ids ) logger . info ( \"Number of unique patients: {} \" , len ( unique_ids )) schema = StructType ([ StructField ( \"person_id\" , LongType (), True )]) filtering_df = self . spark_session . createDataFrame ( [( int ( p ),) for p in unique_ids ], schema = schema ) return filtering_df def _read_table ( self , table_name , person_ids = None ) -> DataFrame : assert table_name in self . available_tables if person_ids is None and self . person_ids is not None : person_ids = self . person_ids if self . database_type == \"OMOP\" : df = self . spark_session . sql ( f \"select * from { self . database_name } . { table_name } \" ) elif self . database_type == \"I2B2\" : df = get_i2b2_table ( spark_session = self . spark_session , db_name = self . database_name , db_source = self . database_source , table = table_name , ) desired_columns = self . tables_to_load [ table_name ] selected_columns = ( df . columns if desired_columns is None else [ col for col in df . columns if col in desired_columns ] ) df = df . select ( * selected_columns ) if \"person_id\" in df . columns and person_ids is not None : df = df . join ( person_ids , on = \"person_id\" , how = \"inner\" ) return df . to_koalas () def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort tables : list of str, default None list of table names to save. Default value is :py:data:`~edsteva.io.settings.default_tables_to_save` \"\"\" if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) folder = os . path . abspath ( folder ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids ) for table in tables : filepath = os . path . join ( folder , f \" { table } .parquet\" ) df = self . _read_table ( table , person_ids = person_ids ) self . _write_df_to_parquet ( df , filepath ) def _write_df_to_parquet ( self , df : DataFrame , filepath : str , ) -> None : assert os . path . isabs ( filepath ) print ( f \"writing { filepath } \" ) spark_filepath = \"file://\" + filepath df . to_parquet ( spark_filepath , mode = \"overwrite\" ) def __getattr__ ( self , table_name : str ) -> DataFrame : if table_name in self . available_tables : # the first time it is called, we actually set the attribute table = self . _read_table ( table_name ) setattr ( self , table_name , table ) return getattr ( self , table_name ) else : raise AttributeError ( f \"Table ' { table_name } ' unknown\" ) def __dir__ ( self ) -> List [ str ]: return list ( set ( list ( super () . __dir__ ()) + self . available_tables )) persist_tables_to_folder persist_tables_to_folder ( folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , ) -> None Save OMOP tables as parquet files in a given folder. PARAMETER DESCRIPTION folder path to folder where the tables will be written. TYPE: str person_ids person_ids to keep in the subcohort TYPE: iterable DEFAULT: None tables list of table names to save. Default value is :py:data: ~edsteva.io.settings.default_tables_to_save TYPE: list of str, default None DEFAULT: None Source code in edsteva/io/hive.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort tables : list of str, default None list of table names to save. Default value is :py:data:`~edsteva.io.settings.default_tables_to_save` \"\"\" if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) folder = os . path . abspath ( folder ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids ) for table in tables : filepath = os . path . join ( folder , f \" { table } .parquet\" ) df = self . _read_table ( table , person_ids = person_ids ) self . _write_df_to_parquet ( df , filepath )","title":"hive"},{"location":"reference/io/hive/#edstevaiohive","text":"","title":"edsteva.io.hive"},{"location":"reference/io/hive/#edsteva.io.hive.HiveData","text":"Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. PARAMETER DESCRIPTION database_name The name of you database in Hive. Ex: \"cse_82727572\" TYPE: str database_type The type of your database. Must be \"OMOP\" or \"I2B2\" TYPE: str DEFAULT: 'OMOP' spark_session If None, a SparkSession will be retrieved or created via SparkSession.builder.enableHiveSupport().getOrCreate() TYPE: pyspark . sql . SparkSession , optional DEFAULT: None person_ids An iterable of person_id that is used to define a subset of the database. TYPE: Optional [ Iterable [ int ]], default DEFAULT: None tables_to_load By default (i.e. if tables_to_load is None ), loaded tables and columns loaded in each table are those listed in :py:data: ~edsteva.io.settings.tables_to_load . A dictionnary can be provided to complement those default settings. Keys should be table names to load, and values should be: - None to load all columns - A list of columns to load (or to add to the default loaded columns if the table is already loaded by default) TYPE: Optional [ Dict [ str , Optional [ List [ str ]]]], default DEFAULT: None ATTRIBUTE DESCRIPTION person Hive data for table person as a koalas dataframe. Other OMOP tables can also be accessed as attributes TYPE: koalas dataframe available_tables names of OMOP tables that can be accessed as attributes with this HiveData object. TYPE: list of str Examples: data = HiveData ( database_name = \"edsomop_prod_a\" ) data . available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data . person type ( person ) # Out: databricks.koalas.frame.DataFrame person [ \"person_id\" ] . count () # Out: 12670874 This class can be used to create a subset of data for a given list of person_id . This is useful because the smaller dataset can then be used to prototype more rapidly. my_person_ids = [ 9226726 , 2092082 , 5097816 ] data = HiveData ( spark_session = spark , database_name = \"edsomop_prod_a\" , person_ids = my_person_ids ) data . person [ \"person_id\" ] . count () # Out: 1000 tables_to_save = [ \"person\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( \"./cohort_sample_1000\" , table_names = tables_to_save ) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... Source code in edsteva/io/hive.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class HiveData : # pragma: no cover \"\"\"Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. Parameters ---------- database_name : str The name of you database in Hive. Ex: \"cse_82727572\" database_type : str The type of your database. Must be \"OMOP\" or \"I2B2\" spark_session : pyspark.sql.SparkSession, optional If None, a SparkSession will be retrieved or created via `SparkSession.builder.enableHiveSupport().getOrCreate()` person_ids : Optional[Iterable[int]], default: None An iterable of `person_id` that is used to define a subset of the database. tables_to_load : Optional[Dict[str, Optional[List[str]]]], default: None By default (i.e. if ``tables_to_load is None``), loaded tables and columns loaded in each table are those listed in :py:data:`~edsteva.io.settings.tables_to_load`. A dictionnary can be provided to complement those default settings. Keys should be table names to load, and values should be: - ``None`` to load all columns - A list of columns to load (or to add to the default loaded columns if the table is already loaded by default) Attributes ---------- person : koalas dataframe Hive data for table `person` as a koalas dataframe. Other OMOP tables can also be accessed as attributes available_tables : list of str names of OMOP tables that can be accessed as attributes with this HiveData object. Examples -------- ```python data = HiveData(database_name=\"edsomop_prod_a\") data.available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data.person type(person) # Out: databricks.koalas.frame.DataFrame person[\"person_id\"].count() # Out: 12670874 ``` This class can be used to create a subset of data for a given list of `person_id`. This is useful because the smaller dataset can then be used to prototype more rapidly. ```python my_person_ids = [9226726, 2092082, 5097816] data = HiveData( spark_session=spark, database_name=\"edsomop_prod_a\", person_ids=my_person_ids ) data.person[\"person_id\"].count() # Out: 1000 tables_to_save = [\"person\", \"visit_occurrence\"] data.persist_tables_to_folder(\"./cohort_sample_1000\", table_names=tables_to_save) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... ``` \"\"\" def __init__ ( self , database_name : str , database_type : str = \"OMOP\" , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , ): self . spark_session = ( spark_session if spark_session is not None else SparkSession . builder . enableHiveSupport () . getOrCreate () ) self . database_name = database_name self . database_type = database_type if self . database_type == \"I2B2\" : self . database_source = \"cse\" if \"cse\" in self . database_name else \"edsprod\" self . omop_to_i2b2 = settings . i2b2_tables [ self . database_source ] self . i2b2_to_omop = {} for omop_col , i2b2_col in self . omop_to_i2b2 . items (): if i2b2_col in self . i2b2_to_omop . keys (): self . i2b2_to_omop [ i2b2_col ] . append ( omop_col ) else : self . i2b2_to_omop [ i2b2_col ] = [ omop_col ] self . person_ids = self . _prepare_person_ids ( person_ids ) tmp_tables_to_load = settings . tables_to_load if isinstance ( tables_to_load , dict ): for table_name , columns in tables_to_load . items (): if columns is None : tmp_tables_to_load [ table_name ] = None else : tmp_tables_to_load [ table_name ] = list ( set ( tmp_tables_to_load . get ( table_name , []) + columns ) ) elif isinstance ( tables_to_load , list ): for table_name in tables_to_load : tmp_tables_to_load [ table_name ] = None self . tables_to_load = tmp_tables_to_load self . available_tables = self . list_available_tables () def list_available_tables ( self ) -> List [ str ]: tables_df = self . spark_session . sql ( f \"SHOW TABLES IN { self . database_name } \" ) . toPandas () available_tables = [] for table_name in tables_df [ \"tableName\" ] . drop_duplicates () . to_list (): if ( self . database_type == \"OMOP\" and table_name in self . tables_to_load . keys () ): available_tables . append ( table_name ) elif ( self . database_type == \"I2B2\" and table_name in self . i2b2_to_omop . keys () ): for omop_table in self . i2b2_to_omop [ table_name ]: if omop_table in self . tables_to_load . keys (): available_tables . append ( omop_table ) available_tables = list ( set ( available_tables )) return available_tables def rename_table ( self , old_table_name : str , new_table_name : str ) -> None : if old_table_name in self . available_tables : setattr ( self , new_table_name , getattr ( self , old_table_name )) self . available_tables . remove ( old_table_name ) self . available_tables . append ( new_table_name ) logger . info ( \"Table {} has been renamed {} \" , old_table_name , new_table_name ) else : logger . info ( \"Table {} is not available\" , old_table_name ) def add_table ( self , table_name : str , columns : List [ str ]) -> None : tables_df = self . spark_session . sql ( f \"SHOW TABLES IN { self . database_name } \" ) . toPandas () if table_name in tables_df [ \"tableName\" ] . drop_duplicates () . to_list (): self . tables_to_load [ table_name ] = list ( set ( self . tables_to_load . get ( table_name , []) + columns ) ) self . available_tables = self . list_available_tables () logger . info ( \"Table {} has been added\" , table_name ) else : raise AttributeError ( f \"Table ' { table_name } ' is not in the database ' { self . database_name } '\" ) def delete_table ( self , table_name : str ) -> None : self . tables_to_load . pop ( table_name , None ) self . available_tables = self . list_available_tables () logger . info ( \"Table {} has been deleted\" , table_name ) def _prepare_person_ids ( self , list_of_person_ids ) -> Optional [ SparkDataFrame ]: if list_of_person_ids is None : return None elif hasattr ( list_of_person_ids , \"to_list\" ): # Useful when list_of_person_ids are Koalas (or Pandas) Series unique_ids = set ( list_of_person_ids . to_list ()) else : unique_ids = set ( list_of_person_ids ) logger . info ( \"Number of unique patients: {} \" , len ( unique_ids )) schema = StructType ([ StructField ( \"person_id\" , LongType (), True )]) filtering_df = self . spark_session . createDataFrame ( [( int ( p ),) for p in unique_ids ], schema = schema ) return filtering_df def _read_table ( self , table_name , person_ids = None ) -> DataFrame : assert table_name in self . available_tables if person_ids is None and self . person_ids is not None : person_ids = self . person_ids if self . database_type == \"OMOP\" : df = self . spark_session . sql ( f \"select * from { self . database_name } . { table_name } \" ) elif self . database_type == \"I2B2\" : df = get_i2b2_table ( spark_session = self . spark_session , db_name = self . database_name , db_source = self . database_source , table = table_name , ) desired_columns = self . tables_to_load [ table_name ] selected_columns = ( df . columns if desired_columns is None else [ col for col in df . columns if col in desired_columns ] ) df = df . select ( * selected_columns ) if \"person_id\" in df . columns and person_ids is not None : df = df . join ( person_ids , on = \"person_id\" , how = \"inner\" ) return df . to_koalas () def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort tables : list of str, default None list of table names to save. Default value is :py:data:`~edsteva.io.settings.default_tables_to_save` \"\"\" if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) folder = os . path . abspath ( folder ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids ) for table in tables : filepath = os . path . join ( folder , f \" { table } .parquet\" ) df = self . _read_table ( table , person_ids = person_ids ) self . _write_df_to_parquet ( df , filepath ) def _write_df_to_parquet ( self , df : DataFrame , filepath : str , ) -> None : assert os . path . isabs ( filepath ) print ( f \"writing { filepath } \" ) spark_filepath = \"file://\" + filepath df . to_parquet ( spark_filepath , mode = \"overwrite\" ) def __getattr__ ( self , table_name : str ) -> DataFrame : if table_name in self . available_tables : # the first time it is called, we actually set the attribute table = self . _read_table ( table_name ) setattr ( self , table_name , table ) return getattr ( self , table_name ) else : raise AttributeError ( f \"Table ' { table_name } ' unknown\" ) def __dir__ ( self ) -> List [ str ]: return list ( set ( list ( super () . __dir__ ()) + self . available_tables ))","title":"HiveData"},{"location":"reference/io/hive/#edsteva.io.hive.HiveData.persist_tables_to_folder","text":"persist_tables_to_folder ( folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , ) -> None Save OMOP tables as parquet files in a given folder. PARAMETER DESCRIPTION folder path to folder where the tables will be written. TYPE: str person_ids person_ids to keep in the subcohort TYPE: iterable DEFAULT: None tables list of table names to save. Default value is :py:data: ~edsteva.io.settings.default_tables_to_save TYPE: list of str, default None DEFAULT: None Source code in edsteva/io/hive.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort tables : list of str, default None list of table names to save. Default value is :py:data:`~edsteva.io.settings.default_tables_to_save` \"\"\" if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) folder = os . path . abspath ( folder ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids ) for table in tables : filepath = os . path . join ( folder , f \" { table } .parquet\" ) df = self . _read_table ( table , person_ids = person_ids ) self . _write_df_to_parquet ( df , filepath )","title":"persist_tables_to_folder()"},{"location":"reference/io/i2b2_mapping/","text":"edsteva.io.i2b2_mapping get_i2b2_table get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str , ) -> SparkDataFrame Retrieve a Spark table in i2b2 and transform it to fit with OMOP standard. PARAMETER DESCRIPTION db_name Name of the database where the data is stored. TYPE: str table Name of the table to extract. TYPE: str RETURNS DESCRIPTION df Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. TYPE: Spark DataFrame Source code in edsteva/io/i2b2_mapping.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str ) -> SparkDataFrame : \"\"\" Retrieve a Spark table in i2b2 and transform it to fit with OMOP standard. Parameters ---------- db_name: str Name of the database where the data is stored. table: str Name of the table to extract. Returns ------- df: Spark DataFrame Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. \"\"\" table_name = i2b2_tables [ db_source ][ table ] columns = i2b2_renaming [ table ] if db_source == \"cse\" : columns . pop ( \"i2b2_action\" , None ) query = \",\" . join ([ \" {k} AS {v} \" . format ( k = k , v = v ) for k , v in columns . items ()]) df = spark_session . sql ( f \"\"\"SELECT { query } FROM { db_name } . { table_name } \"\"\" ) # Special mapping for i2b2 : # CIM10 if table == \"condition_occurrence\" : df = df . withColumn ( \"condition_source_value\" , F . substring ( F . col ( \"condition_source_value\" ), 7 , 20 ), ) # CCAM elif table == \"procedure_occurrence\" : df = df . withColumn ( \"procedure_source_value\" , F . substring ( F . col ( \"procedure_source_value\" ), 6 , 20 ), ) # Visits elif table == \"visit_occurrence\" : df = df . withColumn ( \"visit_source_value\" , mapping_dict ( visit_type_mapping , \"Non Renseign\u00e9\" )( F . col ( \"visit_source_value\" ) ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) df = df . withColumn ( \"visit_occurrence_source_value\" , df [ \"visit_occurrence_id\" ] ) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) . isin ([ - 1 , - 2 ]), \"supprim\u00e9\" ) . otherwise ( \"Actif\" ), ) # Retrieve Hospital trigram ufr = spark_session . sql ( f \"SELECT * FROM { db_name } . { i2b2_tables [ db_source ][ 'visit_detail' ] } \" ) ufr = ufr . withColumn ( \"care_site_id\" , F . substring ( F . split ( F . col ( \"concept_cd\" ), \":\" ) . getItem ( 1 ), 1 , 3 ), ) ufr = ufr . withColumnRenamed ( \"encounter_num\" , \"visit_occurrence_id\" ) ufr = ufr . drop_duplicates ( subset = [ \"visit_occurrence_id\" ]) ufr = ufr . select ([ \"visit_occurrence_id\" , \"care_site_id\" ]) df = df . join ( ufr , how = \"inner\" , on = [ \"visit_occurrence_id\" ]) # Patients elif table == \"person\" : df = df . withColumn ( \"gender_source_value\" , mapping_dict ( sex_cd_mapping , \"Non Renseign\u00e9\" )( F . col ( \"gender_source_value\" )), ) # Documents elif table == \"note\" : df = df . withColumn ( \"note_class_source_value\" , F . substring ( F . col ( \"note_class_source_value\" ), 4 , 100 ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) < 0 , \"SUPP\" ) . otherwise ( \"Actif\" ), ) # Hospital trigrams elif table == \"care_site\" : df = df . withColumn ( \"care_site_type_source_value\" , F . lit ( \"H\u00f4pital\" )) df = df . withColumn ( \"care_site_source_value\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"care_site_id\" , F . substring ( F . col ( \"care_site_source_value\" ), 1 , 3 ) ) df = df . drop_duplicates ( subset = [ \"care_site_id\" ]) df = df . withColumn ( \"care_site_short_name\" , mapping_dict ( dict_code_UFR , \"Non Renseign\u00e9\" )( F . col ( \"care_site_id\" )), ) # UFR elif table == \"visit_detail\" : df = df . withColumn ( \"care_site_id\" , F . split ( F . col ( \"care_site_id\" ), \":\" ) . getItem ( 1 ) ) df = df . withColumn ( \"visit_detail_type_source_value\" , F . lit ( \"PASS\" )) df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) # biology elif table == \"biology\" : df = df . withColumn ( \"biology_source_value\" , F . substring ( F . col ( \"biology_source_value\" ), 5 , 20 ) ) # fact_relationship elif table == \"fact_relationship\" : # Retrieve UF information df = df . withColumn ( \"fact_id_1\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"domain_concept_id_1\" , F . lit ( 57 )) # Care_site domain # Retrieve hospital information df = df . withColumn ( \"fact_id_2\" , F . substring ( F . col ( \"fact_id_1\" ), 1 , 3 )) df = df . withColumn ( \"domain_concept_id_2\" , F . lit ( 57 )) # Care_site domain df = df . drop_duplicates ( subset = [ \"fact_id_1\" , \"fact_id_2\" ]) # Only UF-Hospital relationships in i2b2 df = df . withColumn ( \"relationship_concept_id\" , F . lit ( 46233688 )) # Included in return df mapping_dict mapping_dict ( mapping : Dict [ str , str ], Non_renseigne : str ) -> FunctionUDF Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. PARAMETER DESCRIPTION mapping Mapping dictionnary TYPE: Dict [ str , str ] Non_renseigne Value to return if the function input is not find in the mapping dictionnary. TYPE: str RETURNS DESCRIPTION Callable Function that maps the values of Spark DataFrame column. Source code in edsteva/io/i2b2_mapping.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def mapping_dict ( mapping : Dict [ str , str ], Non_renseigne : str ) -> FunctionUDF : \"\"\" Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. Parameters ---------- mapping: Dict Mapping dictionnary Non_renseigne: str Value to return if the function input is not find in the mapping dictionnary. Returns ------- Callable Function that maps the values of Spark DataFrame column. \"\"\" def f ( x ): if x in mapping : return mapping . get ( x ) return Non_renseigne return F . udf ( f )","title":"i2b2_mapping"},{"location":"reference/io/i2b2_mapping/#edstevaioi2b2_mapping","text":"","title":"edsteva.io.i2b2_mapping"},{"location":"reference/io/i2b2_mapping/#edsteva.io.i2b2_mapping.get_i2b2_table","text":"get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str , ) -> SparkDataFrame Retrieve a Spark table in i2b2 and transform it to fit with OMOP standard. PARAMETER DESCRIPTION db_name Name of the database where the data is stored. TYPE: str table Name of the table to extract. TYPE: str RETURNS DESCRIPTION df Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. TYPE: Spark DataFrame Source code in edsteva/io/i2b2_mapping.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str ) -> SparkDataFrame : \"\"\" Retrieve a Spark table in i2b2 and transform it to fit with OMOP standard. Parameters ---------- db_name: str Name of the database where the data is stored. table: str Name of the table to extract. Returns ------- df: Spark DataFrame Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. \"\"\" table_name = i2b2_tables [ db_source ][ table ] columns = i2b2_renaming [ table ] if db_source == \"cse\" : columns . pop ( \"i2b2_action\" , None ) query = \",\" . join ([ \" {k} AS {v} \" . format ( k = k , v = v ) for k , v in columns . items ()]) df = spark_session . sql ( f \"\"\"SELECT { query } FROM { db_name } . { table_name } \"\"\" ) # Special mapping for i2b2 : # CIM10 if table == \"condition_occurrence\" : df = df . withColumn ( \"condition_source_value\" , F . substring ( F . col ( \"condition_source_value\" ), 7 , 20 ), ) # CCAM elif table == \"procedure_occurrence\" : df = df . withColumn ( \"procedure_source_value\" , F . substring ( F . col ( \"procedure_source_value\" ), 6 , 20 ), ) # Visits elif table == \"visit_occurrence\" : df = df . withColumn ( \"visit_source_value\" , mapping_dict ( visit_type_mapping , \"Non Renseign\u00e9\" )( F . col ( \"visit_source_value\" ) ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) df = df . withColumn ( \"visit_occurrence_source_value\" , df [ \"visit_occurrence_id\" ] ) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) . isin ([ - 1 , - 2 ]), \"supprim\u00e9\" ) . otherwise ( \"Actif\" ), ) # Retrieve Hospital trigram ufr = spark_session . sql ( f \"SELECT * FROM { db_name } . { i2b2_tables [ db_source ][ 'visit_detail' ] } \" ) ufr = ufr . withColumn ( \"care_site_id\" , F . substring ( F . split ( F . col ( \"concept_cd\" ), \":\" ) . getItem ( 1 ), 1 , 3 ), ) ufr = ufr . withColumnRenamed ( \"encounter_num\" , \"visit_occurrence_id\" ) ufr = ufr . drop_duplicates ( subset = [ \"visit_occurrence_id\" ]) ufr = ufr . select ([ \"visit_occurrence_id\" , \"care_site_id\" ]) df = df . join ( ufr , how = \"inner\" , on = [ \"visit_occurrence_id\" ]) # Patients elif table == \"person\" : df = df . withColumn ( \"gender_source_value\" , mapping_dict ( sex_cd_mapping , \"Non Renseign\u00e9\" )( F . col ( \"gender_source_value\" )), ) # Documents elif table == \"note\" : df = df . withColumn ( \"note_class_source_value\" , F . substring ( F . col ( \"note_class_source_value\" ), 4 , 100 ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) < 0 , \"SUPP\" ) . otherwise ( \"Actif\" ), ) # Hospital trigrams elif table == \"care_site\" : df = df . withColumn ( \"care_site_type_source_value\" , F . lit ( \"H\u00f4pital\" )) df = df . withColumn ( \"care_site_source_value\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"care_site_id\" , F . substring ( F . col ( \"care_site_source_value\" ), 1 , 3 ) ) df = df . drop_duplicates ( subset = [ \"care_site_id\" ]) df = df . withColumn ( \"care_site_short_name\" , mapping_dict ( dict_code_UFR , \"Non Renseign\u00e9\" )( F . col ( \"care_site_id\" )), ) # UFR elif table == \"visit_detail\" : df = df . withColumn ( \"care_site_id\" , F . split ( F . col ( \"care_site_id\" ), \":\" ) . getItem ( 1 ) ) df = df . withColumn ( \"visit_detail_type_source_value\" , F . lit ( \"PASS\" )) df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) # biology elif table == \"biology\" : df = df . withColumn ( \"biology_source_value\" , F . substring ( F . col ( \"biology_source_value\" ), 5 , 20 ) ) # fact_relationship elif table == \"fact_relationship\" : # Retrieve UF information df = df . withColumn ( \"fact_id_1\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"domain_concept_id_1\" , F . lit ( 57 )) # Care_site domain # Retrieve hospital information df = df . withColumn ( \"fact_id_2\" , F . substring ( F . col ( \"fact_id_1\" ), 1 , 3 )) df = df . withColumn ( \"domain_concept_id_2\" , F . lit ( 57 )) # Care_site domain df = df . drop_duplicates ( subset = [ \"fact_id_1\" , \"fact_id_2\" ]) # Only UF-Hospital relationships in i2b2 df = df . withColumn ( \"relationship_concept_id\" , F . lit ( 46233688 )) # Included in return df","title":"get_i2b2_table()"},{"location":"reference/io/i2b2_mapping/#edsteva.io.i2b2_mapping.mapping_dict","text":"mapping_dict ( mapping : Dict [ str , str ], Non_renseigne : str ) -> FunctionUDF Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. PARAMETER DESCRIPTION mapping Mapping dictionnary TYPE: Dict [ str , str ] Non_renseigne Value to return if the function input is not find in the mapping dictionnary. TYPE: str RETURNS DESCRIPTION Callable Function that maps the values of Spark DataFrame column. Source code in edsteva/io/i2b2_mapping.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def mapping_dict ( mapping : Dict [ str , str ], Non_renseigne : str ) -> FunctionUDF : \"\"\" Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. Parameters ---------- mapping: Dict Mapping dictionnary Non_renseigne: str Value to return if the function input is not find in the mapping dictionnary. Returns ------- Callable Function that maps the values of Spark DataFrame column. \"\"\" def f ( x ): if x in mapping : return mapping . get ( x ) return Non_renseigne return F . udf ( f )","title":"mapping_dict()"},{"location":"reference/io/postgres/","text":"edsteva.io.postgres PostgresData PostgreSQL interface to run SQL queries. This uses the file ~/.pgpass to find the password and extra connection infos. PARAMETER DESCRIPTION dbname TYPE: str DEFAULT: None schema TYPE: str DEFAULT: None user TYPE: str DEFAULT: None host TYPE: str DEFAULT: None port TYPE: str DEFAULT: None Examples: >>> data = PostgresData ( dbname = \"YOUR_DBNAME\" , schema = \"omop\" , user = \"YOUR_USERNAME\" ) >>> data . read_sql ( \"select count(*) from person\" ) Source code in edsteva/io/postgres.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class PostgresData : # pragma: no cover \"\"\"PostgreSQL interface to run SQL queries. This uses the file `~/.pgpass` to find the password and extra connection infos. Parameters ---------- dbname : str schema : str user : str host : str port : str Examples -------- >>> data = PostgresData(dbname=\"YOUR_DBNAME\", schema=\"omop\", user=\"YOUR_USERNAME\") >>> data.read_sql(\"select count(*) from person\") \"\"\" def __init__ ( self , dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None , ): ( self . host , self . port , self . dbname , self . user , ) = self . _find_matching_pgpass_params ( host , port , dbname , user ) self . schema = schema @staticmethod def _find_matching_pgpass_params ( host : str , port : int , dbname : str , user : str , ) -> Tuple : entries = pgpasslib . _get_entries () consolidated_params = [ ( host or entry . host , port or entry . port , dbname or entry . dbname , user or entry . user , ) for entry in entries ] matching_params = [ params for entry , params in zip ( entries , consolidated_params ) if entry . match ( * params ) ] if len ( matching_params ) == 0 : raise ValueError ( \"Could not find matching entry in .pgpass file.\" ) if len ( matching_params ) > 1 : message = \" \\n \" . join ( [ \"Several entries found in .pgpass file. Be more specific.\" , \"The following entries match what you specified :\" , * [ str ( params ) for params in matching_params ], ] ) raise ValueError ( message ) return matching_params [ 0 ] def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df read_sql read_sql ( sql_query : str , ** kwargs ) -> pd . DataFrame Execute pandas.read_sql() on the database. PARAMETER DESCRIPTION sql_query SQL query (postgres flavor) TYPE: str **kwargs additional arguments passed to pandas.read_sql() DEFAULT: {} RETURNS DESCRIPTION df TYPE: pandas . DataFrame Source code in edsteva/io/postgres.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df","title":"postgres"},{"location":"reference/io/postgres/#edstevaiopostgres","text":"","title":"edsteva.io.postgres"},{"location":"reference/io/postgres/#edsteva.io.postgres.PostgresData","text":"PostgreSQL interface to run SQL queries. This uses the file ~/.pgpass to find the password and extra connection infos. PARAMETER DESCRIPTION dbname TYPE: str DEFAULT: None schema TYPE: str DEFAULT: None user TYPE: str DEFAULT: None host TYPE: str DEFAULT: None port TYPE: str DEFAULT: None Examples: >>> data = PostgresData ( dbname = \"YOUR_DBNAME\" , schema = \"omop\" , user = \"YOUR_USERNAME\" ) >>> data . read_sql ( \"select count(*) from person\" ) Source code in edsteva/io/postgres.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class PostgresData : # pragma: no cover \"\"\"PostgreSQL interface to run SQL queries. This uses the file `~/.pgpass` to find the password and extra connection infos. Parameters ---------- dbname : str schema : str user : str host : str port : str Examples -------- >>> data = PostgresData(dbname=\"YOUR_DBNAME\", schema=\"omop\", user=\"YOUR_USERNAME\") >>> data.read_sql(\"select count(*) from person\") \"\"\" def __init__ ( self , dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None , ): ( self . host , self . port , self . dbname , self . user , ) = self . _find_matching_pgpass_params ( host , port , dbname , user ) self . schema = schema @staticmethod def _find_matching_pgpass_params ( host : str , port : int , dbname : str , user : str , ) -> Tuple : entries = pgpasslib . _get_entries () consolidated_params = [ ( host or entry . host , port or entry . port , dbname or entry . dbname , user or entry . user , ) for entry in entries ] matching_params = [ params for entry , params in zip ( entries , consolidated_params ) if entry . match ( * params ) ] if len ( matching_params ) == 0 : raise ValueError ( \"Could not find matching entry in .pgpass file.\" ) if len ( matching_params ) > 1 : message = \" \\n \" . join ( [ \"Several entries found in .pgpass file. Be more specific.\" , \"The following entries match what you specified :\" , * [ str ( params ) for params in matching_params ], ] ) raise ValueError ( message ) return matching_params [ 0 ] def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df","title":"PostgresData"},{"location":"reference/io/postgres/#edsteva.io.postgres.PostgresData.read_sql","text":"read_sql ( sql_query : str , ** kwargs ) -> pd . DataFrame Execute pandas.read_sql() on the database. PARAMETER DESCRIPTION sql_query SQL query (postgres flavor) TYPE: str **kwargs additional arguments passed to pandas.read_sql() DEFAULT: {} RETURNS DESCRIPTION df TYPE: pandas . DataFrame Source code in edsteva/io/postgres.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df","title":"read_sql()"},{"location":"reference/io/settings/","text":"edsteva.io.settings default_tables_to_save module-attribute default_tables_to_save = [ \"person\" , \"visit_occurrence\" , \"visit_detail\" , \"condition_occurrence\" , \"procedure_occurrence\" , \"care_site\" , \"concept\" , ] The default tables loaded when instanciating a HiveData or a PostgresData tables_to_load module-attribute tables_to_load = { \"person\" : [ \"person_id\" , \"location_id\" , \"year_of_birth\" , \"month_of_birth\" , \"day_of_birth\" , \"birth_datetime\" , \"death_datetime\" , \"gender_source_value\" , \"gender_source_concept_id\" , \"cdm_source\" , ], \"visit_occurrence\" : [ \"visit_occurrence_id\" , \"person_id\" , \"visit_occurrence_source_value\" , \"preceding_visit_occurrence_id\" , \"care_site_id\" , \"visit_start_datetime\" , \"visit_end_datetime\" , \"visit_source_value\" , \"visit_source_concept_id\" , \"visit_type_source_value\" , \"visit_type_source_concept_id\" , \"admitted_from_source_value\" , \"admitted_from_source_concept_id\" , \"discharge_to_source_value\" , \"discharge_to_source_concept_id\" , \"row_status_source_value\" , \"stay_source_value\" , \"stay_source_concept_id\" , \"cdm_source\" , ], \"care_site\" : [ \"care_site_id\" , \"care_site_source_value\" , \"care_site_name\" , \"care_site_short_name\" , \"place_of_service_source_value\" , \"care_site_type_source_value\" , \"valid_start_date\" , \"valid_end_date\" , ], \"visit_detail\" : [ \"visit_detail_id\" , \"visit_occurrence_id\" , \"person_id\" , \"preceding_visit_detail_id\" , \"visit_detail_parent_id\" , \"care_site_id\" , \"visit_detail_start_date\" , \"visit_detail_start_datetime\" , \"visit_detail_end_date\" , \"visit_detail_end_datetime\" , \"visit_detail_source_value\" , \"visit_detail_source_concept_id\" , \"visit_detail_type_source_value\" , \"visit_detail_type_source_concept_id\" , \"admitted_from_source_value\" , \"admitted_from_source_concept_id\" , \"discharge_to_source_value\" , \"discharge_to_source_concept_id\" , \"cdm_source\" , ], \"condition_occurrence\" : [ \"condition_occurrence_id\" , \"person_id\" , \"visit_occurrence_id\" , \"visit_detail_id\" , \"condition_start_datetime\" , \"condition_source_value\" , \"condition_source_concept_id\" , \"condition_status_source_value\" , \"condition_status_source_concept_id\" , \"cdm_source\" , ], \"procedure_occurrence\" : [ \"procedure_occurrence_id\" , \"person_id\" , \"visit_occurrence_id\" , \"visit_detail_id\" , \"procedure_datetime\" , \"procedure_source_value\" , \"procedure_source_concept_id\" , \"cdm_source\" , ], \"concept\" : [ \"concept_id\" , \"concept_name\" , \"domain_id\" , \"vocabulary_id\" , \"concept_class_id\" , \"standard_concept\" , \"concept_code\" , \"valid_start_date\" , \"valid_end_date\" , \"invalid_reason\" , ], } The default columns loaded when instanciating a HiveData or a PostgresData","title":"settings"},{"location":"reference/io/settings/#edstevaiosettings","text":"","title":"edsteva.io.settings"},{"location":"reference/io/settings/#edsteva.io.settings.default_tables_to_save","text":"default_tables_to_save = [ \"person\" , \"visit_occurrence\" , \"visit_detail\" , \"condition_occurrence\" , \"procedure_occurrence\" , \"care_site\" , \"concept\" , ] The default tables loaded when instanciating a HiveData or a PostgresData","title":"default_tables_to_save"},{"location":"reference/io/settings/#edsteva.io.settings.tables_to_load","text":"tables_to_load = { \"person\" : [ \"person_id\" , \"location_id\" , \"year_of_birth\" , \"month_of_birth\" , \"day_of_birth\" , \"birth_datetime\" , \"death_datetime\" , \"gender_source_value\" , \"gender_source_concept_id\" , \"cdm_source\" , ], \"visit_occurrence\" : [ \"visit_occurrence_id\" , \"person_id\" , \"visit_occurrence_source_value\" , \"preceding_visit_occurrence_id\" , \"care_site_id\" , \"visit_start_datetime\" , \"visit_end_datetime\" , \"visit_source_value\" , \"visit_source_concept_id\" , \"visit_type_source_value\" , \"visit_type_source_concept_id\" , \"admitted_from_source_value\" , \"admitted_from_source_concept_id\" , \"discharge_to_source_value\" , \"discharge_to_source_concept_id\" , \"row_status_source_value\" , \"stay_source_value\" , \"stay_source_concept_id\" , \"cdm_source\" , ], \"care_site\" : [ \"care_site_id\" , \"care_site_source_value\" , \"care_site_name\" , \"care_site_short_name\" , \"place_of_service_source_value\" , \"care_site_type_source_value\" , \"valid_start_date\" , \"valid_end_date\" , ], \"visit_detail\" : [ \"visit_detail_id\" , \"visit_occurrence_id\" , \"person_id\" , \"preceding_visit_detail_id\" , \"visit_detail_parent_id\" , \"care_site_id\" , \"visit_detail_start_date\" , \"visit_detail_start_datetime\" , \"visit_detail_end_date\" , \"visit_detail_end_datetime\" , \"visit_detail_source_value\" , \"visit_detail_source_concept_id\" , \"visit_detail_type_source_value\" , \"visit_detail_type_source_concept_id\" , \"admitted_from_source_value\" , \"admitted_from_source_concept_id\" , \"discharge_to_source_value\" , \"discharge_to_source_concept_id\" , \"cdm_source\" , ], \"condition_occurrence\" : [ \"condition_occurrence_id\" , \"person_id\" , \"visit_occurrence_id\" , \"visit_detail_id\" , \"condition_start_datetime\" , \"condition_source_value\" , \"condition_source_concept_id\" , \"condition_status_source_value\" , \"condition_status_source_concept_id\" , \"cdm_source\" , ], \"procedure_occurrence\" : [ \"procedure_occurrence_id\" , \"person_id\" , \"visit_occurrence_id\" , \"visit_detail_id\" , \"procedure_datetime\" , \"procedure_source_value\" , \"procedure_source_concept_id\" , \"cdm_source\" , ], \"concept\" : [ \"concept_id\" , \"concept_name\" , \"domain_id\" , \"vocabulary_id\" , \"concept_class_id\" , \"standard_concept\" , \"concept_code\" , \"valid_start_date\" , \"valid_end_date\" , \"invalid_reason\" , ], } The default columns loaded when instanciating a HiveData or a PostgresData","title":"tables_to_load"},{"location":"reference/io/synthetic/","text":"edsteva.io.synthetic","title":"`edsteva.io.synthetic`"},{"location":"reference/io/synthetic/#edstevaiosynthetic","text":"","title":"edsteva.io.synthetic"},{"location":"reference/io/synthetic/care_site/","text":"edsteva.io.synthetic.care_site","title":"care_site"},{"location":"reference/io/synthetic/care_site/#edstevaiosyntheticcare_site","text":"","title":"edsteva.io.synthetic.care_site"},{"location":"reference/io/synthetic/synthetic/","text":"edsteva.io.synthetic.synthetic","title":"synthetic"},{"location":"reference/io/synthetic/synthetic/#edstevaiosyntheticsynthetic","text":"","title":"edsteva.io.synthetic.synthetic"},{"location":"reference/io/synthetic/utils/","text":"edsteva.io.synthetic.utils","title":"utils"},{"location":"reference/io/synthetic/utils/#edstevaiosyntheticutils","text":"","title":"edsteva.io.synthetic.utils"},{"location":"reference/io/synthetic/visit/","text":"edsteva.io.synthetic.visit","title":"visit"},{"location":"reference/io/synthetic/visit/#edstevaiosyntheticvisit","text":"","title":"edsteva.io.synthetic.visit"},{"location":"reference/metrics/","text":"edsteva.metrics","title":"`edsteva.metrics`"},{"location":"reference/metrics/#edstevametrics","text":"","title":"edsteva.metrics"},{"location":"reference/metrics/error/","text":"edsteva.metrics.error error error ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , x : str = \"date\" , name : str = \"error\" , ) Compute the error between the predictor \\(c(t)\\) and the prediction \\(\\hat{c}(t)\\) as follow: \\[ error = \\frac{\\sum_{t_{min} \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_{min}} \\] Where the loss function \\(\\mathcal{l}\\) can be the L1 distance or the L2 distance. PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame estimates \\(\\hat{c}(t)\\) computed in the Model TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] loss_function The loss function \\(\\mathcal{l}\\) TYPE: str , optional DEFAULT: loss_functions.l2_loss y Target column name of \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' y_0 Target column name of \\(\\hat{c}(t)\\) TYPE: str , optional DEFAULT: 'c_0' x Target column name of \\(t\\) TYPE: str , optional DEFAULT: 'date' name Column name of the output TYPE: str , optional DEFAULT: 'error' Example care_site_level care_site_id stay_type error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 0.028 P\u00f4le/DMU 8312027648 'Urg_Hospit' 0.022 P\u00f4le/DMU 8312027648 'All' 0.014 H\u00f4pital 8312022130 'Urg_Hospit' 0.027 Source code in edsteva/metrics/error.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def error ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , x : str = \"date\" , name : str = \"error\" , ): r \"\"\"Compute the error between the predictor $c(t)$ and the prediction $\\hat{c}(t)$ as follow: $$ error = \\frac{\\sum_{t_{min} \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_{min}} $$ Where the loss function $\\mathcal{l}$ can be the L1 distance or the L2 distance. Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe estimates : pd.DataFrame $\\hat{c}(t)$ computed in the Model index : List[str] Variable from which data is grouped loss_function : str, optional The loss function $\\mathcal{l}$ y : str, optional Target column name of $c(t)$ y_0 : str, optional Target column name of $\\hat{c}(t)$ x : str, optional Target column name of $t$ name : str, optional Column name of the output Example ------- | care_site_level | care_site_id | stay_type | error | | :----------------------- | :----------- | :----------- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 0.014 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 0.027 | \"\"\" check_columns ( df = estimates , required_columns = index + [ y_0 ]) check_columns ( df = predictor , required_columns = index + [ x , y ]) fitted_predictor = predictor . merge ( estimates , on = index ) fitted_predictor [ \"loss\" ] = loss_function ( fitted_predictor [ y ] - fitted_predictor [ y_0 ] ) error = fitted_predictor . groupby ( index )[ \"loss\" ] . mean () . rename ( name ) return error . reset_index ()","title":"error"},{"location":"reference/metrics/error/#edstevametricserror","text":"","title":"edsteva.metrics.error"},{"location":"reference/metrics/error/#edsteva.metrics.error.error","text":"error ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , x : str = \"date\" , name : str = \"error\" , ) Compute the error between the predictor \\(c(t)\\) and the prediction \\(\\hat{c}(t)\\) as follow: \\[ error = \\frac{\\sum_{t_{min} \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_{min}} \\] Where the loss function \\(\\mathcal{l}\\) can be the L1 distance or the L2 distance. PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame estimates \\(\\hat{c}(t)\\) computed in the Model TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] loss_function The loss function \\(\\mathcal{l}\\) TYPE: str , optional DEFAULT: loss_functions.l2_loss y Target column name of \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' y_0 Target column name of \\(\\hat{c}(t)\\) TYPE: str , optional DEFAULT: 'c_0' x Target column name of \\(t\\) TYPE: str , optional DEFAULT: 'date' name Column name of the output TYPE: str , optional DEFAULT: 'error'","title":"error()"},{"location":"reference/metrics/error/#edsteva.metrics.error.error--example","text":"care_site_level care_site_id stay_type error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 0.028 P\u00f4le/DMU 8312027648 'Urg_Hospit' 0.022 P\u00f4le/DMU 8312027648 'All' 0.014 H\u00f4pital 8312022130 'Urg_Hospit' 0.027 Source code in edsteva/metrics/error.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def error ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , x : str = \"date\" , name : str = \"error\" , ): r \"\"\"Compute the error between the predictor $c(t)$ and the prediction $\\hat{c}(t)$ as follow: $$ error = \\frac{\\sum_{t_{min} \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_{min}} $$ Where the loss function $\\mathcal{l}$ can be the L1 distance or the L2 distance. Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe estimates : pd.DataFrame $\\hat{c}(t)$ computed in the Model index : List[str] Variable from which data is grouped loss_function : str, optional The loss function $\\mathcal{l}$ y : str, optional Target column name of $c(t)$ y_0 : str, optional Target column name of $\\hat{c}(t)$ x : str, optional Target column name of $t$ name : str, optional Column name of the output Example ------- | care_site_level | care_site_id | stay_type | error | | :----------------------- | :----------- | :----------- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 0.014 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 0.027 | \"\"\" check_columns ( df = estimates , required_columns = index + [ y_0 ]) check_columns ( df = predictor , required_columns = index + [ x , y ]) fitted_predictor = predictor . merge ( estimates , on = index ) fitted_predictor [ \"loss\" ] = loss_function ( fitted_predictor [ y ] - fitted_predictor [ y_0 ] ) error = fitted_predictor . groupby ( index )[ \"loss\" ] . mean () . rename ( name ) return error . reset_index ()","title":"Example"},{"location":"reference/metrics/error_after_t0/","text":"edsteva.metrics.error_after_t0 error_after_t0 error_after_t0 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , threshold : str = \"t_0\" , x : str = \"date\" , name : str = \"error\" , ) Compute the error between the predictor \\(c(t)\\) and the prediction \\(\\hat{c}(t)\\) after \\(t_0\\) as follow: \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} \\] Where the loss function \\(\\mathcal{l}\\) can be the L1 distance or the L2 distance. PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame estimates \\(\\hat{c}(t)\\) computed in the Model TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] loss_function The loss function \\(\\mathcal{l}\\) TYPE: Callable , optional DEFAULT: loss_functions.l2_loss y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' y_0 Column name for the predicted completeness variable \\(\\hat{c}(t)\\) TYPE: str , optional DEFAULT: 'c_0' threshold Column name for the predicted threshold \\(t_0\\) TYPE: str , optional DEFAULT: 't_0' x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' name Column name for the metric output TYPE: str , optional DEFAULT: 'error' Example care_site_level care_site_id stay_type error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 0.028 P\u00f4le/DMU 8312027648 'Urg' 0.022 P\u00f4le/DMU 8312027648 'All' 0.014 H\u00f4pital 8312022130 'Urg' 0.027 Source code in edsteva/metrics/error_after_t0.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def error_after_t0 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , threshold : str = \"t_0\" , x : str = \"date\" , name : str = \"error\" , ): r \"\"\"Compute the error between the predictor $c(t)$ and the prediction $\\hat{c}(t)$ after $t_0$ as follow: $$ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} $$ Where the loss function $\\mathcal{l}$ can be the L1 distance or the L2 distance. Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe estimates : pd.DataFrame $\\hat{c}(t)$ computed in the Model index : List[str] Variable from which data is grouped loss_function : Callable, optional The loss function $\\mathcal{l}$ y : str, optional Column name for the completeness variable $c(t)$ y_0 : str, optional Column name for the predicted completeness variable $\\hat{c}(t)$ threshold : str, optional Column name for the predicted threshold $t_0$ x : str, optional Column name for the time variable $t$ name : str, optional Column name for the metric output Example ------- | care_site_level | care_site_id | stay_type | error | | :----------------------- | :----------- | :---------| :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg' | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg' | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 0.014 | | H\u00f4pital | 8312022130 | 'Urg' | 0.027 | \"\"\" check_columns ( df = estimates , required_columns = index + [ y_0 , threshold ]) check_columns ( df = predictor , required_columns = index + [ x , y ]) fitted_predictor = predictor . merge ( estimates , on = index ) fitted_predictor = fitted_predictor . dropna ( subset = [ threshold ]) fitted_predictor [ \"loss\" ] = loss_function ( fitted_predictor [ y ] - fitted_predictor [ y_0 ] ) mask_after_t0 = fitted_predictor [ x ] >= fitted_predictor [ threshold ] fitted_predictor = fitted_predictor . loc [ mask_after_t0 ] error = fitted_predictor . groupby ( index )[ \"loss\" ] . mean () . rename ( name ) return error . reset_index ()","title":"error_after_t0"},{"location":"reference/metrics/error_after_t0/#edstevametricserror_after_t0","text":"","title":"edsteva.metrics.error_after_t0"},{"location":"reference/metrics/error_after_t0/#edsteva.metrics.error_after_t0.error_after_t0","text":"error_after_t0 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , threshold : str = \"t_0\" , x : str = \"date\" , name : str = \"error\" , ) Compute the error between the predictor \\(c(t)\\) and the prediction \\(\\hat{c}(t)\\) after \\(t_0\\) as follow: \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} \\] Where the loss function \\(\\mathcal{l}\\) can be the L1 distance or the L2 distance. PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame estimates \\(\\hat{c}(t)\\) computed in the Model TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] loss_function The loss function \\(\\mathcal{l}\\) TYPE: Callable , optional DEFAULT: loss_functions.l2_loss y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' y_0 Column name for the predicted completeness variable \\(\\hat{c}(t)\\) TYPE: str , optional DEFAULT: 'c_0' threshold Column name for the predicted threshold \\(t_0\\) TYPE: str , optional DEFAULT: 't_0' x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' name Column name for the metric output TYPE: str , optional DEFAULT: 'error'","title":"error_after_t0()"},{"location":"reference/metrics/error_after_t0/#edsteva.metrics.error_after_t0.error_after_t0--example","text":"care_site_level care_site_id stay_type error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 0.028 P\u00f4le/DMU 8312027648 'Urg' 0.022 P\u00f4le/DMU 8312027648 'All' 0.014 H\u00f4pital 8312022130 'Urg' 0.027 Source code in edsteva/metrics/error_after_t0.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def error_after_t0 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , threshold : str = \"t_0\" , x : str = \"date\" , name : str = \"error\" , ): r \"\"\"Compute the error between the predictor $c(t)$ and the prediction $\\hat{c}(t)$ after $t_0$ as follow: $$ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} $$ Where the loss function $\\mathcal{l}$ can be the L1 distance or the L2 distance. Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe estimates : pd.DataFrame $\\hat{c}(t)$ computed in the Model index : List[str] Variable from which data is grouped loss_function : Callable, optional The loss function $\\mathcal{l}$ y : str, optional Column name for the completeness variable $c(t)$ y_0 : str, optional Column name for the predicted completeness variable $\\hat{c}(t)$ threshold : str, optional Column name for the predicted threshold $t_0$ x : str, optional Column name for the time variable $t$ name : str, optional Column name for the metric output Example ------- | care_site_level | care_site_id | stay_type | error | | :----------------------- | :----------- | :---------| :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg' | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg' | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 0.014 | | H\u00f4pital | 8312022130 | 'Urg' | 0.027 | \"\"\" check_columns ( df = estimates , required_columns = index + [ y_0 , threshold ]) check_columns ( df = predictor , required_columns = index + [ x , y ]) fitted_predictor = predictor . merge ( estimates , on = index ) fitted_predictor = fitted_predictor . dropna ( subset = [ threshold ]) fitted_predictor [ \"loss\" ] = loss_function ( fitted_predictor [ y ] - fitted_predictor [ y_0 ] ) mask_after_t0 = fitted_predictor [ x ] >= fitted_predictor [ threshold ] fitted_predictor = fitted_predictor . loc [ mask_after_t0 ] error = fitted_predictor . groupby ( index )[ \"loss\" ] . mean () . rename ( name ) return error . reset_index ()","title":"Example"},{"location":"reference/metrics/error_between_t0_t1/","text":"edsteva.metrics.error_between_t0_t1 error_between_t0_t1 error_between_t0_t1 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , t_0 : str = \"t_0\" , t_1 : str = \"t_1\" , x : str = \"date\" , name : str = \"error\" , ) Compute the error between the predictor \\(c(t)\\) and the prediction \\(\\hat{c}(t)\\) after \\(t_0\\) as follow: \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} \\] Where the loss function \\(\\mathcal{l}\\) can be the L1 distance or the L2 distance. PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame estimates \\(\\hat{c}(t)\\) computed in the Model TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] loss_function The loss function \\(\\mathcal{l}\\) TYPE: Callable , optional DEFAULT: loss_functions.l2_loss y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' y_0 Column name for the predicted completeness variable \\(\\hat{c}(t)\\) TYPE: str , optional DEFAULT: 'c_0' t_0 Column name for the predicted threshold \\(t_0\\) TYPE: str , optional DEFAULT: 't_0' t_1 Column name for the predicted threshold \\(t_1\\) TYPE: str , optional DEFAULT: 't_1' x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' name Column name for the metric output TYPE: str , optional DEFAULT: 'error' Example care_site_level care_site_id stay_type error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 0.028 P\u00f4le/DMU 8312027648 'Urg' 0.022 P\u00f4le/DMU 8312027648 'All' 0.014 H\u00f4pital 8312022130 'Urg' 0.027 Source code in edsteva/metrics/error_between_t0_t1.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def error_between_t0_t1 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , t_0 : str = \"t_0\" , t_1 : str = \"t_1\" , x : str = \"date\" , name : str = \"error\" , ): r \"\"\"Compute the error between the predictor $c(t)$ and the prediction $\\hat{c}(t)$ after $t_0$ as follow: $$ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} $$ Where the loss function $\\mathcal{l}$ can be the L1 distance or the L2 distance. Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe estimates : pd.DataFrame $\\hat{c}(t)$ computed in the Model index : List[str] Variable from which data is grouped loss_function : Callable, optional The loss function $\\mathcal{l}$ y : str, optional Column name for the completeness variable $c(t)$ y_0 : str, optional Column name for the predicted completeness variable $\\hat{c}(t)$ t_0 : str, optional Column name for the predicted threshold $t_0$ t_1 : str, optional Column name for the predicted threshold $t_1$ x : str, optional Column name for the time variable $t$ name : str, optional Column name for the metric output Example ------- | care_site_level | care_site_id | stay_type | error | | :----------------------- | :----------- | :---------| :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg' | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg' | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 0.014 | | H\u00f4pital | 8312022130 | 'Urg' | 0.027 | \"\"\" check_columns ( df = estimates , required_columns = index + [ y_0 , t_0 , t_1 ]) check_columns ( df = predictor , required_columns = index + [ x , y ]) fitted_predictor = predictor . merge ( estimates , on = index ) fitted_predictor = fitted_predictor . dropna ( subset = [ t_0 , t_1 ]) fitted_predictor [ \"loss\" ] = loss_function ( fitted_predictor [ y ] - fitted_predictor [ y_0 ] ) mask_between_t0_t1 = ( fitted_predictor [ x ] >= fitted_predictor [ t_0 ]) & ( fitted_predictor [ x ] <= fitted_predictor [ t_1 ] ) fitted_predictor = fitted_predictor . loc [ mask_between_t0_t1 ] error = fitted_predictor . groupby ( index )[ \"loss\" ] . mean () . rename ( name ) return error . reset_index ()","title":"error_between_t0_t1"},{"location":"reference/metrics/error_between_t0_t1/#edstevametricserror_between_t0_t1","text":"","title":"edsteva.metrics.error_between_t0_t1"},{"location":"reference/metrics/error_between_t0_t1/#edsteva.metrics.error_between_t0_t1.error_between_t0_t1","text":"error_between_t0_t1 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , t_0 : str = \"t_0\" , t_1 : str = \"t_1\" , x : str = \"date\" , name : str = \"error\" , ) Compute the error between the predictor \\(c(t)\\) and the prediction \\(\\hat{c}(t)\\) after \\(t_0\\) as follow: \\[ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} \\] Where the loss function \\(\\mathcal{l}\\) can be the L1 distance or the L2 distance. PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame estimates \\(\\hat{c}(t)\\) computed in the Model TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] loss_function The loss function \\(\\mathcal{l}\\) TYPE: Callable , optional DEFAULT: loss_functions.l2_loss y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' y_0 Column name for the predicted completeness variable \\(\\hat{c}(t)\\) TYPE: str , optional DEFAULT: 'c_0' t_0 Column name for the predicted threshold \\(t_0\\) TYPE: str , optional DEFAULT: 't_0' t_1 Column name for the predicted threshold \\(t_1\\) TYPE: str , optional DEFAULT: 't_1' x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' name Column name for the metric output TYPE: str , optional DEFAULT: 'error'","title":"error_between_t0_t1()"},{"location":"reference/metrics/error_between_t0_t1/#edsteva.metrics.error_between_t0_t1.error_between_t0_t1--example","text":"care_site_level care_site_id stay_type error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 0.028 P\u00f4le/DMU 8312027648 'Urg' 0.022 P\u00f4le/DMU 8312027648 'All' 0.014 H\u00f4pital 8312022130 'Urg' 0.027 Source code in edsteva/metrics/error_between_t0_t1.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def error_between_t0_t1 ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], loss_function : Callable = loss_functions . l2_loss , y : str = \"c\" , y_0 : str = \"c_0\" , t_0 : str = \"t_0\" , t_1 : str = \"t_1\" , x : str = \"date\" , name : str = \"error\" , ): r \"\"\"Compute the error between the predictor $c(t)$ and the prediction $\\hat{c}(t)$ after $t_0$ as follow: $$ error = \\frac{\\sum_{t_0 \\leq t \\leq t_{max}} \\mathcal{l}(c(t), \\hat{c}(t))}{t_{max} - t_0} $$ Where the loss function $\\mathcal{l}$ can be the L1 distance or the L2 distance. Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe estimates : pd.DataFrame $\\hat{c}(t)$ computed in the Model index : List[str] Variable from which data is grouped loss_function : Callable, optional The loss function $\\mathcal{l}$ y : str, optional Column name for the completeness variable $c(t)$ y_0 : str, optional Column name for the predicted completeness variable $\\hat{c}(t)$ t_0 : str, optional Column name for the predicted threshold $t_0$ t_1 : str, optional Column name for the predicted threshold $t_1$ x : str, optional Column name for the time variable $t$ name : str, optional Column name for the metric output Example ------- | care_site_level | care_site_id | stay_type | error | | :----------------------- | :----------- | :---------| :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg' | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg' | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 0.014 | | H\u00f4pital | 8312022130 | 'Urg' | 0.027 | \"\"\" check_columns ( df = estimates , required_columns = index + [ y_0 , t_0 , t_1 ]) check_columns ( df = predictor , required_columns = index + [ x , y ]) fitted_predictor = predictor . merge ( estimates , on = index ) fitted_predictor = fitted_predictor . dropna ( subset = [ t_0 , t_1 ]) fitted_predictor [ \"loss\" ] = loss_function ( fitted_predictor [ y ] - fitted_predictor [ y_0 ] ) mask_between_t0_t1 = ( fitted_predictor [ x ] >= fitted_predictor [ t_0 ]) & ( fitted_predictor [ x ] <= fitted_predictor [ t_1 ] ) fitted_predictor = fitted_predictor . loc [ mask_between_t0_t1 ] error = fitted_predictor . groupby ( index )[ \"loss\" ] . mean () . rename ( name ) return error . reset_index ()","title":"Example"},{"location":"reference/models/","text":"edsteva.models","title":"`edsteva.models`"},{"location":"reference/models/#edstevamodels","text":"","title":"edsteva.models"},{"location":"reference/models/base/","text":"edsteva.models.base BaseModel Base class for Models ATTRIBUTE DESCRIPTION _coefs The list of the Model coefficients TYPE: List [ str ] estimates Available with the fit() method TYPE: pd . DataFrame _metrics Available with the fit() method The list of computed metrics if any TYPE: List [ str ] params Available with the fit() method Ths list of extra keyword parameters used. TYPE: List [ str ] Source code in edsteva/models/base.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 class BaseModel ( metaclass = ABCMeta ): \"\"\"Base class for Models Attributes ---------- _coefs: List[str] The list of the Model coefficients estimates: pd.DataFrame Available with the [``fit()``][edsteva.models.base.BaseModel.fit] method _metrics: List[str] Available with the [``fit()``][edsteva.models.base.BaseModel.fit] method The list of computed metrics if any params: List[str] Available with the [``fit()``][edsteva.models.base.BaseModel.fit] method Ths list of extra keyword parameters used. \"\"\" def __init__ ( self ): self . is_valid_model () self . name = type ( self ) . __name__ def is_valid_model ( self ) -> None : \"\"\"Raises an error if the instantiated Model is not valid\"\"\" if not hasattr ( self , \"_coefs\" ): raise Exception ( \"Model must have _coefs attribute. Please review the code of your model\" ) def is_computed_estimates ( self ) -> None : \"\"\"Raises an error if the Probe has not been fitted properly\"\"\" if hasattr ( self , \"estimates\" ): if isinstance ( self . estimates , pd . DataFrame ): if len ( self . estimates ) == 0 : raise Exception ( \"Estimates are empty, please review the process method or your arguments\" ) else : raise Exception ( \"The fit process must return a Pandas Dataframe and not {} \" . format ( type ( self . estimates ) ) ) else : raise Exception ( \"Model has not been fitted, please use the fit method as follow: Model.fit()\" ) @abstractmethod def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , ** kwargs , ): \"\"\"Fit the Probe in order to obtain estimates\"\"\" @abstractmethod def predict_process ( self , prediction : pd . DataFrame , ** kwargs , ): \"\"\"Compute the predicted Probe\"\"\" def fit ( self , probe : BaseProbe , metric_functions : List [ Callable ] = None , start_date : str = None , end_date : str = None , ** kwargs , ) -> None : \"\"\"Fit the model to the probe instance Parameters ---------- probe : BaseProbe Target variable to be fitted metric_functions : List[Callable], optional Metrics to apply on the fitted Probe. By default it will apply the default metric specified in the model. **EXAMPLE**: `[error, error_after_t0]` start_date : str, optional **EXAMPLE**: `\"2019-05-01\"` end_date : str, optional **EXAMPLE**: `\"2021-07-01\"` Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | error | | :----------------------- | :----------- | :----------- | :--------- | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 2019-05-01 | 0.397 | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 2021-03-01 | 0.677 | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | 0.014 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 2022-02-01 | 0.652 | 0.027 | \"\"\" if isinstance ( probe , BaseProbe ): probe . is_computed_probe () else : raise TypeError ( \"Unsupported type {} for probe.\" . format ( type ( probe ))) predictor = filter_table_by_date ( table = probe . predictor , table_name = \"predictor\" , start_date = start_date , end_date = end_date , ) index = probe . _index estimates = self . fit_process ( predictor = predictor , index = index , ** kwargs , ) metrics = self . _compute_metrics ( predictor = predictor , estimates = estimates , index = index , metric_functions = metric_functions , ) if metrics is not None : self . _metrics = list ( metrics . columns . difference ( index )) self . estimates = estimates . merge ( metrics , on = index ) else : self . estimates = estimates self . is_computed_estimates () self . params = kwargs def predict ( self , probe : BaseProbe , ) -> pd . DataFrame : \"\"\"Computes the predicted probe by using the estimates Parameters ---------- probe : BaseProbe Target variable to be predicted Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model.predict(visit).head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" predictor = probe . predictor index = probe . _index prediction = self . predict_process ( predictor = predictor , index = index ) return prediction def load ( self , path = None ) -> None : \"\"\"Loads a Model from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" if not path : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" loaded_model = load_object ( path ) self . __dict__ = loaded_model . __dict__ . copy () self . path = path def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"fitted_visit\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_estimates () if not path : if name : self . name = name path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" self . path = path save_object ( self , path ) def delete ( self , path : str = None ) -> bool : \"\"\"Delete the saved Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" delete_object ( self , path ) def _compute_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], metric_functions : List [ Callable ] = None , ): if metric_functions : if callable ( metric_functions ): metrics = metric_functions ( predictor = predictor , estimates = estimates , index = index ) elif isinstance ( metric_functions , list ): metrics = [] for metric_function in metric_functions : if callable ( metric_function ): metrics . append ( metric_function ( predictor = predictor , estimates = estimates , index = index ) ) else : raise TypeError ( \" {} is not callable. The metrics input must be a list of callable functions\" . format ( type ( metric_function ) ) ) metrics = reduce ( lambda left , right : pd . merge ( left , right , on = index ), metrics ) else : raise TypeError ( \" {} is not callable. The metrics input must be a callable function\" . format ( type ( metrics ) ) ) elif hasattr ( self , \"default_metrics\" ): metrics = self . default_metrics ( predictor = predictor , estimates = estimates , index = index ) else : metrics = None return metrics def is_predictable_probe ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Raises an error if the model has not been fitted on the input predictor. Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Predictor along with the fitted estimates Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe \"\"\" prediction = predictor . merge ( self . estimates , on = index , how = \"left\" , validate = \"many_to_one\" , indicator = True ) if ( prediction [ \"_merge\" ] == \"both\" ) . all (): prediction = prediction . drop ( columns = \"_merge\" ) return prediction else : raise Exception ( \"Some indexes have no associated estimates, the model must be fitted on an adequate probe\" ) is_valid_model is_valid_model () -> None Raises an error if the instantiated Model is not valid Source code in edsteva/models/base.py 41 42 43 44 45 46 def is_valid_model ( self ) -> None : \"\"\"Raises an error if the instantiated Model is not valid\"\"\" if not hasattr ( self , \"_coefs\" ): raise Exception ( \"Model must have _coefs attribute. Please review the code of your model\" ) is_computed_estimates is_computed_estimates () -> None Raises an error if the Probe has not been fitted properly Source code in edsteva/models/base.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def is_computed_estimates ( self ) -> None : \"\"\"Raises an error if the Probe has not been fitted properly\"\"\" if hasattr ( self , \"estimates\" ): if isinstance ( self . estimates , pd . DataFrame ): if len ( self . estimates ) == 0 : raise Exception ( \"Estimates are empty, please review the process method or your arguments\" ) else : raise Exception ( \"The fit process must return a Pandas Dataframe and not {} \" . format ( type ( self . estimates ) ) ) else : raise Exception ( \"Model has not been fitted, please use the fit method as follow: Model.fit()\" ) fit_process abstractmethod fit_process ( predictor : pd . DataFrame , index : List [ str ] = None , ** kwargs ) Fit the Probe in order to obtain estimates Source code in edsteva/models/base.py 68 69 70 71 72 73 74 75 @abstractmethod def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , ** kwargs , ): \"\"\"Fit the Probe in order to obtain estimates\"\"\" predict_process abstractmethod predict_process ( prediction : pd . DataFrame , ** kwargs ) Compute the predicted Probe Source code in edsteva/models/base.py 77 78 79 80 81 82 83 @abstractmethod def predict_process ( self , prediction : pd . DataFrame , ** kwargs , ): \"\"\"Compute the predicted Probe\"\"\" fit fit ( probe : BaseProbe , metric_functions : List [ Callable ] = None , start_date : str = None , end_date : str = None , ** kwargs ) -> None Fit the model to the probe instance PARAMETER DESCRIPTION probe Target variable to be fitted TYPE: BaseProbe metric_functions Metrics to apply on the fitted Probe. By default it will apply the default metric specified in the model. EXAMPLE : [error, error_after_t0] TYPE: List [ Callable ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: str , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: str , optional DEFAULT: None Examples: from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Urg_Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Urg_Hospit' 2022-02-01 0.652 0.027 Source code in edsteva/models/base.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def fit ( self , probe : BaseProbe , metric_functions : List [ Callable ] = None , start_date : str = None , end_date : str = None , ** kwargs , ) -> None : \"\"\"Fit the model to the probe instance Parameters ---------- probe : BaseProbe Target variable to be fitted metric_functions : List[Callable], optional Metrics to apply on the fitted Probe. By default it will apply the default metric specified in the model. **EXAMPLE**: `[error, error_after_t0]` start_date : str, optional **EXAMPLE**: `\"2019-05-01\"` end_date : str, optional **EXAMPLE**: `\"2021-07-01\"` Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | error | | :----------------------- | :----------- | :----------- | :--------- | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 2019-05-01 | 0.397 | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 2021-03-01 | 0.677 | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | 0.014 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 2022-02-01 | 0.652 | 0.027 | \"\"\" if isinstance ( probe , BaseProbe ): probe . is_computed_probe () else : raise TypeError ( \"Unsupported type {} for probe.\" . format ( type ( probe ))) predictor = filter_table_by_date ( table = probe . predictor , table_name = \"predictor\" , start_date = start_date , end_date = end_date , ) index = probe . _index estimates = self . fit_process ( predictor = predictor , index = index , ** kwargs , ) metrics = self . _compute_metrics ( predictor = predictor , estimates = estimates , index = index , metric_functions = metric_functions , ) if metrics is not None : self . _metrics = list ( metrics . columns . difference ( index )) self . estimates = estimates . merge ( metrics , on = index ) else : self . estimates = estimates self . is_computed_estimates () self . params = kwargs predict predict ( probe : BaseProbe ) -> pd . DataFrame Computes the predicted probe by using the estimates PARAMETER DESCRIPTION probe Target variable to be predicted TYPE: BaseProbe Examples: from edsteva.models.step_function import StepFunction step_function_model . predict ( visit ) . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c c_fit Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 0.758 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 0.758 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 0 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.784 0.874 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.974 0.912 Source code in edsteva/models/base.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def predict ( self , probe : BaseProbe , ) -> pd . DataFrame : \"\"\"Computes the predicted probe by using the estimates Parameters ---------- probe : BaseProbe Target variable to be predicted Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model.predict(visit).head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" predictor = probe . predictor index = probe . _index prediction = self . predict_process ( predictor = predictor , index = index ) return prediction load load ( path = None ) -> None Loads a Model from local PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . load ( path = probe_path ) Source code in edsteva/models/base.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def load ( self , path = None ) -> None : \"\"\"Loads a Model from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" if not path : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" loaded_model = load_object ( path ) self . __dict__ = loaded_model . __dict__ . copy () self . path = path save save ( path : str = None , name : str = None ) -> bool Saves computed Model instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None name EXAMPLE : \"fitted_visit\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . compute ( data ) visit . save ( path = probe_path ) Source code in edsteva/models/base.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"fitted_visit\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_estimates () if not path : if name : self . name = name path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" self . path = path save_object ( self , path ) delete delete ( path : str = None ) -> bool Delete the saved Model instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/models/base.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def delete ( self , path : str = None ) -> bool : \"\"\"Delete the saved Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" delete_object ( self , path ) is_predictable_probe is_predictable_probe ( predictor : pd . DataFrame , index : List [ str ] ) -> pd . DataFrame Raises an error if the model has not been fitted on the input predictor. PARAMETER DESCRIPTION predictor Target DataFrame to be predicted TYPE: pd . DataFrame index List of the columns given by Probe._index TYPE: List [ str ] RETURNS DESCRIPTION pd . DataFrame Predictor along with the fitted estimates RAISES DESCRIPTION Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Source code in edsteva/models/base.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def is_predictable_probe ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Raises an error if the model has not been fitted on the input predictor. Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Predictor along with the fitted estimates Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe \"\"\" prediction = predictor . merge ( self . estimates , on = index , how = \"left\" , validate = \"many_to_one\" , indicator = True ) if ( prediction [ \"_merge\" ] == \"both\" ) . all (): prediction = prediction . drop ( columns = \"_merge\" ) return prediction else : raise Exception ( \"Some indexes have no associated estimates, the model must be fitted on an adequate probe\" )","title":"base"},{"location":"reference/models/base/#edstevamodelsbase","text":"","title":"edsteva.models.base"},{"location":"reference/models/base/#edsteva.models.base.BaseModel","text":"Base class for Models ATTRIBUTE DESCRIPTION _coefs The list of the Model coefficients TYPE: List [ str ] estimates Available with the fit() method TYPE: pd . DataFrame _metrics Available with the fit() method The list of computed metrics if any TYPE: List [ str ] params Available with the fit() method Ths list of extra keyword parameters used. TYPE: List [ str ] Source code in edsteva/models/base.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 class BaseModel ( metaclass = ABCMeta ): \"\"\"Base class for Models Attributes ---------- _coefs: List[str] The list of the Model coefficients estimates: pd.DataFrame Available with the [``fit()``][edsteva.models.base.BaseModel.fit] method _metrics: List[str] Available with the [``fit()``][edsteva.models.base.BaseModel.fit] method The list of computed metrics if any params: List[str] Available with the [``fit()``][edsteva.models.base.BaseModel.fit] method Ths list of extra keyword parameters used. \"\"\" def __init__ ( self ): self . is_valid_model () self . name = type ( self ) . __name__ def is_valid_model ( self ) -> None : \"\"\"Raises an error if the instantiated Model is not valid\"\"\" if not hasattr ( self , \"_coefs\" ): raise Exception ( \"Model must have _coefs attribute. Please review the code of your model\" ) def is_computed_estimates ( self ) -> None : \"\"\"Raises an error if the Probe has not been fitted properly\"\"\" if hasattr ( self , \"estimates\" ): if isinstance ( self . estimates , pd . DataFrame ): if len ( self . estimates ) == 0 : raise Exception ( \"Estimates are empty, please review the process method or your arguments\" ) else : raise Exception ( \"The fit process must return a Pandas Dataframe and not {} \" . format ( type ( self . estimates ) ) ) else : raise Exception ( \"Model has not been fitted, please use the fit method as follow: Model.fit()\" ) @abstractmethod def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , ** kwargs , ): \"\"\"Fit the Probe in order to obtain estimates\"\"\" @abstractmethod def predict_process ( self , prediction : pd . DataFrame , ** kwargs , ): \"\"\"Compute the predicted Probe\"\"\" def fit ( self , probe : BaseProbe , metric_functions : List [ Callable ] = None , start_date : str = None , end_date : str = None , ** kwargs , ) -> None : \"\"\"Fit the model to the probe instance Parameters ---------- probe : BaseProbe Target variable to be fitted metric_functions : List[Callable], optional Metrics to apply on the fitted Probe. By default it will apply the default metric specified in the model. **EXAMPLE**: `[error, error_after_t0]` start_date : str, optional **EXAMPLE**: `\"2019-05-01\"` end_date : str, optional **EXAMPLE**: `\"2021-07-01\"` Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | error | | :----------------------- | :----------- | :----------- | :--------- | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 2019-05-01 | 0.397 | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 2021-03-01 | 0.677 | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | 0.014 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 2022-02-01 | 0.652 | 0.027 | \"\"\" if isinstance ( probe , BaseProbe ): probe . is_computed_probe () else : raise TypeError ( \"Unsupported type {} for probe.\" . format ( type ( probe ))) predictor = filter_table_by_date ( table = probe . predictor , table_name = \"predictor\" , start_date = start_date , end_date = end_date , ) index = probe . _index estimates = self . fit_process ( predictor = predictor , index = index , ** kwargs , ) metrics = self . _compute_metrics ( predictor = predictor , estimates = estimates , index = index , metric_functions = metric_functions , ) if metrics is not None : self . _metrics = list ( metrics . columns . difference ( index )) self . estimates = estimates . merge ( metrics , on = index ) else : self . estimates = estimates self . is_computed_estimates () self . params = kwargs def predict ( self , probe : BaseProbe , ) -> pd . DataFrame : \"\"\"Computes the predicted probe by using the estimates Parameters ---------- probe : BaseProbe Target variable to be predicted Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model.predict(visit).head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" predictor = probe . predictor index = probe . _index prediction = self . predict_process ( predictor = predictor , index = index ) return prediction def load ( self , path = None ) -> None : \"\"\"Loads a Model from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" if not path : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" loaded_model = load_object ( path ) self . __dict__ = loaded_model . __dict__ . copy () self . path = path def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"fitted_visit\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_estimates () if not path : if name : self . name = name path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" self . path = path save_object ( self , path ) def delete ( self , path : str = None ) -> bool : \"\"\"Delete the saved Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" delete_object ( self , path ) def _compute_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], metric_functions : List [ Callable ] = None , ): if metric_functions : if callable ( metric_functions ): metrics = metric_functions ( predictor = predictor , estimates = estimates , index = index ) elif isinstance ( metric_functions , list ): metrics = [] for metric_function in metric_functions : if callable ( metric_function ): metrics . append ( metric_function ( predictor = predictor , estimates = estimates , index = index ) ) else : raise TypeError ( \" {} is not callable. The metrics input must be a list of callable functions\" . format ( type ( metric_function ) ) ) metrics = reduce ( lambda left , right : pd . merge ( left , right , on = index ), metrics ) else : raise TypeError ( \" {} is not callable. The metrics input must be a callable function\" . format ( type ( metrics ) ) ) elif hasattr ( self , \"default_metrics\" ): metrics = self . default_metrics ( predictor = predictor , estimates = estimates , index = index ) else : metrics = None return metrics def is_predictable_probe ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Raises an error if the model has not been fitted on the input predictor. Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Predictor along with the fitted estimates Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe \"\"\" prediction = predictor . merge ( self . estimates , on = index , how = \"left\" , validate = \"many_to_one\" , indicator = True ) if ( prediction [ \"_merge\" ] == \"both\" ) . all (): prediction = prediction . drop ( columns = \"_merge\" ) return prediction else : raise Exception ( \"Some indexes have no associated estimates, the model must be fitted on an adequate probe\" )","title":"BaseModel"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.is_valid_model","text":"is_valid_model () -> None Raises an error if the instantiated Model is not valid Source code in edsteva/models/base.py 41 42 43 44 45 46 def is_valid_model ( self ) -> None : \"\"\"Raises an error if the instantiated Model is not valid\"\"\" if not hasattr ( self , \"_coefs\" ): raise Exception ( \"Model must have _coefs attribute. Please review the code of your model\" )","title":"is_valid_model()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.is_computed_estimates","text":"is_computed_estimates () -> None Raises an error if the Probe has not been fitted properly Source code in edsteva/models/base.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def is_computed_estimates ( self ) -> None : \"\"\"Raises an error if the Probe has not been fitted properly\"\"\" if hasattr ( self , \"estimates\" ): if isinstance ( self . estimates , pd . DataFrame ): if len ( self . estimates ) == 0 : raise Exception ( \"Estimates are empty, please review the process method or your arguments\" ) else : raise Exception ( \"The fit process must return a Pandas Dataframe and not {} \" . format ( type ( self . estimates ) ) ) else : raise Exception ( \"Model has not been fitted, please use the fit method as follow: Model.fit()\" )","title":"is_computed_estimates()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.fit_process","text":"fit_process ( predictor : pd . DataFrame , index : List [ str ] = None , ** kwargs ) Fit the Probe in order to obtain estimates Source code in edsteva/models/base.py 68 69 70 71 72 73 74 75 @abstractmethod def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , ** kwargs , ): \"\"\"Fit the Probe in order to obtain estimates\"\"\"","title":"fit_process()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.predict_process","text":"predict_process ( prediction : pd . DataFrame , ** kwargs ) Compute the predicted Probe Source code in edsteva/models/base.py 77 78 79 80 81 82 83 @abstractmethod def predict_process ( self , prediction : pd . DataFrame , ** kwargs , ): \"\"\"Compute the predicted Probe\"\"\"","title":"predict_process()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.fit","text":"fit ( probe : BaseProbe , metric_functions : List [ Callable ] = None , start_date : str = None , end_date : str = None , ** kwargs ) -> None Fit the model to the probe instance PARAMETER DESCRIPTION probe Target variable to be fitted TYPE: BaseProbe metric_functions Metrics to apply on the fitted Probe. By default it will apply the default metric specified in the model. EXAMPLE : [error, error_after_t0] TYPE: List [ Callable ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: str , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: str , optional DEFAULT: None Examples: from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 2019-05-01 0.397 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 0.028 P\u00f4le/DMU 8312027648 'Urg_Hospit' 2021-03-01 0.677 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 0.014 H\u00f4pital 8312022130 'Urg_Hospit' 2022-02-01 0.652 0.027 Source code in edsteva/models/base.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def fit ( self , probe : BaseProbe , metric_functions : List [ Callable ] = None , start_date : str = None , end_date : str = None , ** kwargs , ) -> None : \"\"\"Fit the model to the probe instance Parameters ---------- probe : BaseProbe Target variable to be fitted metric_functions : List[Callable], optional Metrics to apply on the fitted Probe. By default it will apply the default metric specified in the model. **EXAMPLE**: `[error, error_after_t0]` start_date : str, optional **EXAMPLE**: `\"2019-05-01\"` end_date : str, optional **EXAMPLE**: `\"2021-07-01\"` Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | error | | :----------------------- | :----------- | :----------- | :--------- | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 2019-05-01 | 0.397 | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 2021-03-01 | 0.677 | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | 0.014 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 2022-02-01 | 0.652 | 0.027 | \"\"\" if isinstance ( probe , BaseProbe ): probe . is_computed_probe () else : raise TypeError ( \"Unsupported type {} for probe.\" . format ( type ( probe ))) predictor = filter_table_by_date ( table = probe . predictor , table_name = \"predictor\" , start_date = start_date , end_date = end_date , ) index = probe . _index estimates = self . fit_process ( predictor = predictor , index = index , ** kwargs , ) metrics = self . _compute_metrics ( predictor = predictor , estimates = estimates , index = index , metric_functions = metric_functions , ) if metrics is not None : self . _metrics = list ( metrics . columns . difference ( index )) self . estimates = estimates . merge ( metrics , on = index ) else : self . estimates = estimates self . is_computed_estimates () self . params = kwargs","title":"fit()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.predict","text":"predict ( probe : BaseProbe ) -> pd . DataFrame Computes the predicted probe by using the estimates PARAMETER DESCRIPTION probe Target variable to be predicted TYPE: BaseProbe Examples: from edsteva.models.step_function import StepFunction step_function_model . predict ( visit ) . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c c_fit Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 0.758 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 0.758 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 0 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.784 0.874 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.974 0.912 Source code in edsteva/models/base.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def predict ( self , probe : BaseProbe , ) -> pd . DataFrame : \"\"\"Computes the predicted probe by using the estimates Parameters ---------- probe : BaseProbe Target variable to be predicted Examples -------- ```python from edsteva.models.step_function import StepFunction step_function_model.predict(visit).head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" predictor = probe . predictor index = probe . _index prediction = self . predict_process ( predictor = predictor , index = index ) return prediction","title":"predict()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.load","text":"load ( path = None ) -> None Loads a Model from local PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . load ( path = probe_path ) Source code in edsteva/models/base.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def load ( self , path = None ) -> None : \"\"\"Loads a Model from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" if not path : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" loaded_model = load_object ( path ) self . __dict__ = loaded_model . __dict__ . copy () self . path = path","title":"load()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.save","text":"save ( path : str = None , name : str = None ) -> bool Saves computed Model instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None name EXAMPLE : \"fitted_visit\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . compute ( data ) visit . save ( path = probe_path ) Source code in edsteva/models/base.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"fitted_visit\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_estimates () if not path : if name : self . name = name path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" self . path = path save_object ( self , path )","title":"save()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.delete","text":"delete ( path : str = None ) -> bool Delete the saved Model instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/models/base.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def delete ( self , path : str = None ) -> bool : \"\"\"Delete the saved Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = CACHE_DIR / \"edsteva\" / \"models\" / f \" { self . name . lower () } .pickle\" delete_object ( self , path )","title":"delete()"},{"location":"reference/models/base/#edsteva.models.base.BaseModel.is_predictable_probe","text":"is_predictable_probe ( predictor : pd . DataFrame , index : List [ str ] ) -> pd . DataFrame Raises an error if the model has not been fitted on the input predictor. PARAMETER DESCRIPTION predictor Target DataFrame to be predicted TYPE: pd . DataFrame index List of the columns given by Probe._index TYPE: List [ str ] RETURNS DESCRIPTION pd . DataFrame Predictor along with the fitted estimates RAISES DESCRIPTION Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Source code in edsteva/models/base.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def is_predictable_probe ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Raises an error if the model has not been fitted on the input predictor. Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Predictor along with the fitted estimates Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe \"\"\" prediction = predictor . merge ( self . estimates , on = index , how = \"left\" , validate = \"many_to_one\" , indicator = True ) if ( prediction [ \"_merge\" ] == \"both\" ) . all (): prediction = prediction . drop ( columns = \"_merge\" ) return prediction else : raise Exception ( \"Some indexes have no associated estimates, the model must be fitted on an adequate probe\" )","title":"is_predictable_probe()"},{"location":"reference/models/rectangle_function/","text":"edsteva.models.rectangle_function","title":"`edsteva.models.rectangle_function`"},{"location":"reference/models/rectangle_function/#edstevamodelsrectangle_function","text":"","title":"edsteva.models.rectangle_function"},{"location":"reference/models/rectangle_function/rectangle_function/","text":"edsteva.models.rectangle_function.rectangle_function RectangleFunction Bases: BaseModel It models the completeness predictor \\(c(t)\\) as a rectangle function \\(f_{t_0, c_0, t_1}(t)\\) as follow: \\[ f_{t_0, c_0, t_1}(t) = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) \\] It computes the following estimates \\((t_0, c_0, t_1)\\) : the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic time \\(t_1\\) estimates the time after which the data is not available anymore. the characteristic value \\(c_0\\) estimates the completeness between \\(t_0\\) and \\(t_1\\) . ATTRIBUTE DESCRIPTION _coefs Model coefficients VALUE : [\"t_0\", \"c_0\", \"t_1\"] TYPE: List [ str ] Example from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 t_1 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 2020-05-01 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 2013-04-01 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 2022-03-01 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 2019-08-01 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 2022-08-01 0.027 Source code in edsteva/models/rectangle_function/rectangle_function.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class RectangleFunction ( BaseModel ): r \"\"\"It models the completeness predictor $c(t)$ as a rectangle function $f_{t_0, c_0, t_1}(t)$ as follow: $$ f_{t_0, c_0, t_1}(t) = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) $$ It computes the following estimates $(t_0, c_0, t_1)$: - the characteristic time $t_0$ estimates the time after which the data is available. - the characteristic time $t_1$ estimates the time after which the data is not available anymore. - the characteristic value $c_0$ estimates the completeness between $t_0$ and $t_1$. Attributes ---------- _coefs: List[str] Model coefficients **VALUE**: ``[\"t_0\", \"c_0\", \"t_1\"]`` Example ---------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | t_1 | error | | :----------------------- | :----------- | :-------- | :--------- | :---- | :--------- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg' | 2019-05-01 | 0.397 | 2020-05-01 | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | 2013-04-01 | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Hospit' | 2021-03-01 | 0.677 | 2022-03-01 | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | 2019-08-01 | 0.014 | | H\u00f4pital | 8312022130 | 'Hospit' | 2022-02-01 | 0.652 | 2022-08-01 | 0.027 | \"\"\" _coefs = [ \"t_0\" , \"c_0\" , \"t_1\" ] def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs , ): \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" return algo ( predictor , index , ** kwargs ) def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ): \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor , index ) rect_mask = ( prediction [ \"date\" ] >= prediction [ \"t_0\" ]) & ( prediction [ \"date\" ] <= prediction [ \"t_1\" ] ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( rect_mask , 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics ) def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error between $t_0$ and $t_1$. Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_between_t0_t1 ( predictor = predictor , estimates = estimates , index = index , ) fit_process fit_process ( predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) Script to be used by fit() PARAMETER DESCRIPTION predictor Target variable to be fitted TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ], optional DEFAULT: None algo Algorithm used for the coefficients estimation ( \\(t_0\\) and \\(c_0\\) ) TYPE: Callable , optional DEFAULT: algos.loss_minimization Source code in edsteva/models/rectangle_function/rectangle_function.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs , ): \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" return algo ( predictor , index , ** kwargs ) predict_process predict_process ( predictor : pd . DataFrame , index : List [ str ]) Script to be used by predict() PARAMETER DESCRIPTION predictor Target DataFrame to be predicted TYPE: pd . DataFrame index List of the columns given by Probe._index TYPE: List [ str ] RETURNS DESCRIPTION pd . DataFrame Prediction RAISES DESCRIPTION Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples: care_site_level care_site_id care_site_short_name stay_type date n_visit c c_fit Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 0.758 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 0.758 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 0 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.784 0.874 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.974 0.912 Source code in edsteva/models/rectangle_function/rectangle_function.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ): \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor , index ) rect_mask = ( prediction [ \"date\" ] >= prediction [ \"t_0\" ]) & ( prediction [ \"date\" ] <= prediction [ \"t_1\" ] ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( rect_mask , 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics ) default_metrics default_metrics ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ) Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error between \\(t_0\\) and \\(t_1\\) . PARAMETER DESCRIPTION predictor Target DataFrame describing the completeness predictor \\(c(t)\\) TYPE: pd . DataFrame estimates Target DataFrame describing the estimates \\((\\hat{t_0}, \\hat{c_0})\\) TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/models/rectangle_function/rectangle_function.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error between $t_0$ and $t_1$. Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_between_t0_t1 ( predictor = predictor , estimates = estimates , index = index , )","title":"rectangle_function"},{"location":"reference/models/rectangle_function/rectangle_function/#edstevamodelsrectangle_functionrectangle_function","text":"","title":"edsteva.models.rectangle_function.rectangle_function"},{"location":"reference/models/rectangle_function/rectangle_function/#edsteva.models.rectangle_function.rectangle_function.RectangleFunction","text":"Bases: BaseModel It models the completeness predictor \\(c(t)\\) as a rectangle function \\(f_{t_0, c_0, t_1}(t)\\) as follow: \\[ f_{t_0, c_0, t_1}(t) = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) \\] It computes the following estimates \\((t_0, c_0, t_1)\\) : the characteristic time \\(t_0\\) estimates the time after which the data is available. the characteristic time \\(t_1\\) estimates the time after which the data is not available anymore. the characteristic value \\(c_0\\) estimates the completeness between \\(t_0\\) and \\(t_1\\) . ATTRIBUTE DESCRIPTION _coefs Model coefficients VALUE : [\"t_0\", \"c_0\", \"t_1\"] TYPE: List [ str ]","title":"RectangleFunction"},{"location":"reference/models/rectangle_function/rectangle_function/#edsteva.models.rectangle_function.rectangle_function.RectangleFunction--example","text":"from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 t_1 error Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg' 2019-05-01 0.397 2020-05-01 0.040 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 2013-04-01 0.028 P\u00f4le/DMU 8312027648 'Hospit' 2021-03-01 0.677 2022-03-01 0.022 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 2019-08-01 0.014 H\u00f4pital 8312022130 'Hospit' 2022-02-01 0.652 2022-08-01 0.027 Source code in edsteva/models/rectangle_function/rectangle_function.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class RectangleFunction ( BaseModel ): r \"\"\"It models the completeness predictor $c(t)$ as a rectangle function $f_{t_0, c_0, t_1}(t)$ as follow: $$ f_{t_0, c_0, t_1}(t) = c_0 \\ \\mathbb{1}_{t_0 \\leq t \\leq t_1}(t) $$ It computes the following estimates $(t_0, c_0, t_1)$: - the characteristic time $t_0$ estimates the time after which the data is available. - the characteristic time $t_1$ estimates the time after which the data is not available anymore. - the characteristic value $c_0$ estimates the completeness between $t_0$ and $t_1$. Attributes ---------- _coefs: List[str] Model coefficients **VALUE**: ``[\"t_0\", \"c_0\", \"t_1\"]`` Example ---------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | t_1 | error | | :----------------------- | :----------- | :-------- | :--------- | :---- | :--------- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg' | 2019-05-01 | 0.397 | 2020-05-01 | 0.040 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | 2013-04-01 | 0.028 | | P\u00f4le/DMU | 8312027648 | 'Hospit' | 2021-03-01 | 0.677 | 2022-03-01 | 0.022 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | 2019-08-01 | 0.014 | | H\u00f4pital | 8312022130 | 'Hospit' | 2022-02-01 | 0.652 | 2022-08-01 | 0.027 | \"\"\" _coefs = [ \"t_0\" , \"c_0\" , \"t_1\" ] def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs , ): \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" return algo ( predictor , index , ** kwargs ) def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ): \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor , index ) rect_mask = ( prediction [ \"date\" ] >= prediction [ \"t_0\" ]) & ( prediction [ \"date\" ] <= prediction [ \"t_1\" ] ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( rect_mask , 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics ) def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error between $t_0$ and $t_1$. Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_between_t0_t1 ( predictor = predictor , estimates = estimates , index = index , )","title":"Example"},{"location":"reference/models/rectangle_function/rectangle_function/#edsteva.models.rectangle_function.rectangle_function.RectangleFunction.fit_process","text":"fit_process ( predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) Script to be used by fit() PARAMETER DESCRIPTION predictor Target variable to be fitted TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ], optional DEFAULT: None algo Algorithm used for the coefficients estimation ( \\(t_0\\) and \\(c_0\\) ) TYPE: Callable , optional DEFAULT: algos.loss_minimization Source code in edsteva/models/rectangle_function/rectangle_function.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs , ): \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" return algo ( predictor , index , ** kwargs )","title":"fit_process()"},{"location":"reference/models/rectangle_function/rectangle_function/#edsteva.models.rectangle_function.rectangle_function.RectangleFunction.predict_process","text":"predict_process ( predictor : pd . DataFrame , index : List [ str ]) Script to be used by predict() PARAMETER DESCRIPTION predictor Target DataFrame to be predicted TYPE: pd . DataFrame index List of the columns given by Probe._index TYPE: List [ str ] RETURNS DESCRIPTION pd . DataFrame Prediction RAISES DESCRIPTION Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples: care_site_level care_site_id care_site_short_name stay_type date n_visit c c_fit Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 0.758 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 0.758 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 0 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.784 0.874 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.974 0.912 Source code in edsteva/models/rectangle_function/rectangle_function.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ): \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor , index ) rect_mask = ( prediction [ \"date\" ] >= prediction [ \"t_0\" ]) & ( prediction [ \"date\" ] <= prediction [ \"t_1\" ] ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( rect_mask , 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics )","title":"predict_process()"},{"location":"reference/models/rectangle_function/rectangle_function/#edsteva.models.rectangle_function.rectangle_function.RectangleFunction.default_metrics","text":"default_metrics ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ) Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error between \\(t_0\\) and \\(t_1\\) . PARAMETER DESCRIPTION predictor Target DataFrame describing the completeness predictor \\(c(t)\\) TYPE: pd . DataFrame estimates Target DataFrame describing the estimates \\((\\hat{t_0}, \\hat{c_0})\\) TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/models/rectangle_function/rectangle_function.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error between $t_0$ and $t_1$. Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_between_t0_t1 ( predictor = predictor , estimates = estimates , index = index , )","title":"default_metrics()"},{"location":"reference/models/rectangle_function/algos/","text":"edsteva.models.rectangle_function.algos","title":"`edsteva.models.rectangle_function.algos`"},{"location":"reference/models/rectangle_function/algos/#edstevamodelsrectangle_functionalgos","text":"","title":"edsteva.models.rectangle_function.algos"},{"location":"reference/models/rectangle_function/algos/loss_minimization/","text":"edsteva.models.rectangle_function.algos.loss_minimization loss_minimization loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , min_rect_month_width = 3 , ) Computes the threshold \\(t_0\\) and \\(t_1\\) of a predictor \\(c(t)\\) by minimizing the following loss function: \\[ \\begin{aligned} \\mathcal{L}(t_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}) & = \\underset{t_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, t_1)) \\end{aligned} \\] Where the loss function \\(\\mathcal{l}\\) is by default the L2 distance and the estimated completeness \\(c_0\\) is the mean completeness between \\(t_0\\) and \\(t_1\\) . \\[ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0, t_1}(t)) & = |c(t) - f_{t_0, t_1}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_1} c(t)}{t_1 - t_0} \\end{aligned} \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe. TYPE: pd . DataFrame index Variable from which data is grouped. EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] x_col Column name for the time variable \\(t\\) . TYPE: str , optional DEFAULT: 'date' y_col Column name for the completeness variable \\(c(t)\\) . TYPE: str , optional DEFAULT: 'c' loss_function The loss function \\(\\mathcal{L}\\) . TYPE: Callable , optional DEFAULT: l2_loss min_rect_month_width Min number of months between \\(t_0\\) and \\(t_1\\) . TYPE: int , optional DEFAULT: 3 Source code in edsteva/models/rectangle_function/algos/loss_minimization.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , min_rect_month_width = 3 , ): r \"\"\"Computes the threshold $t_0$ and $t_1$ of a predictor $c(t)$ by minimizing the following loss function: $$ \\begin{aligned} \\mathcal{L}(t_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}) & = \\underset{t_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, t_1)) \\end{aligned} $$ Where the loss function $\\mathcal{l}$ is by default the L2 distance and the estimated completeness $c_0$ is the mean completeness between $t_0$ and $t_1$. $$ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0, t_1}(t)) & = |c(t) - f_{t_0, t_1}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_1} c(t)}{t_1 - t_0} \\end{aligned} $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe. index : List[str] Variable from which data is grouped. **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` x_col : str, optional Column name for the time variable $t$. y_col : str, optional Column name for the completeness variable $c(t)$. loss_function : Callable, optional The loss function $\\mathcal{L}$. min_rect_month_width : int, optional Min number of months between $t_0$ and $t_1$. \"\"\" check_columns ( df = predictor , required_columns = index + [ x_col , y_col ]) cols = index + [ x_col , y_col ] iter = predictor [ cols ] . groupby ( index ) results = [] for partition , group in iter : row = dict ( zip ( index , partition )) t_0 , c_0 , t_1 = _compute_one_double_threshold ( group , x_col , y_col , loss_function , min_rect_month_width , ) row [ \"t_0\" ] = t_0 row [ \"c_0\" ] = c_0 row [ \"t_1\" ] = t_1 results . append ( row ) return pd . DataFrame ( results )","title":"loss_minimization"},{"location":"reference/models/rectangle_function/algos/loss_minimization/#edstevamodelsrectangle_functionalgosloss_minimization","text":"","title":"edsteva.models.rectangle_function.algos.loss_minimization"},{"location":"reference/models/rectangle_function/algos/loss_minimization/#edsteva.models.rectangle_function.algos.loss_minimization.loss_minimization","text":"loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , min_rect_month_width = 3 , ) Computes the threshold \\(t_0\\) and \\(t_1\\) of a predictor \\(c(t)\\) by minimizing the following loss function: \\[ \\begin{aligned} \\mathcal{L}(t_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}) & = \\underset{t_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, t_1)) \\end{aligned} \\] Where the loss function \\(\\mathcal{l}\\) is by default the L2 distance and the estimated completeness \\(c_0\\) is the mean completeness between \\(t_0\\) and \\(t_1\\) . \\[ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0, t_1}(t)) & = |c(t) - f_{t_0, t_1}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_1} c(t)}{t_1 - t_0} \\end{aligned} \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe. TYPE: pd . DataFrame index Variable from which data is grouped. EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] x_col Column name for the time variable \\(t\\) . TYPE: str , optional DEFAULT: 'date' y_col Column name for the completeness variable \\(c(t)\\) . TYPE: str , optional DEFAULT: 'c' loss_function The loss function \\(\\mathcal{L}\\) . TYPE: Callable , optional DEFAULT: l2_loss min_rect_month_width Min number of months between \\(t_0\\) and \\(t_1\\) . TYPE: int , optional DEFAULT: 3 Source code in edsteva/models/rectangle_function/algos/loss_minimization.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , min_rect_month_width = 3 , ): r \"\"\"Computes the threshold $t_0$ and $t_1$ of a predictor $c(t)$ by minimizing the following loss function: $$ \\begin{aligned} \\mathcal{L}(t_0, t_1) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0, t_1}(t))}{t_{max} - t_{min}} \\\\ (\\hat{t_0}, \\hat{t_1}) & = \\underset{t_0, t_1}{\\mathrm{argmin}}(\\mathcal{L}(t_0, t_1)) \\end{aligned} $$ Where the loss function $\\mathcal{l}$ is by default the L2 distance and the estimated completeness $c_0$ is the mean completeness between $t_0$ and $t_1$. $$ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0, t_1}(t)) & = |c(t) - f_{t_0, t_1}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_1} c(t)}{t_1 - t_0} \\end{aligned} $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe. index : List[str] Variable from which data is grouped. **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` x_col : str, optional Column name for the time variable $t$. y_col : str, optional Column name for the completeness variable $c(t)$. loss_function : Callable, optional The loss function $\\mathcal{L}$. min_rect_month_width : int, optional Min number of months between $t_0$ and $t_1$. \"\"\" check_columns ( df = predictor , required_columns = index + [ x_col , y_col ]) cols = index + [ x_col , y_col ] iter = predictor [ cols ] . groupby ( index ) results = [] for partition , group in iter : row = dict ( zip ( index , partition )) t_0 , c_0 , t_1 = _compute_one_double_threshold ( group , x_col , y_col , loss_function , min_rect_month_width , ) row [ \"t_0\" ] = t_0 row [ \"c_0\" ] = c_0 row [ \"t_1\" ] = t_1 results . append ( row ) return pd . DataFrame ( results )","title":"loss_minimization()"},{"location":"reference/models/step_function/","text":"edsteva.models.step_function","title":"`edsteva.models.step_function`"},{"location":"reference/models/step_function/#edstevamodelsstep_function","text":"","title":"edsteva.models.step_function"},{"location":"reference/models/step_function/step_function/","text":"edsteva.models.step_function.step_function StepFunction Bases: BaseModel It models the completeness predictor \\(c(t)\\) as a step function \\(f_{t_0, c_0}(t)\\) as follow: \\[ f_{t_0, c_0}(t) = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\] It computes the following estimates \\((t_0, c_0)\\) : the characteristic time \\(t_0\\) estimates the time after which the data is available the characteristic value \\(c_0\\) estimates the stabilized routine completeness ATTRIBUTE DESCRIPTION _coefs Model coefficients VALUE : [\"t_0\", \"c_0\"] TYPE: List [ str ] Example from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 2019-05-01 0.397 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 P\u00f4le/DMU 8312027648 'Urg_Hospit' 2021-03-01 0.677 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 H\u00f4pital 8312022130 'Urg_Hospit' 2022-02-01 0.652 Source code in edsteva/models/step_function/step_function.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class StepFunction ( BaseModel ): r \"\"\"It models the completeness predictor $c(t)$ as a step function $f_{t_0, c_0}(t)$ as follow: $$ f_{t_0, c_0}(t) = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) $$ It computes the following estimates $(t_0, c_0)$: - the characteristic time $t_0$ estimates the time after which the data is available - the characteristic value $c_0$ estimates the stabilized routine completeness Attributes ---------- _coefs: List[str] Model coefficients **VALUE**: ``[\"t_0\", \"c_0\"]`` Example ---------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | | :----------------------- | :----------- | :----------- | :--------- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 2019-05-01 | 0.397 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 2021-03-01 | 0.677 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 2022-02-01 | 0.652 | \"\"\" _coefs = [ \"t_0\" , \"c_0\" ] def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) -> None : \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" estimates = algo ( predictor = predictor , index = index , ** kwargs ) return estimates [ index + self . _coefs ] def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor = predictor , index = index ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( prediction [ \"date\" ] >= prediction [ \"t_0\" ], 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics ) def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error computed after $t_0$ Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_after_t0 ( predictor = predictor , estimates = estimates , index = index ) fit_process fit_process ( predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) -> None Script to be used by fit() PARAMETER DESCRIPTION predictor Target variable to be fitted TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ], optional DEFAULT: None algo Algorithm used for the coefficients estimation ( \\(t_0\\) and \\(c_0\\) ) TYPE: Callable , optional DEFAULT: algos.loss_minimization Source code in edsteva/models/step_function/step_function.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) -> None : \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" estimates = algo ( predictor = predictor , index = index , ** kwargs ) return estimates [ index + self . _coefs ] predict_process predict_process ( predictor : pd . DataFrame , index : List [ str ] ) -> pd . DataFrame Script to be used by predict() PARAMETER DESCRIPTION predictor Target DataFrame to be predicted TYPE: pd . DataFrame index List of the columns given by Probe._index TYPE: List [ str ] RETURNS DESCRIPTION pd . DataFrame Prediction RAISES DESCRIPTION Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples: care_site_level care_site_id care_site_short_name stay_type date n_visit c c_fit Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 0.758 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 0.758 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 0 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.784 0.874 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.974 0.912 Source code in edsteva/models/step_function/step_function.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor = predictor , index = index ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( prediction [ \"date\" ] >= prediction [ \"t_0\" ], 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics ) default_metrics default_metrics ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ) Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error computed after \\(t_0\\) PARAMETER DESCRIPTION predictor Target DataFrame describing the completeness predictor \\(c(t)\\) TYPE: pd . DataFrame estimates Target DataFrame describing the estimates \\((\\hat{t_0}, \\hat{c_0})\\) TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/models/step_function/step_function.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error computed after $t_0$ Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_after_t0 ( predictor = predictor , estimates = estimates , index = index )","title":"step_function"},{"location":"reference/models/step_function/step_function/#edstevamodelsstep_functionstep_function","text":"","title":"edsteva.models.step_function.step_function"},{"location":"reference/models/step_function/step_function/#edsteva.models.step_function.step_function.StepFunction","text":"Bases: BaseModel It models the completeness predictor \\(c(t)\\) as a step function \\(f_{t_0, c_0}(t)\\) as follow: \\[ f_{t_0, c_0}(t) = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) \\] It computes the following estimates \\((t_0, c_0)\\) : the characteristic time \\(t_0\\) estimates the time after which the data is available the characteristic value \\(c_0\\) estimates the stabilized routine completeness ATTRIBUTE DESCRIPTION _coefs Model coefficients VALUE : [\"t_0\", \"c_0\"] TYPE: List [ str ]","title":"StepFunction"},{"location":"reference/models/step_function/step_function/#edsteva.models.step_function.step_function.StepFunction--example","text":"from edsteva.models.step_function import StepFunction step_function_model = StepFunction () step_function_model . fit ( probe ) step_function_model . estimates . head () care_site_level care_site_id stay_type t_0 c_0 Unit\u00e9 Fonctionnelle (UF) 8312056386 'Urg_Hospit' 2019-05-01 0.397 Unit\u00e9 Fonctionnelle (UF) 8312056386 'All' 2011-04-01 0.583 P\u00f4le/DMU 8312027648 'Urg_Hospit' 2021-03-01 0.677 P\u00f4le/DMU 8312027648 'All' 2018-08-01 0.764 H\u00f4pital 8312022130 'Urg_Hospit' 2022-02-01 0.652 Source code in edsteva/models/step_function/step_function.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class StepFunction ( BaseModel ): r \"\"\"It models the completeness predictor $c(t)$ as a step function $f_{t_0, c_0}(t)$ as follow: $$ f_{t_0, c_0}(t) = c_0 \\ \\mathbb{1}_{t \\geq t_0}(t) $$ It computes the following estimates $(t_0, c_0)$: - the characteristic time $t_0$ estimates the time after which the data is available - the characteristic value $c_0$ estimates the stabilized routine completeness Attributes ---------- _coefs: List[str] Model coefficients **VALUE**: ``[\"t_0\", \"c_0\"]`` Example ---------- ```python from edsteva.models.step_function import StepFunction step_function_model = StepFunction() step_function_model.fit(probe) step_function_model.estimates.head() ``` | care_site_level | care_site_id | stay_type | t_0 | c_0 | | :----------------------- | :----------- | :----------- | :--------- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'Urg_Hospit' | 2019-05-01 | 0.397 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | 'All' | 2011-04-01 | 0.583 | | P\u00f4le/DMU | 8312027648 | 'Urg_Hospit' | 2021-03-01 | 0.677 | | P\u00f4le/DMU | 8312027648 | 'All' | 2018-08-01 | 0.764 | | H\u00f4pital | 8312022130 | 'Urg_Hospit' | 2022-02-01 | 0.652 | \"\"\" _coefs = [ \"t_0\" , \"c_0\" ] def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) -> None : \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" estimates = algo ( predictor = predictor , index = index , ** kwargs ) return estimates [ index + self . _coefs ] def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor = predictor , index = index ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( prediction [ \"date\" ] >= prediction [ \"t_0\" ], 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics ) def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error computed after $t_0$ Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_after_t0 ( predictor = predictor , estimates = estimates , index = index )","title":"Example"},{"location":"reference/models/step_function/step_function/#edsteva.models.step_function.step_function.StepFunction.fit_process","text":"fit_process ( predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) -> None Script to be used by fit() PARAMETER DESCRIPTION predictor Target variable to be fitted TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ], optional DEFAULT: None algo Algorithm used for the coefficients estimation ( \\(t_0\\) and \\(c_0\\) ) TYPE: Callable , optional DEFAULT: algos.loss_minimization Source code in edsteva/models/step_function/step_function.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def fit_process ( self , predictor : pd . DataFrame , index : List [ str ] = None , algo : Callable = algos . loss_minimization , ** kwargs ) -> None : \"\"\"Script to be used by [``fit()``][edsteva.models.base.BaseModel.fit] Parameters ---------- predictor : pd.DataFrame Target variable to be fitted index : List[str], optional Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` algo : Callable, optional Algorithm used for the coefficients estimation ($t_0$ and $c_0$) \"\"\" estimates = algo ( predictor = predictor , index = index , ** kwargs ) return estimates [ index + self . _coefs ]","title":"fit_process()"},{"location":"reference/models/step_function/step_function/#edsteva.models.step_function.step_function.StepFunction.predict_process","text":"predict_process ( predictor : pd . DataFrame , index : List [ str ] ) -> pd . DataFrame Script to be used by predict() PARAMETER DESCRIPTION predictor Target DataFrame to be predicted TYPE: pd . DataFrame index List of the columns given by Probe._index TYPE: List [ str ] RETURNS DESCRIPTION pd . DataFrame Prediction RAISES DESCRIPTION Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples: care_site_level care_site_id care_site_short_name stay_type date n_visit c c_fit Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'Urg_Hospit' 2019-05-01 233.0 0.841 0.758 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 0.758 P\u00f4le/DMU 8312027648 Care site 2 'Urg_Hospit' 2011-03-01 204.0 0.497 0 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.784 0.874 H\u00f4pital 8312022130 Care site 3 'Urg_Hospit' 2022-02-01 9746.0 0.974 0.912 Source code in edsteva/models/step_function/step_function.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def predict_process ( self , predictor : pd . DataFrame , index : List [ str ], ) -> pd . DataFrame : \"\"\"Script to be used by [``predict()``][edsteva.models.base.BaseModel.predict] Parameters ---------- predictor : pd.DataFrame Target DataFrame to be predicted index : List[str] List of the columns given by Probe._index Returns ------- pd.DataFrame Prediction Raises ------ Exception Some indexes have no associated estimates, the model must be fitted on an adequate probe Examples -------- | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | c_fit | | :----------------------- | :----------- | :------------------- | :----------- | :--------- | :------ | :---- | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'Urg_Hospit' | 2019-05-01 | 233.0 | 0.841 | 0.758 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | 0.758 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'Urg_Hospit' | 2011-03-01 | 204.0 | 0.497 | 0 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.784 | 0.874 | | H\u00f4pital | 8312022130 | Care site 3 | 'Urg_Hospit' | 2022-02-01 | 9746.0 | 0.974 | 0.912 | \"\"\" prediction = self . is_predictable_probe ( predictor = predictor , index = index ) prediction [ \"c_hat\" ] = prediction [ \"c_0\" ] . where ( prediction [ \"date\" ] >= prediction [ \"t_0\" ], 0 ) return prediction . drop ( columns = self . _coefs + self . _metrics )","title":"predict_process()"},{"location":"reference/models/step_function/step_function/#edsteva.models.step_function.step_function.StepFunction.default_metrics","text":"default_metrics ( predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ) Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error computed after \\(t_0\\) PARAMETER DESCRIPTION predictor Target DataFrame describing the completeness predictor \\(c(t)\\) TYPE: pd . DataFrame estimates Target DataFrame describing the estimates \\((\\hat{t_0}, \\hat{c_0})\\) TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/models/step_function/step_function.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def default_metrics ( self , predictor : pd . DataFrame , estimates : pd . DataFrame , index : List [ str ], ): r \"\"\"Default metrics used if metric_functions is set to None. Here the default metric is the mean squared error computed after $t_0$ Parameters ---------- predictor : pd.DataFrame Target DataFrame describing the completeness predictor $c(t)$ estimates : pd.DataFrame Target DataFrame describing the estimates $(\\hat{t_0}, \\hat{c_0})$ index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` \"\"\" return error_after_t0 ( predictor = predictor , estimates = estimates , index = index )","title":"default_metrics()"},{"location":"reference/models/step_function/algos/","text":"edsteva.models.step_function.algos","title":"`edsteva.models.step_function.algos`"},{"location":"reference/models/step_function/algos/#edstevamodelsstep_functionalgos","text":"","title":"edsteva.models.step_function.algos"},{"location":"reference/models/step_function/algos/loss_minimization/","text":"edsteva.models.step_function.algos.loss_minimization loss_minimization loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , ) -> pd . DataFrame Computes the threshold \\(t_0\\) of a predictor \\(c(t)\\) by minimizing the following loss function: \\[ \\begin{aligned} \\mathcal{L}(t_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0}(t))}{t_{max} - t_{min}} \\\\ \\hat{t_0} & = \\underset{t_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0)) \\end{aligned} \\] Where the loss function \\(\\mathcal{l}\\) is by default the L2 distance and the estimated completeness \\(c_0\\) is the mean completeness after \\(t_0\\) . \\[ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0}(t)) & = |c(t) - f_{t_0}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_{max}} c(t)}{t_{max} - t_0} \\end{aligned} \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] x_col Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' y_col Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' loss_function The loss function \\(\\mathcal{L}\\) TYPE: Callable , optional DEFAULT: l2_loss Source code in edsteva/models/step_function/algos/loss_minimization.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , ) -> pd . DataFrame : r \"\"\"Computes the threshold $t_0$ of a predictor $c(t)$ by minimizing the following loss function: $$ \\begin{aligned} \\mathcal{L}(t_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0}(t))}{t_{max} - t_{min}} \\\\ \\hat{t_0} & = \\underset{t_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0)) \\end{aligned} $$ Where the loss function $\\mathcal{l}$ is by default the L2 distance and the estimated completeness $c_0$ is the mean completeness after $t_0$. $$ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0}(t)) & = |c(t) - f_{t_0}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_{max}} c(t)}{t_{max} - t_0} \\end{aligned} $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` x_col : str, optional Column name for the time variable $t$ y_col : str, optional Column name for the completeness variable $c(t)$ loss_function : Callable, optional The loss function $\\mathcal{L}$ \"\"\" check_columns ( df = predictor , required_columns = index + [ x_col , y_col ]) cols = index + [ x_col , y_col ] iter = predictor [ cols ] . groupby ( index ) results = [] for partition , group in iter : row = dict ( zip ( index , partition )) t_0 , c_0 = _compute_one_threshold ( group , x_col , y_col , loss_function , ) row [ \"t_0\" ] = t_0 row [ \"c_0\" ] = c_0 results . append ( row ) return pd . DataFrame ( results )","title":"loss_minimization"},{"location":"reference/models/step_function/algos/loss_minimization/#edstevamodelsstep_functionalgosloss_minimization","text":"","title":"edsteva.models.step_function.algos.loss_minimization"},{"location":"reference/models/step_function/algos/loss_minimization/#edsteva.models.step_function.algos.loss_minimization.loss_minimization","text":"loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , ) -> pd . DataFrame Computes the threshold \\(t_0\\) of a predictor \\(c(t)\\) by minimizing the following loss function: \\[ \\begin{aligned} \\mathcal{L}(t_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0}(t))}{t_{max} - t_{min}} \\\\ \\hat{t_0} & = \\underset{t_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0)) \\end{aligned} \\] Where the loss function \\(\\mathcal{l}\\) is by default the L2 distance and the estimated completeness \\(c_0\\) is the mean completeness after \\(t_0\\) . \\[ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0}(t)) & = |c(t) - f_{t_0}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_{max}} c(t)}{t_{max} - t_0} \\end{aligned} \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] x_col Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' y_col Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' loss_function The loss function \\(\\mathcal{L}\\) TYPE: Callable , optional DEFAULT: l2_loss Source code in edsteva/models/step_function/algos/loss_minimization.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def loss_minimization ( predictor : pd . DataFrame , index : List [ str ], x_col : str = \"date\" , y_col : str = \"c\" , loss_function : Callable = l2_loss , ) -> pd . DataFrame : r \"\"\"Computes the threshold $t_0$ of a predictor $c(t)$ by minimizing the following loss function: $$ \\begin{aligned} \\mathcal{L}(t_0) & = \\frac{\\sum_{t = t_{min}}^{t_{max}} \\mathcal{l}(c(t), f_{t_0}(t))}{t_{max} - t_{min}} \\\\ \\hat{t_0} & = \\underset{t_0}{\\mathrm{argmin}}(\\mathcal{L}(t_0)) \\end{aligned} $$ Where the loss function $\\mathcal{l}$ is by default the L2 distance and the estimated completeness $c_0$ is the mean completeness after $t_0$. $$ \\begin{aligned} \\mathcal{l}(c(t), f_{t_0}(t)) & = |c(t) - f_{t_0}(t)|^2 \\\\ c_0 & = \\frac{\\sum_{t = t_0}^{t_{max}} c(t)}{t_{max} - t_0} \\end{aligned} $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` x_col : str, optional Column name for the time variable $t$ y_col : str, optional Column name for the completeness variable $c(t)$ loss_function : Callable, optional The loss function $\\mathcal{L}$ \"\"\" check_columns ( df = predictor , required_columns = index + [ x_col , y_col ]) cols = index + [ x_col , y_col ] iter = predictor [ cols ] . groupby ( index ) results = [] for partition , group in iter : row = dict ( zip ( index , partition )) t_0 , c_0 = _compute_one_threshold ( group , x_col , y_col , loss_function , ) row [ \"t_0\" ] = t_0 row [ \"c_0\" ] = c_0 results . append ( row ) return pd . DataFrame ( results )","title":"loss_minimization()"},{"location":"reference/models/step_function/algos/quantile/","text":"edsteva.models.step_function.algos.quantile c_0_from_quantile c_0_from_quantile ( predictor : pd . DataFrame , index : List [ str ], q : float = 0.8 , x : str = \"date\" , y : str = \"c\" , ) -> pd . DataFrame Compute the quantile on the given y-axis. Column \\(c_0\\) is created. \\[ \\hat{c_0} = x^{th} \\text{ quantile of } c(t) \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] q Quantile value TYPE: float , optional DEFAULT: 0.8 x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' Source code in edsteva/models/step_function/algos/quantile.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def c_0_from_quantile ( predictor : pd . DataFrame , index : List [ str ], q : float = 0.8 , x : str = \"date\" , y : str = \"c\" , ) -> pd . DataFrame : r \"\"\"Compute the quantile on the given y-axis. Column $c_0$ is created. $$ \\hat{c_0} = x^{th} \\text{ quantile of } c(t) $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` q : float, optional Quantile value x : str, optional Column name for the time variable $t$ y : str, optional Column name for the completeness variable $c(t)$ \"\"\" check_columns ( df = predictor , required_columns = index + [ x , y ]) quantile = ( predictor . groupby ( index )[[ y ]] . agg ( lambda g : np . quantile ( g , q = q )) . rename ( columns = { y : \"c_0\" }) ) return predictor . merge ( quantile , on = index ) t_0_from_c_0 t_0_from_c_0 ( predictor : pd . DataFrame , index : List [ str ], x : str = \"date\" , y : str = \"c\" , threshold : str = \"c_0\" , ) -> pd . DataFrame Compute \\(t_0\\) column using value of \\(c_0\\) Returns the first date at which values are greater than \\(c_0\\) : \\[ \\hat{t_0} = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' threshold Column name for the threshold variable \\(t_0\\) TYPE: str , optional DEFAULT: 'c_0' Source code in edsteva/models/step_function/algos/quantile.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def t_0_from_c_0 ( predictor : pd . DataFrame , index : List [ str ], x : str = \"date\" , y : str = \"c\" , threshold : str = \"c_0\" , ) -> pd . DataFrame : r \"\"\"Compute $t_0$ column using value of $c_0$ Returns the first date at which values are greater than $c_0$: $$ \\hat{t_0} = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped x : str, optional Column name for the time variable $t$ y : str, optional Column name for the completeness variable $c(t)$ threshold : str, optional Column name for the threshold variable $t_0$ \"\"\" check_columns ( df = predictor , required_columns = index + [ x , y , threshold ]) threshold = ( predictor [ predictor [ y ] > predictor [ threshold ]] . groupby ( index )[[ x ]] . min () . rename ( columns = { x : \"t_0\" }) ) return predictor . merge ( threshold , on = index )","title":"quantile"},{"location":"reference/models/step_function/algos/quantile/#edstevamodelsstep_functionalgosquantile","text":"","title":"edsteva.models.step_function.algos.quantile"},{"location":"reference/models/step_function/algos/quantile/#edsteva.models.step_function.algos.quantile.c_0_from_quantile","text":"c_0_from_quantile ( predictor : pd . DataFrame , index : List [ str ], q : float = 0.8 , x : str = \"date\" , y : str = \"c\" , ) -> pd . DataFrame Compute the quantile on the given y-axis. Column \\(c_0\\) is created. \\[ \\hat{c_0} = x^{th} \\text{ quantile of } c(t) \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped EXAMPLE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] q Quantile value TYPE: float , optional DEFAULT: 0.8 x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' Source code in edsteva/models/step_function/algos/quantile.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def c_0_from_quantile ( predictor : pd . DataFrame , index : List [ str ], q : float = 0.8 , x : str = \"date\" , y : str = \"c\" , ) -> pd . DataFrame : r \"\"\"Compute the quantile on the given y-axis. Column $c_0$ is created. $$ \\hat{c_0} = x^{th} \\text{ quantile of } c(t) $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped **EXAMPLE**: `[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]` q : float, optional Quantile value x : str, optional Column name for the time variable $t$ y : str, optional Column name for the completeness variable $c(t)$ \"\"\" check_columns ( df = predictor , required_columns = index + [ x , y ]) quantile = ( predictor . groupby ( index )[[ y ]] . agg ( lambda g : np . quantile ( g , q = q )) . rename ( columns = { y : \"c_0\" }) ) return predictor . merge ( quantile , on = index )","title":"c_0_from_quantile()"},{"location":"reference/models/step_function/algos/quantile/#edsteva.models.step_function.algos.quantile.t_0_from_c_0","text":"t_0_from_c_0 ( predictor : pd . DataFrame , index : List [ str ], x : str = \"date\" , y : str = \"c\" , threshold : str = \"c_0\" , ) -> pd . DataFrame Compute \\(t_0\\) column using value of \\(c_0\\) Returns the first date at which values are greater than \\(c_0\\) : \\[ \\hat{t_0} = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) \\] PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x Column name for the time variable \\(t\\) TYPE: str , optional DEFAULT: 'date' y Column name for the completeness variable \\(c(t)\\) TYPE: str , optional DEFAULT: 'c' threshold Column name for the threshold variable \\(t_0\\) TYPE: str , optional DEFAULT: 'c_0' Source code in edsteva/models/step_function/algos/quantile.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def t_0_from_c_0 ( predictor : pd . DataFrame , index : List [ str ], x : str = \"date\" , y : str = \"c\" , threshold : str = \"c_0\" , ) -> pd . DataFrame : r \"\"\"Compute $t_0$ column using value of $c_0$ Returns the first date at which values are greater than $c_0$: $$ \\hat{t_0} = \\underset{t}{\\mathrm{argmin}}(c(t) \\geq \\hat{c_0}) $$ Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped x : str, optional Column name for the time variable $t$ y : str, optional Column name for the completeness variable $c(t)$ threshold : str, optional Column name for the threshold variable $t_0$ \"\"\" check_columns ( df = predictor , required_columns = index + [ x , y , threshold ]) threshold = ( predictor [ predictor [ y ] > predictor [ threshold ]] . groupby ( index )[[ x ]] . min () . rename ( columns = { x : \"t_0\" }) ) return predictor . merge ( threshold , on = index )","title":"t_0_from_c_0()"},{"location":"reference/probes/","text":"edsteva.probes","title":"`edsteva.probes`"},{"location":"reference/probes/#edstevaprobes","text":"","title":"edsteva.probes"},{"location":"reference/probes/base/","text":"edsteva.probes.base BaseProbe Base class for Probes ATTRIBUTE DESCRIPTION _schema The columns a predictor must have VALUE : [\"care_site_id\", \"care_site_level\", \"stay_type\", \"date\", \"c\"] TYPE: List [ str ] predictor Available with the compute() method TYPE: pd . DataFrame _cache_predictor Available with the compute() method It is a copy of the predictor DataFrame used to reset_predictor() TYPE: pd . DataFrame care_site_relationship Available with the compute() method It describes the care site structure (cf. get_care_site_relationship() ) TYPE: pd . DataFrame Source code in edsteva/probes/base.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 class BaseProbe ( metaclass = ABCMeta ): \"\"\"Base class for Probes Attributes ---------- _schema: List[str] The columns a predictor must have **VALUE**: ``[\"care_site_id\", \"care_site_level\", \"stay_type\", \"date\", \"c\"]`` predictor: pd.DataFrame Available with the [``compute()``][edsteva.probes.base.BaseProbe.compute] method _cache_predictor: pd.DataFrame Available with the [``compute()``][edsteva.probes.base.BaseProbe.compute] method It is a copy of the predictor DataFrame used to [``reset_predictor()``][edsteva.probes.base.BaseProbe.reset_predictor] care_site_relationship: pd.DataFrame Available with the [``compute()``][edsteva.probes.base.BaseProbe.compute] method It describes the care site structure (cf. [``get_care_site_relationship()``][edsteva.probes.utils.get_care_site_relationship]) \"\"\" def __init__ ( self ): self . is_valid_probe () self . name = self . _get_name () _schema = [ \"care_site_id\" , \"care_site_level\" , \"stay_type\" , \"date\" , \"c\" ] def validate_input_data ( self , data : Data ) -> None : \"\"\"Raises an error if the input data is not valid Parameters ---------- data: Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] \"\"\" if not isinstance ( data , Data . __args__ ): raise TypeError ( \"Unsupported type {} for data\" . format ( type ( data ))) check_tables ( data = data , required_tables = [ \"visit_occurrence\" , \"care_site\" , \"fact_relationship\" , ], ) def is_valid_probe ( self ) -> None : \"\"\"Raises an error if the instantiated Probe is not valid\"\"\" if not hasattr ( self , \"_index\" ): raise Exception ( \"Probe must have _index attribute. Please review the code of your probe\" ) def is_computed_probe ( self ) -> None : \"\"\"Raises an error if the Probe has not been computed properly\"\"\" if hasattr ( self , \"predictor\" ): if not isinstance ( self . predictor , pd . DataFrame ): raise TypeError ( \"Predictor must be a Pandas DataFrame and not a {} , please review the process method or your arguments\" . format ( type ( self . predictor ) ) ) if self . predictor . empty : raise Exception ( \"Predictor is empty, please review the process method or your arguments\" ) check_columns ( self . predictor , required_columns = self . _schema , ) if not self . predictor . dtypes [ \"date\" ] == \"datetime64[ns]\" : try : self . predictor [ \"date\" ] = self . predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) except Exception as e : raise TypeError ( \"Predictor column 'date' type is {} and cannot convert to datetime and return the following error: {} . Please review the process method or your arguments\" . format ( self . predictor . dtypes [ \"date\" ], e ) ) else : raise Exception ( \"Predictor has not been computed, please use the compute method as follow: Predictor.compute()\" ) def impute_missing_date ( self , only_impute_per_care_site : bool = False , ) -> pd . DataFrame : \"\"\"Impute missing date with 0 on the predictor of a probe. Parameters ---------- only_impute_per_care_site : bool, optional If True it will only impute missing date between the first and the last observation of each care site. If False it will impute missing data on the entire study period whatever the care site \"\"\" # Check if probe has been computed. self . is_computed_probe () # Set start_date to the beginning of the month. date_index = pd . date_range ( start = self . start_date , end = self . end_date , freq = \"MS\" , closed = \"left\" , ) date_index = pd . DataFrame ({ \"date\" : date_index }) # Precompute the mapping: # {'H\u00f4pital-1': {'min': Timestamp('2010-06-01'), 'max': Timestamp('2019-11-01')} if only_impute_per_care_site : site_to_min_max_ds = ( self . predictor . groupby ([ \"care_site_short_name\" ])[ \"date\" ] . agg ([ min , max ]) . to_dict ( \"index\" ) ) partition_cols = self . _index + [ \"care_site_short_name\" ] groups = [] for partition , group in self . predictor . groupby ( partition_cols ): group = date_index . merge ( group , on = \"date\" , how = \"left\" ) # Filter on each care site timeframe. if only_impute_per_care_site : care_site_short_name = partition [ - 1 ] ds_min = site_to_min_max_ds [ care_site_short_name ][ \"min\" ] ds_max = site_to_min_max_ds [ care_site_short_name ][ \"max\" ] group = group . loc [( group [ \"date\" ] >= ds_min ) & ( group [ \"date\" ] <= ds_max )] # Fill specific partition values. for key , val in zip ( partition_cols , partition ): group [ key ] = val # Fill remaining NaN from counts values with 0. group . fillna ( 0 , inplace = True ) groups . append ( group ) self . predictor = pd . concat ( groups ) @abstractmethod def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Process the data in order to obtain a predictor table\"\"\" def compute ( self , data : Data , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , impute_missing_dates : bool = True , only_impute_per_care_site : bool = False , ** kwargs , ) -> None : \"\"\"Calls [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] Here are the following computation steps: - check if input data is valid with [``validate_input_data()``][edsteva.probes.base.BaseProbe.validate_input_data] method - query care site relationship table with [``get_care_site_relationship()``][edsteva.probes.utils.get_care_site_relationship] - compute predictor with [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] method - check if predictor is valid with [``is_computed_probe()``][edsteva.probes.base.BaseProbe.is_computed_probe] method Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` Attributes ---------- Add to the Probe th following attributes: - predictor is the target DataFrame - _cache_predictor is a copy of the target DataFrame (used to [``reset_predictor()``][edsteva.probes.base.BaseProbe.reset_predictor]) - care_site_relationship is a DataFrame with the hierarchy of the care site structure Examples ------- ```python from edsteva.probes import VisitProbe visit = VisitProbe() visit.compute( data, stay_types={\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}, care_site_levels=[\"Hospital\", \"Pole\", \"UF\"], ) visit.predictor.head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | | :----------------------- | :----------- | :------------------- | :-------------- | :--------- | :------ | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | Urg_and_consult | 2019-05-01 | 233.0 | 0.841 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | | P\u00f4le/DMU | 8312027648 | Care site 2 | Urg_and_consult | 2011-03-01 | 204.0 | 0.497 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.274 | | H\u00f4pital | 8312022130 | Care site 3 | Urg_and_consult | 2022-02-01 | 9746.0 | 0.769 | \"\"\" self . validate_input_data ( data = data ) care_site_relationship = get_care_site_relationship ( data = data ) self . predictor = self . compute_process ( data = data , care_site_relationship = care_site_relationship , start_date = start_date , end_date = end_date , care_site_levels = care_site_levels , stay_types = stay_types , care_site_ids = care_site_ids , ** kwargs , ) self . is_computed_probe () self . start_date = ( pd . to_datetime ( start_date ) if start_date else self . predictor [ \"date\" ] . min () ) self . end_date = ( pd . to_datetime ( end_date ) if end_date else self . predictor [ \"date\" ] . max () ) if impute_missing_dates : self . impute_missing_date ( only_impute_per_care_site = only_impute_per_care_site , ) self . cache_predictor () self . care_site_relationship = care_site_relationship def reset_predictor ( self , ) -> None : \"\"\"Reset the predictor to its initial state\"\"\" self . predictor = self . _cache_predictor . copy () def cache_predictor ( self , ) -> None : \"\"\"Cache the predictor\"\"\" self . _cache_predictor = self . predictor . copy () logger . info ( \"Cache the predictor, you can reset the predictor to this state with the method reset_predictor\" ) def filter_care_site ( self , care_site_ids : Union [ int , List [ int ]] = None , care_site_short_names : Union [ str , List [ str ]] = None , ) -> None : \"\"\"Filters all the care sites related to the selected care sites. Parameters ---------- care_site_ids : Union[int, List[int]], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : Union[str, List[str]], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" self . predictor = filter_table_by_care_site ( table_to_filter = self . predictor , table_name = \" {} predictor\" . format ( type ( self ) . __name__ . lower ()), care_site_relationship = self . care_site_relationship , care_site_ids = care_site_ids , care_site_short_names = care_site_short_names , ) logger . info ( \"Use probe.reset_predictor() to get back the initial predictor\" ) def load ( self , path = None ) -> None : \"\"\"Loads a Probe from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" path = path or self . _get_path () loaded_probe = load_object ( path ) self . __dict__ = loaded_probe . __dict__ . copy () self . path = path def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"visit_from_BCT\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_probe () if not path : if name : self . name = name path = self . _get_path () self . path = path save_object ( self , path ) def delete ( self , path : str = None ): \"\"\"Delete the saved Probe instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = self . _get_path () delete_object ( self , path ) def _get_path ( self ): base_path = CACHE_DIR / \"edsteva\" / \"probes\" filename = f \" { self . name . lower () } .pickle\" return base_path / filename def _get_name ( self ): return type ( self ) . __name__ validate_input_data validate_input_data ( data : Data ) -> None Raises an error if the input data is not valid PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data Source code in edsteva/probes/base.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def validate_input_data ( self , data : Data ) -> None : \"\"\"Raises an error if the input data is not valid Parameters ---------- data: Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] \"\"\" if not isinstance ( data , Data . __args__ ): raise TypeError ( \"Unsupported type {} for data\" . format ( type ( data ))) check_tables ( data = data , required_tables = [ \"visit_occurrence\" , \"care_site\" , \"fact_relationship\" , ], ) is_valid_probe is_valid_probe () -> None Raises an error if the instantiated Probe is not valid Source code in edsteva/probes/base.py 68 69 70 71 72 73 def is_valid_probe ( self ) -> None : \"\"\"Raises an error if the instantiated Probe is not valid\"\"\" if not hasattr ( self , \"_index\" ): raise Exception ( \"Probe must have _index attribute. Please review the code of your probe\" ) is_computed_probe is_computed_probe () -> None Raises an error if the Probe has not been computed properly Source code in edsteva/probes/base.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def is_computed_probe ( self ) -> None : \"\"\"Raises an error if the Probe has not been computed properly\"\"\" if hasattr ( self , \"predictor\" ): if not isinstance ( self . predictor , pd . DataFrame ): raise TypeError ( \"Predictor must be a Pandas DataFrame and not a {} , please review the process method or your arguments\" . format ( type ( self . predictor ) ) ) if self . predictor . empty : raise Exception ( \"Predictor is empty, please review the process method or your arguments\" ) check_columns ( self . predictor , required_columns = self . _schema , ) if not self . predictor . dtypes [ \"date\" ] == \"datetime64[ns]\" : try : self . predictor [ \"date\" ] = self . predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) except Exception as e : raise TypeError ( \"Predictor column 'date' type is {} and cannot convert to datetime and return the following error: {} . Please review the process method or your arguments\" . format ( self . predictor . dtypes [ \"date\" ], e ) ) else : raise Exception ( \"Predictor has not been computed, please use the compute method as follow: Predictor.compute()\" ) impute_missing_date impute_missing_date ( only_impute_per_care_site : bool = False , ) -> pd . DataFrame Impute missing date with 0 on the predictor of a probe. PARAMETER DESCRIPTION only_impute_per_care_site If True it will only impute missing date between the first and the last observation of each care site. If False it will impute missing data on the entire study period whatever the care site TYPE: bool , optional DEFAULT: False Source code in edsteva/probes/base.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def impute_missing_date ( self , only_impute_per_care_site : bool = False , ) -> pd . DataFrame : \"\"\"Impute missing date with 0 on the predictor of a probe. Parameters ---------- only_impute_per_care_site : bool, optional If True it will only impute missing date between the first and the last observation of each care site. If False it will impute missing data on the entire study period whatever the care site \"\"\" # Check if probe has been computed. self . is_computed_probe () # Set start_date to the beginning of the month. date_index = pd . date_range ( start = self . start_date , end = self . end_date , freq = \"MS\" , closed = \"left\" , ) date_index = pd . DataFrame ({ \"date\" : date_index }) # Precompute the mapping: # {'H\u00f4pital-1': {'min': Timestamp('2010-06-01'), 'max': Timestamp('2019-11-01')} if only_impute_per_care_site : site_to_min_max_ds = ( self . predictor . groupby ([ \"care_site_short_name\" ])[ \"date\" ] . agg ([ min , max ]) . to_dict ( \"index\" ) ) partition_cols = self . _index + [ \"care_site_short_name\" ] groups = [] for partition , group in self . predictor . groupby ( partition_cols ): group = date_index . merge ( group , on = \"date\" , how = \"left\" ) # Filter on each care site timeframe. if only_impute_per_care_site : care_site_short_name = partition [ - 1 ] ds_min = site_to_min_max_ds [ care_site_short_name ][ \"min\" ] ds_max = site_to_min_max_ds [ care_site_short_name ][ \"max\" ] group = group . loc [( group [ \"date\" ] >= ds_min ) & ( group [ \"date\" ] <= ds_max )] # Fill specific partition values. for key , val in zip ( partition_cols , partition ): group [ key ] = val # Fill remaining NaN from counts values with 0. group . fillna ( 0 , inplace = True ) groups . append ( group ) self . predictor = pd . concat ( groups ) compute_process abstractmethod compute_process ( data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , ** kwargs ) -> pd . DataFrame Process the data in order to obtain a predictor table Source code in edsteva/probes/base.py 162 163 164 165 166 167 168 169 170 171 172 173 174 @abstractmethod def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Process the data in order to obtain a predictor table\"\"\" compute compute ( data : Data , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , impute_missing_dates : bool = True , only_impute_per_care_site : bool = False , ** kwargs ) -> None Calls compute_process() Here are the following computation steps: check if input data is valid with validate_input_data() method query care site relationship table with get_care_site_relationship() compute predictor with compute_process() method check if predictor is valid with is_computed_probe() method PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_levels EXAMPLE : [\"Hospital\", \"Pole\", \"UF\"] TYPE: List [ str ], optional DEFAULT: None stay_types EXAMPLE : {\"All\": \".*\"} or {\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"} or \"hospitalis\u00e9s TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: None care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None ATTRIBUTE DESCRIPTION Add to the Probe th following attributes predictor is the target DataFrame _cache_predictor is a copy of the target DataFrame (used to reset_predictor() ) care_site_relationship is a DataFrame with the hierarchy of the care site structure Examples: from edsteva.probes import VisitProbe visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg_and_consult\" : \"urgences|consultation\" }, care_site_levels = [ \"Hospital\" , \"Pole\" , \"UF\" ], ) visit . predictor . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 Urg_and_consult 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 Urg_and_consult 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 Urg_and_consult 2022-02-01 9746.0 0.769 Source code in edsteva/probes/base.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def compute ( self , data : Data , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , impute_missing_dates : bool = True , only_impute_per_care_site : bool = False , ** kwargs , ) -> None : \"\"\"Calls [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] Here are the following computation steps: - check if input data is valid with [``validate_input_data()``][edsteva.probes.base.BaseProbe.validate_input_data] method - query care site relationship table with [``get_care_site_relationship()``][edsteva.probes.utils.get_care_site_relationship] - compute predictor with [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] method - check if predictor is valid with [``is_computed_probe()``][edsteva.probes.base.BaseProbe.is_computed_probe] method Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` Attributes ---------- Add to the Probe th following attributes: - predictor is the target DataFrame - _cache_predictor is a copy of the target DataFrame (used to [``reset_predictor()``][edsteva.probes.base.BaseProbe.reset_predictor]) - care_site_relationship is a DataFrame with the hierarchy of the care site structure Examples ------- ```python from edsteva.probes import VisitProbe visit = VisitProbe() visit.compute( data, stay_types={\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}, care_site_levels=[\"Hospital\", \"Pole\", \"UF\"], ) visit.predictor.head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | | :----------------------- | :----------- | :------------------- | :-------------- | :--------- | :------ | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | Urg_and_consult | 2019-05-01 | 233.0 | 0.841 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | | P\u00f4le/DMU | 8312027648 | Care site 2 | Urg_and_consult | 2011-03-01 | 204.0 | 0.497 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.274 | | H\u00f4pital | 8312022130 | Care site 3 | Urg_and_consult | 2022-02-01 | 9746.0 | 0.769 | \"\"\" self . validate_input_data ( data = data ) care_site_relationship = get_care_site_relationship ( data = data ) self . predictor = self . compute_process ( data = data , care_site_relationship = care_site_relationship , start_date = start_date , end_date = end_date , care_site_levels = care_site_levels , stay_types = stay_types , care_site_ids = care_site_ids , ** kwargs , ) self . is_computed_probe () self . start_date = ( pd . to_datetime ( start_date ) if start_date else self . predictor [ \"date\" ] . min () ) self . end_date = ( pd . to_datetime ( end_date ) if end_date else self . predictor [ \"date\" ] . max () ) if impute_missing_dates : self . impute_missing_date ( only_impute_per_care_site = only_impute_per_care_site , ) self . cache_predictor () self . care_site_relationship = care_site_relationship reset_predictor reset_predictor () -> None Reset the predictor to its initial state Source code in edsteva/probes/base.py 277 278 279 280 281 def reset_predictor ( self , ) -> None : \"\"\"Reset the predictor to its initial state\"\"\" self . predictor = self . _cache_predictor . copy () cache_predictor cache_predictor () -> None Cache the predictor Source code in edsteva/probes/base.py 283 284 285 286 287 288 289 290 def cache_predictor ( self , ) -> None : \"\"\"Cache the predictor\"\"\" self . _cache_predictor = self . predictor . copy () logger . info ( \"Cache the predictor, you can reset the predictor to this state with the method reset_predictor\" ) filter_care_site filter_care_site ( care_site_ids : Union [ int , List [ int ]] = None , care_site_short_names : Union [ str , List [ str ]] = None , ) -> None Filters all the care sites related to the selected care sites. PARAMETER DESCRIPTION care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: Union [ int , List [ int ]], optional DEFAULT: None care_site_short_names EXAMPLE : [\"HOSPITAL 1\", \"HOSPITAL 2\"] TYPE: Union [ str , List [ str ]], optional DEFAULT: None Source code in edsteva/probes/base.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def filter_care_site ( self , care_site_ids : Union [ int , List [ int ]] = None , care_site_short_names : Union [ str , List [ str ]] = None , ) -> None : \"\"\"Filters all the care sites related to the selected care sites. Parameters ---------- care_site_ids : Union[int, List[int]], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : Union[str, List[str]], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" self . predictor = filter_table_by_care_site ( table_to_filter = self . predictor , table_name = \" {} predictor\" . format ( type ( self ) . __name__ . lower ()), care_site_relationship = self . care_site_relationship , care_site_ids = care_site_ids , care_site_short_names = care_site_short_names , ) logger . info ( \"Use probe.reset_predictor() to get back the initial predictor\" ) load load ( path = None ) -> None Loads a Probe from local PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . load ( path = probe_path ) Source code in edsteva/probes/base.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def load ( self , path = None ) -> None : \"\"\"Loads a Probe from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" path = path or self . _get_path () loaded_probe = load_object ( path ) self . __dict__ = loaded_probe . __dict__ . copy () self . path = path save save ( path : str = None , name : str = None ) -> bool Saves computed Model instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None name EXAMPLE : \"visit_from_BCT\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . compute ( data ) visit . save ( path = probe_path ) Source code in edsteva/probes/base.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"visit_from_BCT\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_probe () if not path : if name : self . name = name path = self . _get_path () self . path = path save_object ( self , path ) delete delete ( path : str = None ) Delete the saved Probe instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/probes/base.py 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def delete ( self , path : str = None ): \"\"\"Delete the saved Probe instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = self . _get_path () delete_object ( self , path )","title":"base"},{"location":"reference/probes/base/#edstevaprobesbase","text":"","title":"edsteva.probes.base"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe","text":"Base class for Probes ATTRIBUTE DESCRIPTION _schema The columns a predictor must have VALUE : [\"care_site_id\", \"care_site_level\", \"stay_type\", \"date\", \"c\"] TYPE: List [ str ] predictor Available with the compute() method TYPE: pd . DataFrame _cache_predictor Available with the compute() method It is a copy of the predictor DataFrame used to reset_predictor() TYPE: pd . DataFrame care_site_relationship Available with the compute() method It describes the care site structure (cf. get_care_site_relationship() ) TYPE: pd . DataFrame Source code in edsteva/probes/base.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 class BaseProbe ( metaclass = ABCMeta ): \"\"\"Base class for Probes Attributes ---------- _schema: List[str] The columns a predictor must have **VALUE**: ``[\"care_site_id\", \"care_site_level\", \"stay_type\", \"date\", \"c\"]`` predictor: pd.DataFrame Available with the [``compute()``][edsteva.probes.base.BaseProbe.compute] method _cache_predictor: pd.DataFrame Available with the [``compute()``][edsteva.probes.base.BaseProbe.compute] method It is a copy of the predictor DataFrame used to [``reset_predictor()``][edsteva.probes.base.BaseProbe.reset_predictor] care_site_relationship: pd.DataFrame Available with the [``compute()``][edsteva.probes.base.BaseProbe.compute] method It describes the care site structure (cf. [``get_care_site_relationship()``][edsteva.probes.utils.get_care_site_relationship]) \"\"\" def __init__ ( self ): self . is_valid_probe () self . name = self . _get_name () _schema = [ \"care_site_id\" , \"care_site_level\" , \"stay_type\" , \"date\" , \"c\" ] def validate_input_data ( self , data : Data ) -> None : \"\"\"Raises an error if the input data is not valid Parameters ---------- data: Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] \"\"\" if not isinstance ( data , Data . __args__ ): raise TypeError ( \"Unsupported type {} for data\" . format ( type ( data ))) check_tables ( data = data , required_tables = [ \"visit_occurrence\" , \"care_site\" , \"fact_relationship\" , ], ) def is_valid_probe ( self ) -> None : \"\"\"Raises an error if the instantiated Probe is not valid\"\"\" if not hasattr ( self , \"_index\" ): raise Exception ( \"Probe must have _index attribute. Please review the code of your probe\" ) def is_computed_probe ( self ) -> None : \"\"\"Raises an error if the Probe has not been computed properly\"\"\" if hasattr ( self , \"predictor\" ): if not isinstance ( self . predictor , pd . DataFrame ): raise TypeError ( \"Predictor must be a Pandas DataFrame and not a {} , please review the process method or your arguments\" . format ( type ( self . predictor ) ) ) if self . predictor . empty : raise Exception ( \"Predictor is empty, please review the process method or your arguments\" ) check_columns ( self . predictor , required_columns = self . _schema , ) if not self . predictor . dtypes [ \"date\" ] == \"datetime64[ns]\" : try : self . predictor [ \"date\" ] = self . predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) except Exception as e : raise TypeError ( \"Predictor column 'date' type is {} and cannot convert to datetime and return the following error: {} . Please review the process method or your arguments\" . format ( self . predictor . dtypes [ \"date\" ], e ) ) else : raise Exception ( \"Predictor has not been computed, please use the compute method as follow: Predictor.compute()\" ) def impute_missing_date ( self , only_impute_per_care_site : bool = False , ) -> pd . DataFrame : \"\"\"Impute missing date with 0 on the predictor of a probe. Parameters ---------- only_impute_per_care_site : bool, optional If True it will only impute missing date between the first and the last observation of each care site. If False it will impute missing data on the entire study period whatever the care site \"\"\" # Check if probe has been computed. self . is_computed_probe () # Set start_date to the beginning of the month. date_index = pd . date_range ( start = self . start_date , end = self . end_date , freq = \"MS\" , closed = \"left\" , ) date_index = pd . DataFrame ({ \"date\" : date_index }) # Precompute the mapping: # {'H\u00f4pital-1': {'min': Timestamp('2010-06-01'), 'max': Timestamp('2019-11-01')} if only_impute_per_care_site : site_to_min_max_ds = ( self . predictor . groupby ([ \"care_site_short_name\" ])[ \"date\" ] . agg ([ min , max ]) . to_dict ( \"index\" ) ) partition_cols = self . _index + [ \"care_site_short_name\" ] groups = [] for partition , group in self . predictor . groupby ( partition_cols ): group = date_index . merge ( group , on = \"date\" , how = \"left\" ) # Filter on each care site timeframe. if only_impute_per_care_site : care_site_short_name = partition [ - 1 ] ds_min = site_to_min_max_ds [ care_site_short_name ][ \"min\" ] ds_max = site_to_min_max_ds [ care_site_short_name ][ \"max\" ] group = group . loc [( group [ \"date\" ] >= ds_min ) & ( group [ \"date\" ] <= ds_max )] # Fill specific partition values. for key , val in zip ( partition_cols , partition ): group [ key ] = val # Fill remaining NaN from counts values with 0. group . fillna ( 0 , inplace = True ) groups . append ( group ) self . predictor = pd . concat ( groups ) @abstractmethod def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Process the data in order to obtain a predictor table\"\"\" def compute ( self , data : Data , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , impute_missing_dates : bool = True , only_impute_per_care_site : bool = False , ** kwargs , ) -> None : \"\"\"Calls [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] Here are the following computation steps: - check if input data is valid with [``validate_input_data()``][edsteva.probes.base.BaseProbe.validate_input_data] method - query care site relationship table with [``get_care_site_relationship()``][edsteva.probes.utils.get_care_site_relationship] - compute predictor with [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] method - check if predictor is valid with [``is_computed_probe()``][edsteva.probes.base.BaseProbe.is_computed_probe] method Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` Attributes ---------- Add to the Probe th following attributes: - predictor is the target DataFrame - _cache_predictor is a copy of the target DataFrame (used to [``reset_predictor()``][edsteva.probes.base.BaseProbe.reset_predictor]) - care_site_relationship is a DataFrame with the hierarchy of the care site structure Examples ------- ```python from edsteva.probes import VisitProbe visit = VisitProbe() visit.compute( data, stay_types={\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}, care_site_levels=[\"Hospital\", \"Pole\", \"UF\"], ) visit.predictor.head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | | :----------------------- | :----------- | :------------------- | :-------------- | :--------- | :------ | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | Urg_and_consult | 2019-05-01 | 233.0 | 0.841 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | | P\u00f4le/DMU | 8312027648 | Care site 2 | Urg_and_consult | 2011-03-01 | 204.0 | 0.497 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.274 | | H\u00f4pital | 8312022130 | Care site 3 | Urg_and_consult | 2022-02-01 | 9746.0 | 0.769 | \"\"\" self . validate_input_data ( data = data ) care_site_relationship = get_care_site_relationship ( data = data ) self . predictor = self . compute_process ( data = data , care_site_relationship = care_site_relationship , start_date = start_date , end_date = end_date , care_site_levels = care_site_levels , stay_types = stay_types , care_site_ids = care_site_ids , ** kwargs , ) self . is_computed_probe () self . start_date = ( pd . to_datetime ( start_date ) if start_date else self . predictor [ \"date\" ] . min () ) self . end_date = ( pd . to_datetime ( end_date ) if end_date else self . predictor [ \"date\" ] . max () ) if impute_missing_dates : self . impute_missing_date ( only_impute_per_care_site = only_impute_per_care_site , ) self . cache_predictor () self . care_site_relationship = care_site_relationship def reset_predictor ( self , ) -> None : \"\"\"Reset the predictor to its initial state\"\"\" self . predictor = self . _cache_predictor . copy () def cache_predictor ( self , ) -> None : \"\"\"Cache the predictor\"\"\" self . _cache_predictor = self . predictor . copy () logger . info ( \"Cache the predictor, you can reset the predictor to this state with the method reset_predictor\" ) def filter_care_site ( self , care_site_ids : Union [ int , List [ int ]] = None , care_site_short_names : Union [ str , List [ str ]] = None , ) -> None : \"\"\"Filters all the care sites related to the selected care sites. Parameters ---------- care_site_ids : Union[int, List[int]], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : Union[str, List[str]], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" self . predictor = filter_table_by_care_site ( table_to_filter = self . predictor , table_name = \" {} predictor\" . format ( type ( self ) . __name__ . lower ()), care_site_relationship = self . care_site_relationship , care_site_ids = care_site_ids , care_site_short_names = care_site_short_names , ) logger . info ( \"Use probe.reset_predictor() to get back the initial predictor\" ) def load ( self , path = None ) -> None : \"\"\"Loads a Probe from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" path = path or self . _get_path () loaded_probe = load_object ( path ) self . __dict__ = loaded_probe . __dict__ . copy () self . path = path def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"visit_from_BCT\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_probe () if not path : if name : self . name = name path = self . _get_path () self . path = path save_object ( self , path ) def delete ( self , path : str = None ): \"\"\"Delete the saved Probe instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = self . _get_path () delete_object ( self , path ) def _get_path ( self ): base_path = CACHE_DIR / \"edsteva\" / \"probes\" filename = f \" { self . name . lower () } .pickle\" return base_path / filename def _get_name ( self ): return type ( self ) . __name__","title":"BaseProbe"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.validate_input_data","text":"validate_input_data ( data : Data ) -> None Raises an error if the input data is not valid PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data Source code in edsteva/probes/base.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def validate_input_data ( self , data : Data ) -> None : \"\"\"Raises an error if the input data is not valid Parameters ---------- data: Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] \"\"\" if not isinstance ( data , Data . __args__ ): raise TypeError ( \"Unsupported type {} for data\" . format ( type ( data ))) check_tables ( data = data , required_tables = [ \"visit_occurrence\" , \"care_site\" , \"fact_relationship\" , ], )","title":"validate_input_data()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.is_valid_probe","text":"is_valid_probe () -> None Raises an error if the instantiated Probe is not valid Source code in edsteva/probes/base.py 68 69 70 71 72 73 def is_valid_probe ( self ) -> None : \"\"\"Raises an error if the instantiated Probe is not valid\"\"\" if not hasattr ( self , \"_index\" ): raise Exception ( \"Probe must have _index attribute. Please review the code of your probe\" )","title":"is_valid_probe()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.is_computed_probe","text":"is_computed_probe () -> None Raises an error if the Probe has not been computed properly Source code in edsteva/probes/base.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def is_computed_probe ( self ) -> None : \"\"\"Raises an error if the Probe has not been computed properly\"\"\" if hasattr ( self , \"predictor\" ): if not isinstance ( self . predictor , pd . DataFrame ): raise TypeError ( \"Predictor must be a Pandas DataFrame and not a {} , please review the process method or your arguments\" . format ( type ( self . predictor ) ) ) if self . predictor . empty : raise Exception ( \"Predictor is empty, please review the process method or your arguments\" ) check_columns ( self . predictor , required_columns = self . _schema , ) if not self . predictor . dtypes [ \"date\" ] == \"datetime64[ns]\" : try : self . predictor [ \"date\" ] = self . predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) except Exception as e : raise TypeError ( \"Predictor column 'date' type is {} and cannot convert to datetime and return the following error: {} . Please review the process method or your arguments\" . format ( self . predictor . dtypes [ \"date\" ], e ) ) else : raise Exception ( \"Predictor has not been computed, please use the compute method as follow: Predictor.compute()\" )","title":"is_computed_probe()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.impute_missing_date","text":"impute_missing_date ( only_impute_per_care_site : bool = False , ) -> pd . DataFrame Impute missing date with 0 on the predictor of a probe. PARAMETER DESCRIPTION only_impute_per_care_site If True it will only impute missing date between the first and the last observation of each care site. If False it will impute missing data on the entire study period whatever the care site TYPE: bool , optional DEFAULT: False Source code in edsteva/probes/base.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def impute_missing_date ( self , only_impute_per_care_site : bool = False , ) -> pd . DataFrame : \"\"\"Impute missing date with 0 on the predictor of a probe. Parameters ---------- only_impute_per_care_site : bool, optional If True it will only impute missing date between the first and the last observation of each care site. If False it will impute missing data on the entire study period whatever the care site \"\"\" # Check if probe has been computed. self . is_computed_probe () # Set start_date to the beginning of the month. date_index = pd . date_range ( start = self . start_date , end = self . end_date , freq = \"MS\" , closed = \"left\" , ) date_index = pd . DataFrame ({ \"date\" : date_index }) # Precompute the mapping: # {'H\u00f4pital-1': {'min': Timestamp('2010-06-01'), 'max': Timestamp('2019-11-01')} if only_impute_per_care_site : site_to_min_max_ds = ( self . predictor . groupby ([ \"care_site_short_name\" ])[ \"date\" ] . agg ([ min , max ]) . to_dict ( \"index\" ) ) partition_cols = self . _index + [ \"care_site_short_name\" ] groups = [] for partition , group in self . predictor . groupby ( partition_cols ): group = date_index . merge ( group , on = \"date\" , how = \"left\" ) # Filter on each care site timeframe. if only_impute_per_care_site : care_site_short_name = partition [ - 1 ] ds_min = site_to_min_max_ds [ care_site_short_name ][ \"min\" ] ds_max = site_to_min_max_ds [ care_site_short_name ][ \"max\" ] group = group . loc [( group [ \"date\" ] >= ds_min ) & ( group [ \"date\" ] <= ds_max )] # Fill specific partition values. for key , val in zip ( partition_cols , partition ): group [ key ] = val # Fill remaining NaN from counts values with 0. group . fillna ( 0 , inplace = True ) groups . append ( group ) self . predictor = pd . concat ( groups )","title":"impute_missing_date()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.compute_process","text":"compute_process ( data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , ** kwargs ) -> pd . DataFrame Process the data in order to obtain a predictor table Source code in edsteva/probes/base.py 162 163 164 165 166 167 168 169 170 171 172 173 174 @abstractmethod def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Process the data in order to obtain a predictor table\"\"\"","title":"compute_process()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.compute","text":"compute ( data : Data , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , impute_missing_dates : bool = True , only_impute_per_care_site : bool = False , ** kwargs ) -> None Calls compute_process() Here are the following computation steps: check if input data is valid with validate_input_data() method query care site relationship table with get_care_site_relationship() compute predictor with compute_process() method check if predictor is valid with is_computed_probe() method PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_levels EXAMPLE : [\"Hospital\", \"Pole\", \"UF\"] TYPE: List [ str ], optional DEFAULT: None stay_types EXAMPLE : {\"All\": \".*\"} or {\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"} or \"hospitalis\u00e9s TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: None care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None ATTRIBUTE DESCRIPTION Add to the Probe th following attributes predictor is the target DataFrame _cache_predictor is a copy of the target DataFrame (used to reset_predictor() ) care_site_relationship is a DataFrame with the hierarchy of the care site structure Examples: from edsteva.probes import VisitProbe visit = VisitProbe () visit . compute ( data , stay_types = { \"All\" : \".*\" , \"Urg_and_consult\" : \"urgences|consultation\" }, care_site_levels = [ \"Hospital\" , \"Pole\" , \"UF\" ], ) visit . predictor . head () care_site_level care_site_id care_site_short_name stay_type date n_visit c Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 Urg_and_consult 2019-05-01 233.0 0.841 Unit\u00e9 Fonctionnelle (UF) 8312056386 Care site 1 'All' 2021-04-01 393.0 0.640 P\u00f4le/DMU 8312027648 Care site 2 Urg_and_consult 2011-03-01 204.0 0.497 P\u00f4le/DMU 8312027648 Care site 2 'All' 2018-08-01 22.0 0.274 H\u00f4pital 8312022130 Care site 3 Urg_and_consult 2022-02-01 9746.0 0.769 Source code in edsteva/probes/base.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def compute ( self , data : Data , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , impute_missing_dates : bool = True , only_impute_per_care_site : bool = False , ** kwargs , ) -> None : \"\"\"Calls [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] Here are the following computation steps: - check if input data is valid with [``validate_input_data()``][edsteva.probes.base.BaseProbe.validate_input_data] method - query care site relationship table with [``get_care_site_relationship()``][edsteva.probes.utils.get_care_site_relationship] - compute predictor with [``compute_process()``][edsteva.probes.base.BaseProbe.compute_process] method - check if predictor is valid with [``is_computed_probe()``][edsteva.probes.base.BaseProbe.is_computed_probe] method Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` Attributes ---------- Add to the Probe th following attributes: - predictor is the target DataFrame - _cache_predictor is a copy of the target DataFrame (used to [``reset_predictor()``][edsteva.probes.base.BaseProbe.reset_predictor]) - care_site_relationship is a DataFrame with the hierarchy of the care site structure Examples ------- ```python from edsteva.probes import VisitProbe visit = VisitProbe() visit.compute( data, stay_types={\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}, care_site_levels=[\"Hospital\", \"Pole\", \"UF\"], ) visit.predictor.head() ``` | care_site_level | care_site_id | care_site_short_name | stay_type | date | n_visit | c | | :----------------------- | :----------- | :------------------- | :-------------- | :--------- | :------ | :---- | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | Urg_and_consult | 2019-05-01 | 233.0 | 0.841 | | Unit\u00e9 Fonctionnelle (UF) | 8312056386 | Care site 1 | 'All' | 2021-04-01 | 393.0 | 0.640 | | P\u00f4le/DMU | 8312027648 | Care site 2 | Urg_and_consult | 2011-03-01 | 204.0 | 0.497 | | P\u00f4le/DMU | 8312027648 | Care site 2 | 'All' | 2018-08-01 | 22.0 | 0.274 | | H\u00f4pital | 8312022130 | Care site 3 | Urg_and_consult | 2022-02-01 | 9746.0 | 0.769 | \"\"\" self . validate_input_data ( data = data ) care_site_relationship = get_care_site_relationship ( data = data ) self . predictor = self . compute_process ( data = data , care_site_relationship = care_site_relationship , start_date = start_date , end_date = end_date , care_site_levels = care_site_levels , stay_types = stay_types , care_site_ids = care_site_ids , ** kwargs , ) self . is_computed_probe () self . start_date = ( pd . to_datetime ( start_date ) if start_date else self . predictor [ \"date\" ] . min () ) self . end_date = ( pd . to_datetime ( end_date ) if end_date else self . predictor [ \"date\" ] . max () ) if impute_missing_dates : self . impute_missing_date ( only_impute_per_care_site = only_impute_per_care_site , ) self . cache_predictor () self . care_site_relationship = care_site_relationship","title":"compute()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.reset_predictor","text":"reset_predictor () -> None Reset the predictor to its initial state Source code in edsteva/probes/base.py 277 278 279 280 281 def reset_predictor ( self , ) -> None : \"\"\"Reset the predictor to its initial state\"\"\" self . predictor = self . _cache_predictor . copy ()","title":"reset_predictor()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.cache_predictor","text":"cache_predictor () -> None Cache the predictor Source code in edsteva/probes/base.py 283 284 285 286 287 288 289 290 def cache_predictor ( self , ) -> None : \"\"\"Cache the predictor\"\"\" self . _cache_predictor = self . predictor . copy () logger . info ( \"Cache the predictor, you can reset the predictor to this state with the method reset_predictor\" )","title":"cache_predictor()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.filter_care_site","text":"filter_care_site ( care_site_ids : Union [ int , List [ int ]] = None , care_site_short_names : Union [ str , List [ str ]] = None , ) -> None Filters all the care sites related to the selected care sites. PARAMETER DESCRIPTION care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: Union [ int , List [ int ]], optional DEFAULT: None care_site_short_names EXAMPLE : [\"HOSPITAL 1\", \"HOSPITAL 2\"] TYPE: Union [ str , List [ str ]], optional DEFAULT: None Source code in edsteva/probes/base.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def filter_care_site ( self , care_site_ids : Union [ int , List [ int ]] = None , care_site_short_names : Union [ str , List [ str ]] = None , ) -> None : \"\"\"Filters all the care sites related to the selected care sites. Parameters ---------- care_site_ids : Union[int, List[int]], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : Union[str, List[str]], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" self . predictor = filter_table_by_care_site ( table_to_filter = self . predictor , table_name = \" {} predictor\" . format ( type ( self ) . __name__ . lower ()), care_site_relationship = self . care_site_relationship , care_site_ids = care_site_ids , care_site_short_names = care_site_short_names , ) logger . info ( \"Use probe.reset_predictor() to get back the initial predictor\" )","title":"filter_care_site()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.load","text":"load ( path = None ) -> None Loads a Probe from local PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . load ( path = probe_path ) Source code in edsteva/probes/base.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def load ( self , path = None ) -> None : \"\"\"Loads a Probe from local Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.load(path=probe_path) ``` \"\"\" path = path or self . _get_path () loaded_probe = load_object ( path ) self . __dict__ = loaded_probe . __dict__ . copy () self . path = path","title":"load()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.save","text":"save ( path : str = None , name : str = None ) -> bool Saves computed Model instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None name EXAMPLE : \"visit_from_BCT\" TYPE: str , optional DEFAULT: None Examples: from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe () visit . compute ( data ) visit . save ( path = probe_path ) Source code in edsteva/probes/base.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def save ( self , path : str = None , name : str = None ) -> bool : \"\"\"Saves computed Model instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` name : str, optional **EXAMPLE**: `\"visit_from_BCT\"` Examples ------- ```python from edsteva.probes import VisitProbe probe_path = \"my_path/visit.pkl\" visit = VisitProbe() visit.compute(data) visit.save(path=probe_path) ``` \"\"\" self . is_computed_probe () if not path : if name : self . name = name path = self . _get_path () self . path = path save_object ( self , path )","title":"save()"},{"location":"reference/probes/base/#edsteva.probes.base.BaseProbe.delete","text":"delete ( path : str = None ) Delete the saved Probe instance PARAMETER DESCRIPTION path EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/probes/base.py 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def delete ( self , path : str = None ): \"\"\"Delete the saved Probe instance Parameters ---------- path : str, optional **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not path : if hasattr ( self , \"path\" ): path = self . path else : path = self . _get_path () delete_object ( self , path )","title":"delete()"},{"location":"reference/probes/note/","text":"edsteva.probes.note NoteProbe Bases: BaseProbe The NoteProbe computes \\(c(t)\\) the availability of clinical documents linked to patients' visits: \\[ c(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{with\\,doc}\\) the number of visits having at least one document and \\(t\\) is the month. ATTRIBUTE DESCRIPTION _index Variable from which data is grouped VALUE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/probes/note.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 class NoteProbe ( BaseProbe ): r \"\"\" The ``NoteProbe`` computes $c(t)$ the availability of clinical documents linked to patients' visits: $$ c(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} $$ Where $n_{visit}(t)$ is the number of visits, $n_{with\\,doc}$ the number of visits having at least one document and $t$ is the month. Attributes ---------- _index: List[str] Variable from which data is grouped **VALUE**: ``[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]`` \"\"\" _index = [ \"care_site_level\" , \"stay_type\" , \"note_type\" , \"care_site_id\" ] def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , extra_data : Data = None , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , care_site_short_names : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , note_types : Union [ str , Dict [ str , str ]] = { \"All\" : \".*\" , \"Urgence\" : \"urge\" , \"Ordonnance\" : \"ordo\" , \"CRH\" : \"crh\" , }, care_site_ids : List [ int ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. extra_data : Data, optional Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData]. This is not OMOP-standardized data but data needed to associate note with UF and Pole. If not provided, it will only compute the predictor for hospitals. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` note_types : Union[str, Dict[str, str]], optional care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) note = prepare_note ( data , note_types ) hospital_visit = get_hospital_visit ( note , visit_occurrence , care_site ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] note_predictor_by_level = { hospital_name : hospital_visit } # UF selection if not hospital_only ( care_site_levels = care_site_levels ) and extra_data : visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_visit = get_uf_visit ( extra_data , note , visit_occurrence , visit_detail , care_site , care_site_relationship , ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] note_predictor_by_level [ uf_name ] = uf_visit pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship ) pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] note_predictor_by_level [ pole_name ] = pole_visit # Concatenate all predictors note_predictor = concatenate_predictor_by_level ( predictor_by_level = note_predictor_by_level , care_site_levels = care_site_levels , ) note_predictor = note_predictor . drop_duplicates ( [ \"visit_id\" , \"care_site_id\" , \"stay_type\" , \"note_type\" , \"date\" ] ) if is_koalas ( note_predictor ): note_predictor . spark . cache () return compute_completeness ( note_predictor ) compute_process compute_process ( data : Data , care_site_relationship : pd . DataFrame , extra_data : Data = None , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , care_site_short_names : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , note_types : Union [ str , Dict [ str , str ]] = { \"All\" : \".*\" , \"Urgence\" : \"urge\" , \"Ordonnance\" : \"ordo\" , \"CRH\" : \"crh\" , }, care_site_ids : List [ int ] = None , ) Script to be used by compute() PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data care_site_relationship DataFrame computed in the compute() that gives the hierarchy of the care site structure. TYPE: pd . DataFrame extra_data Instantiated HiveData , PostgresData or LocalData . This is not OMOP-standardized data but data needed to associate note with UF and Pole. If not provided, it will only compute the predictor for hospitals. TYPE: Data , optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_levels EXAMPLE : [\"Hospital\", \"Pole\", \"UF\"] TYPE: List [ str ], optional DEFAULT: None care_site_short_names EXAMPLE : [\"HOSPITAL 1\", \"HOSPITAL 2\"] TYPE: List [ str ], optional DEFAULT: None stay_types EXAMPLE : {\"All\": \".*\"} or {\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"} or \"hospitalis\u00e9s TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: None note_types TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: {'All': '.*', 'Urgence': 'urge', 'Ordonnance': 'ordo', 'CRH': 'crh'} care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None Source code in edsteva/probes/note.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , extra_data : Data = None , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , care_site_short_names : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , note_types : Union [ str , Dict [ str , str ]] = { \"All\" : \".*\" , \"Urgence\" : \"urge\" , \"Ordonnance\" : \"ordo\" , \"CRH\" : \"crh\" , }, care_site_ids : List [ int ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. extra_data : Data, optional Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData]. This is not OMOP-standardized data but data needed to associate note with UF and Pole. If not provided, it will only compute the predictor for hospitals. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` note_types : Union[str, Dict[str, str]], optional care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) note = prepare_note ( data , note_types ) hospital_visit = get_hospital_visit ( note , visit_occurrence , care_site ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] note_predictor_by_level = { hospital_name : hospital_visit } # UF selection if not hospital_only ( care_site_levels = care_site_levels ) and extra_data : visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_visit = get_uf_visit ( extra_data , note , visit_occurrence , visit_detail , care_site , care_site_relationship , ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] note_predictor_by_level [ uf_name ] = uf_visit pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship ) pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] note_predictor_by_level [ pole_name ] = pole_visit # Concatenate all predictors note_predictor = concatenate_predictor_by_level ( predictor_by_level = note_predictor_by_level , care_site_levels = care_site_levels , ) note_predictor = note_predictor . drop_duplicates ( [ \"visit_id\" , \"care_site_id\" , \"stay_type\" , \"note_type\" , \"date\" ] ) if is_koalas ( note_predictor ): note_predictor . spark . cache () return compute_completeness ( note_predictor )","title":"note"},{"location":"reference/probes/note/#edstevaprobesnote","text":"","title":"edsteva.probes.note"},{"location":"reference/probes/note/#edsteva.probes.note.NoteProbe","text":"Bases: BaseProbe The NoteProbe computes \\(c(t)\\) the availability of clinical documents linked to patients' visits: \\[ c(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{with\\,doc}\\) the number of visits having at least one document and \\(t\\) is the month. ATTRIBUTE DESCRIPTION _index Variable from which data is grouped VALUE : [\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/probes/note.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 class NoteProbe ( BaseProbe ): r \"\"\" The ``NoteProbe`` computes $c(t)$ the availability of clinical documents linked to patients' visits: $$ c(t) = \\frac{n_{with\\,doc}(t)}{n_{visit}(t)} $$ Where $n_{visit}(t)$ is the number of visits, $n_{with\\,doc}$ the number of visits having at least one document and $t$ is the month. Attributes ---------- _index: List[str] Variable from which data is grouped **VALUE**: ``[\"care_site_level\", \"stay_type\", \"note_type\", \"care_site_id\"]`` \"\"\" _index = [ \"care_site_level\" , \"stay_type\" , \"note_type\" , \"care_site_id\" ] def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , extra_data : Data = None , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , care_site_short_names : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , note_types : Union [ str , Dict [ str , str ]] = { \"All\" : \".*\" , \"Urgence\" : \"urge\" , \"Ordonnance\" : \"ordo\" , \"CRH\" : \"crh\" , }, care_site_ids : List [ int ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. extra_data : Data, optional Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData]. This is not OMOP-standardized data but data needed to associate note with UF and Pole. If not provided, it will only compute the predictor for hospitals. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` note_types : Union[str, Dict[str, str]], optional care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) note = prepare_note ( data , note_types ) hospital_visit = get_hospital_visit ( note , visit_occurrence , care_site ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] note_predictor_by_level = { hospital_name : hospital_visit } # UF selection if not hospital_only ( care_site_levels = care_site_levels ) and extra_data : visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_visit = get_uf_visit ( extra_data , note , visit_occurrence , visit_detail , care_site , care_site_relationship , ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] note_predictor_by_level [ uf_name ] = uf_visit pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship ) pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] note_predictor_by_level [ pole_name ] = pole_visit # Concatenate all predictors note_predictor = concatenate_predictor_by_level ( predictor_by_level = note_predictor_by_level , care_site_levels = care_site_levels , ) note_predictor = note_predictor . drop_duplicates ( [ \"visit_id\" , \"care_site_id\" , \"stay_type\" , \"note_type\" , \"date\" ] ) if is_koalas ( note_predictor ): note_predictor . spark . cache () return compute_completeness ( note_predictor )","title":"NoteProbe"},{"location":"reference/probes/note/#edsteva.probes.note.NoteProbe.compute_process","text":"compute_process ( data : Data , care_site_relationship : pd . DataFrame , extra_data : Data = None , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , care_site_short_names : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , note_types : Union [ str , Dict [ str , str ]] = { \"All\" : \".*\" , \"Urgence\" : \"urge\" , \"Ordonnance\" : \"ordo\" , \"CRH\" : \"crh\" , }, care_site_ids : List [ int ] = None , ) Script to be used by compute() PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data care_site_relationship DataFrame computed in the compute() that gives the hierarchy of the care site structure. TYPE: pd . DataFrame extra_data Instantiated HiveData , PostgresData or LocalData . This is not OMOP-standardized data but data needed to associate note with UF and Pole. If not provided, it will only compute the predictor for hospitals. TYPE: Data , optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_levels EXAMPLE : [\"Hospital\", \"Pole\", \"UF\"] TYPE: List [ str ], optional DEFAULT: None care_site_short_names EXAMPLE : [\"HOSPITAL 1\", \"HOSPITAL 2\"] TYPE: List [ str ], optional DEFAULT: None stay_types EXAMPLE : {\"All\": \".*\"} or {\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"} or \"hospitalis\u00e9s TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: None note_types TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: {'All': '.*', 'Urgence': 'urge', 'Ordonnance': 'ordo', 'CRH': 'crh'} care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None Source code in edsteva/probes/note.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , extra_data : Data = None , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , care_site_short_names : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , note_types : Union [ str , Dict [ str , str ]] = { \"All\" : \".*\" , \"Urgence\" : \"urge\" , \"Ordonnance\" : \"ordo\" , \"CRH\" : \"crh\" , }, care_site_ids : List [ int ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. extra_data : Data, optional Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData]. This is not OMOP-standardized data but data needed to associate note with UF and Pole. If not provided, it will only compute the predictor for hospitals. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` note_types : Union[str, Dict[str, str]], optional care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) note = prepare_note ( data , note_types ) hospital_visit = get_hospital_visit ( note , visit_occurrence , care_site ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] note_predictor_by_level = { hospital_name : hospital_visit } # UF selection if not hospital_only ( care_site_levels = care_site_levels ) and extra_data : visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_visit = get_uf_visit ( extra_data , note , visit_occurrence , visit_detail , care_site , care_site_relationship , ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] note_predictor_by_level [ uf_name ] = uf_visit pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship ) pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] note_predictor_by_level [ pole_name ] = pole_visit # Concatenate all predictors note_predictor = concatenate_predictor_by_level ( predictor_by_level = note_predictor_by_level , care_site_levels = care_site_levels , ) note_predictor = note_predictor . drop_duplicates ( [ \"visit_id\" , \"care_site_id\" , \"stay_type\" , \"note_type\" , \"date\" ] ) if is_koalas ( note_predictor ): note_predictor . spark . cache () return compute_completeness ( note_predictor )","title":"compute_process()"},{"location":"reference/probes/utils/","text":"edsteva.probes.utils get_care_site_relationship get_care_site_relationship ( data : Data ) -> pd . DataFrame Computes hierarchical care site structure PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data Example care_site_id care_site_level care_site_short_name parent_care_site_id parent_care_site_level parent_care_site_short_name 8312056386 Unit\u00e9 Fonctionnelle (UF) UF A 8312027648 P\u00f4le/DMU Pole A 8312022130 P\u00f4le/DMU Pole B 8312033550 H\u00f4pital Hospital A 8312016782 Service/D\u00e9partement Service A 8312033550 H\u00f4pital Hospital A 8312010155 Unit\u00e9 Fonctionnelle (UF) UF B 8312022130 P\u00f4le/DMU Pole B 8312067829 Unit\u00e9 de consultation (UC) UC A 8312051097 Unit\u00e9 de consultation (UC) UC B Source code in edsteva/probes/utils.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 def get_care_site_relationship ( data : Data ) -> pd . DataFrame : \"\"\"Computes hierarchical care site structure Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] Example ------- | care_site_id | care_site_level | care_site_short_name | parent_care_site_id | parent_care_site_level | parent_care_site_short_name | | :----------- | :------------------------- | :------------------- | :------------------ | :------------------------- | :-------------------------- | | 8312056386 | Unit\u00e9 Fonctionnelle (UF) | UF A | 8312027648 | P\u00f4le/DMU | Pole A | | 8312022130 | P\u00f4le/DMU | Pole B | 8312033550 | H\u00f4pital | Hospital A | | 8312016782 | Service/D\u00e9partement | Service A | 8312033550 | H\u00f4pital | Hospital A | | 8312010155 | Unit\u00e9 Fonctionnelle (UF) | UF B | 8312022130 | P\u00f4le/DMU | Pole B | | 8312067829 | Unit\u00e9 de consultation (UC) | UC A | 8312051097 | Unit\u00e9 de consultation (UC) | UC B | \"\"\" fact_relationship = data . fact_relationship [ [ \"fact_id_1\" , \"fact_id_2\" , \"domain_concept_id_1\" , \"relationship_concept_id\" , ] ] fact_relationship = to ( \"pandas\" , fact_relationship ) care_site_relationship = fact_relationship [ ( fact_relationship [ \"domain_concept_id_1\" ] == 57 ) # Care_site domain & ( fact_relationship [ \"relationship_concept_id\" ] == 46233688 ) # Included in ] care_site_relationship = care_site_relationship . drop ( columns = [ \"domain_concept_id_1\" , \"relationship_concept_id\" ] ) care_site_relationship = care_site_relationship . rename ( columns = { \"fact_id_1\" : \"care_site_id\" , \"fact_id_2\" : \"parent_care_site_id\" } ) care_site = data . care_site [ [ \"care_site_id\" , \"care_site_type_source_value\" , \"care_site_short_name\" , ] ] care_site = to ( \"pandas\" , care_site ) care_site = care_site . rename ( columns = { \"care_site_type_source_value\" : \"care_site_level\" , } ) care_site_relationship = care_site . merge ( care_site_relationship , on = \"care_site_id\" , how = \"left\" ) parent_care_site = care_site . rename ( columns = { \"care_site_level\" : \"parent_care_site_level\" , \"care_site_id\" : \"parent_care_site_id\" , \"care_site_short_name\" : \"parent_care_site_short_name\" , } ) logger . debug ( \"Create care site relationship to link UC to UF and UF to Pole\" ) return care_site_relationship . merge ( parent_care_site , on = \"parent_care_site_id\" , how = \"left\" )","title":"utils"},{"location":"reference/probes/utils/#edstevaprobesutils","text":"","title":"edsteva.probes.utils"},{"location":"reference/probes/utils/#edsteva.probes.utils.get_care_site_relationship","text":"get_care_site_relationship ( data : Data ) -> pd . DataFrame Computes hierarchical care site structure PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data","title":"get_care_site_relationship()"},{"location":"reference/probes/utils/#edsteva.probes.utils.get_care_site_relationship--example","text":"care_site_id care_site_level care_site_short_name parent_care_site_id parent_care_site_level parent_care_site_short_name 8312056386 Unit\u00e9 Fonctionnelle (UF) UF A 8312027648 P\u00f4le/DMU Pole A 8312022130 P\u00f4le/DMU Pole B 8312033550 H\u00f4pital Hospital A 8312016782 Service/D\u00e9partement Service A 8312033550 H\u00f4pital Hospital A 8312010155 Unit\u00e9 Fonctionnelle (UF) UF B 8312022130 P\u00f4le/DMU Pole B 8312067829 Unit\u00e9 de consultation (UC) UC A 8312051097 Unit\u00e9 de consultation (UC) UC B Source code in edsteva/probes/utils.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 def get_care_site_relationship ( data : Data ) -> pd . DataFrame : \"\"\"Computes hierarchical care site structure Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] Example ------- | care_site_id | care_site_level | care_site_short_name | parent_care_site_id | parent_care_site_level | parent_care_site_short_name | | :----------- | :------------------------- | :------------------- | :------------------ | :------------------------- | :-------------------------- | | 8312056386 | Unit\u00e9 Fonctionnelle (UF) | UF A | 8312027648 | P\u00f4le/DMU | Pole A | | 8312022130 | P\u00f4le/DMU | Pole B | 8312033550 | H\u00f4pital | Hospital A | | 8312016782 | Service/D\u00e9partement | Service A | 8312033550 | H\u00f4pital | Hospital A | | 8312010155 | Unit\u00e9 Fonctionnelle (UF) | UF B | 8312022130 | P\u00f4le/DMU | Pole B | | 8312067829 | Unit\u00e9 de consultation (UC) | UC A | 8312051097 | Unit\u00e9 de consultation (UC) | UC B | \"\"\" fact_relationship = data . fact_relationship [ [ \"fact_id_1\" , \"fact_id_2\" , \"domain_concept_id_1\" , \"relationship_concept_id\" , ] ] fact_relationship = to ( \"pandas\" , fact_relationship ) care_site_relationship = fact_relationship [ ( fact_relationship [ \"domain_concept_id_1\" ] == 57 ) # Care_site domain & ( fact_relationship [ \"relationship_concept_id\" ] == 46233688 ) # Included in ] care_site_relationship = care_site_relationship . drop ( columns = [ \"domain_concept_id_1\" , \"relationship_concept_id\" ] ) care_site_relationship = care_site_relationship . rename ( columns = { \"fact_id_1\" : \"care_site_id\" , \"fact_id_2\" : \"parent_care_site_id\" } ) care_site = data . care_site [ [ \"care_site_id\" , \"care_site_type_source_value\" , \"care_site_short_name\" , ] ] care_site = to ( \"pandas\" , care_site ) care_site = care_site . rename ( columns = { \"care_site_type_source_value\" : \"care_site_level\" , } ) care_site_relationship = care_site . merge ( care_site_relationship , on = \"care_site_id\" , how = \"left\" ) parent_care_site = care_site . rename ( columns = { \"care_site_level\" : \"parent_care_site_level\" , \"care_site_id\" : \"parent_care_site_id\" , \"care_site_short_name\" : \"parent_care_site_short_name\" , } ) logger . debug ( \"Create care site relationship to link UC to UF and UF to Pole\" ) return care_site_relationship . merge ( parent_care_site , on = \"parent_care_site_id\" , how = \"left\" )","title":"Example"},{"location":"reference/probes/visit/","text":"edsteva.probes.visit VisitProbe Bases: BaseProbe The VisitProbe computes \\(c_(t)\\) the availability of administrative data related to visits for each care site according to time: \\[ c(t) = \\frac{n_{visit}(t)}{n_{99}} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{99}\\) is the \\(99^{th}\\) percentile of visits and \\(t\\) is the month. ATTRIBUTE DESCRIPTION _index Variable from which data is grouped VALUE : [\"care_site_level\", \"stay_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/probes/visit.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 class VisitProbe ( BaseProbe ): r \"\"\" The ``VisitProbe`` computes $c_(t)$ the availability of administrative data related to visits for each care site according to time: $$ c(t) = \\frac{n_{visit}(t)}{n_{99}} $$ Where $n_{visit}(t)$ is the number of visits, $n_{99}$ is the $99^{th}$ percentile of visits and $t$ is the month. Attributes ---------- _index: List[str] Variable from which data is grouped **VALUE**: ``[\"care_site_level\", \"stay_type\", \"care_site_id\"]`` \"\"\" _index = [ \"care_site_level\" , \"stay_type\" , \"care_site_id\" ] def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , care_site_short_names : List [ str ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) hospital_visit = get_hospital_visit ( visit_occurrence , care_site , ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] visit_predictor_by_level = { hospital_name : hospital_visit } if not hospital_only ( care_site_levels = care_site_levels ): visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] uf_visit = get_uf_visit ( visit_occurrence , visit_detail , care_site , care_site_relationship , ) visit_predictor_by_level [ uf_name ] = uf_visit pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship , ) visit_predictor_by_level [ pole_name ] = pole_visit visit_predictor = concatenate_predictor_by_level ( predictor_by_level = visit_predictor_by_level , care_site_levels = care_site_levels , ) return compute_completeness ( visit_predictor ) compute_process compute_process ( data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , care_site_short_names : List [ str ] = None , ) Script to be used by compute() PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data care_site_relationship DataFrame computed in the compute() that gives the hierarchy of the care site structure. TYPE: pd . DataFrame start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_levels EXAMPLE : [\"Hospital\", \"Pole\", \"UF\"] TYPE: List [ str ], optional DEFAULT: None stay_types EXAMPLE : {\"All\": \".*\"} or {\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"} or \"hospitalis\u00e9s TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: None care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None care_site_short_names EXAMPLE : [\"HOSPITAL 1\", \"HOSPITAL 2\"] TYPE: List [ str ], optional DEFAULT: None Source code in edsteva/probes/visit.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , care_site_short_names : List [ str ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) hospital_visit = get_hospital_visit ( visit_occurrence , care_site , ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] visit_predictor_by_level = { hospital_name : hospital_visit } if not hospital_only ( care_site_levels = care_site_levels ): visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] uf_visit = get_uf_visit ( visit_occurrence , visit_detail , care_site , care_site_relationship , ) visit_predictor_by_level [ uf_name ] = uf_visit pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship , ) visit_predictor_by_level [ pole_name ] = pole_visit visit_predictor = concatenate_predictor_by_level ( predictor_by_level = visit_predictor_by_level , care_site_levels = care_site_levels , ) return compute_completeness ( visit_predictor )","title":"visit"},{"location":"reference/probes/visit/#edstevaprobesvisit","text":"","title":"edsteva.probes.visit"},{"location":"reference/probes/visit/#edsteva.probes.visit.VisitProbe","text":"Bases: BaseProbe The VisitProbe computes \\(c_(t)\\) the availability of administrative data related to visits for each care site according to time: \\[ c(t) = \\frac{n_{visit}(t)}{n_{99}} \\] Where \\(n_{visit}(t)\\) is the number of visits, \\(n_{99}\\) is the \\(99^{th}\\) percentile of visits and \\(t\\) is the month. ATTRIBUTE DESCRIPTION _index Variable from which data is grouped VALUE : [\"care_site_level\", \"stay_type\", \"care_site_id\"] TYPE: List [ str ] Source code in edsteva/probes/visit.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 class VisitProbe ( BaseProbe ): r \"\"\" The ``VisitProbe`` computes $c_(t)$ the availability of administrative data related to visits for each care site according to time: $$ c(t) = \\frac{n_{visit}(t)}{n_{99}} $$ Where $n_{visit}(t)$ is the number of visits, $n_{99}$ is the $99^{th}$ percentile of visits and $t$ is the month. Attributes ---------- _index: List[str] Variable from which data is grouped **VALUE**: ``[\"care_site_level\", \"stay_type\", \"care_site_id\"]`` \"\"\" _index = [ \"care_site_level\" , \"stay_type\" , \"care_site_id\" ] def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , care_site_short_names : List [ str ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) hospital_visit = get_hospital_visit ( visit_occurrence , care_site , ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] visit_predictor_by_level = { hospital_name : hospital_visit } if not hospital_only ( care_site_levels = care_site_levels ): visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] uf_visit = get_uf_visit ( visit_occurrence , visit_detail , care_site , care_site_relationship , ) visit_predictor_by_level [ uf_name ] = uf_visit pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship , ) visit_predictor_by_level [ pole_name ] = pole_visit visit_predictor = concatenate_predictor_by_level ( predictor_by_level = visit_predictor_by_level , care_site_levels = care_site_levels , ) return compute_completeness ( visit_predictor )","title":"VisitProbe"},{"location":"reference/probes/visit/#edsteva.probes.visit.VisitProbe.compute_process","text":"compute_process ( data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , care_site_short_names : List [ str ] = None , ) Script to be used by compute() PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data care_site_relationship DataFrame computed in the compute() that gives the hierarchy of the care site structure. TYPE: pd . DataFrame start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_levels EXAMPLE : [\"Hospital\", \"Pole\", \"UF\"] TYPE: List [ str ], optional DEFAULT: None stay_types EXAMPLE : {\"All\": \".*\"} or {\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"} or \"hospitalis\u00e9s TYPE: Union [ str , Dict [ str , str ]], optional DEFAULT: None care_site_ids EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None care_site_short_names EXAMPLE : [\"HOSPITAL 1\", \"HOSPITAL 2\"] TYPE: List [ str ], optional DEFAULT: None Source code in edsteva/probes/visit.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def compute_process ( self , data : Data , care_site_relationship : pd . DataFrame , start_date : datetime = None , end_date : datetime = None , care_site_levels : List [ str ] = None , stay_types : Union [ str , Dict [ str , str ]] = None , care_site_ids : List [ int ] = None , care_site_short_names : List [ str ] = None , ): \"\"\"Script to be used by [``compute()``][edsteva.probes.base.BaseProbe.compute] Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] care_site_relationship : pd.DataFrame DataFrame computed in the [``compute()``][edsteva.probes.base.BaseProbe.compute] that gives the hierarchy of the care site structure. start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_levels : List[str], optional **EXAMPLE**: `[\"Hospital\", \"Pole\", \"UF\"]` stay_types : Union[str, Dict[str, str]], optional **EXAMPLE**: `{\"All\": \".*\"}` or `{\"All\": \".*\", \"Urg_and_consult\": \"urgences|consultation\"}` or `\"hospitalis\u00e9s` care_site_ids : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_short_names : List[str], optional **EXAMPLE**: `[\"HOSPITAL 1\", \"HOSPITAL 2\"]` \"\"\" visit_occurrence = prepare_visit_occurrence ( data , start_date , end_date , stay_types , ) care_site = prepare_care_site ( data , care_site_ids , care_site_short_names , care_site_relationship , ) hospital_visit = get_hospital_visit ( visit_occurrence , care_site , ) hospital_name = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ] visit_predictor_by_level = { hospital_name : hospital_visit } if not hospital_only ( care_site_levels = care_site_levels ): visit_detail = prepare_visit_detail ( data , start_date , end_date ) uf_name = CARE_SITE_LEVEL_NAMES [ \"UF\" ] uf_visit = get_uf_visit ( visit_occurrence , visit_detail , care_site , care_site_relationship , ) visit_predictor_by_level [ uf_name ] = uf_visit pole_name = CARE_SITE_LEVEL_NAMES [ \"Pole\" ] pole_visit = get_pole_visit ( uf_visit , care_site , care_site_relationship , ) visit_predictor_by_level [ pole_name ] = pole_visit visit_predictor = concatenate_predictor_by_level ( predictor_by_level = visit_predictor_by_level , care_site_levels = care_site_levels , ) return compute_completeness ( visit_predictor )","title":"compute_process()"},{"location":"reference/utils/","text":"edsteva.utils","title":"`edsteva.utils`"},{"location":"reference/utils/#edstevautils","text":"","title":"edsteva.utils"},{"location":"reference/utils/checks/","text":"edsteva.utils.checks MissingColumnError Bases: Exception Exception raised when a concept is missing Source code in edsteva/utils/checks.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class MissingColumnError ( Exception ): \"\"\"Exception raised when a concept is missing\"\"\" def __init__ ( self , required_columns : Union [ List , dict ], df_name : str = \"\" , ): if isinstance ( required_columns , dict ): to_display_per_column = [ f \"- { column } ( { msg } )\" if msg is not None else f \" { column } \" for column , msg in required_columns . items () ] else : to_display_per_column = [ f \"- { column } \" for column in required_columns ] str_to_display = \" \\n \" . join ( to_display_per_column ) if df_name : df_name = f \" { df_name } \" message = ( f \"The { df_name } DataFrame is missing some columns, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message ) MissingTableError Bases: Exception Exception raised when a table is missing in the Data Source code in edsteva/utils/checks.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class MissingTableError ( Exception ): \"\"\"Exception raised when a table is missing in the Data\"\"\" def __init__ ( self , required_tables : Union [ List , dict ], data_name : str = \"\" , ): if isinstance ( required_tables , dict ): to_display_per_concept = [ f \"- { concept } ( { msg } )\" if msg is not None else f \" { concept } \" for concept , msg in required_tables . items () ] else : to_display_per_concept = [ f \"- { concept } \" for concept in required_tables ] str_to_display = \" \\n \" . join ( to_display_per_concept ) if data_name : data_name = f \" { data_name } \" message = ( f \"The { data_name } Data is missing some tables, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message )","title":"checks"},{"location":"reference/utils/checks/#edstevautilschecks","text":"","title":"edsteva.utils.checks"},{"location":"reference/utils/checks/#edsteva.utils.checks.MissingColumnError","text":"Bases: Exception Exception raised when a concept is missing Source code in edsteva/utils/checks.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class MissingColumnError ( Exception ): \"\"\"Exception raised when a concept is missing\"\"\" def __init__ ( self , required_columns : Union [ List , dict ], df_name : str = \"\" , ): if isinstance ( required_columns , dict ): to_display_per_column = [ f \"- { column } ( { msg } )\" if msg is not None else f \" { column } \" for column , msg in required_columns . items () ] else : to_display_per_column = [ f \"- { column } \" for column in required_columns ] str_to_display = \" \\n \" . join ( to_display_per_column ) if df_name : df_name = f \" { df_name } \" message = ( f \"The { df_name } DataFrame is missing some columns, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message )","title":"MissingColumnError"},{"location":"reference/utils/checks/#edsteva.utils.checks.MissingTableError","text":"Bases: Exception Exception raised when a table is missing in the Data Source code in edsteva/utils/checks.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class MissingTableError ( Exception ): \"\"\"Exception raised when a table is missing in the Data\"\"\" def __init__ ( self , required_tables : Union [ List , dict ], data_name : str = \"\" , ): if isinstance ( required_tables , dict ): to_display_per_concept = [ f \"- { concept } ( { msg } )\" if msg is not None else f \" { concept } \" for concept , msg in required_tables . items () ] else : to_display_per_concept = [ f \"- { concept } \" for concept in required_tables ] str_to_display = \" \\n \" . join ( to_display_per_concept ) if data_name : data_name = f \" { data_name } \" message = ( f \"The { data_name } Data is missing some tables, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message )","title":"MissingTableError"},{"location":"reference/utils/framework/","text":"edsteva.utils.framework","title":"framework"},{"location":"reference/utils/framework/#edstevautilsframework","text":"","title":"edsteva.utils.framework"},{"location":"reference/utils/loss_functions/","text":"edsteva.utils.loss_functions","title":"loss_functions"},{"location":"reference/utils/loss_functions/#edstevautilsloss_functions","text":"","title":"edsteva.utils.loss_functions"},{"location":"reference/utils/typing/","text":"edsteva.utils.typing","title":"typing"},{"location":"reference/utils/typing/#edstevautilstyping","text":"","title":"edsteva.utils.typing"},{"location":"reference/viz/","text":"edsteva.viz Top-level package for EDS-TeVa visualization.","title":"`edsteva.viz`"},{"location":"reference/viz/#edstevaviz","text":"Top-level package for EDS-TeVa visualization.","title":"edsteva.viz"},{"location":"reference/viz/utils/","text":"edsteva.viz.utils save_html save_html ( obj : alt . Chart , filename : str ) Save chart in the specified file PARAMETER DESCRIPTION obj Altair chart to be saved TYPE: alt . Chart filename Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str Source code in edsteva/viz/utils.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def save_html ( obj : alt . Chart , filename : str ): \"\"\"Save chart in the specified file Parameters ---------- obj : alt.Chart Altair chart to be saved filename : str Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not isinstance ( filename , Path ): filename = Path ( filename ) os . makedirs ( filename . parent , exist_ok = True ) if hasattr ( obj , \"save\" ): obj . save ( filename ) else : with open ( filename , \"w\" ) as f : f . write ( obj ) logger . info ( \"The chart has been saved in {} \" , filename )","title":"utils"},{"location":"reference/viz/utils/#edstevavizutils","text":"","title":"edsteva.viz.utils"},{"location":"reference/viz/utils/#edsteva.viz.utils.save_html","text":"save_html ( obj : alt . Chart , filename : str ) Save chart in the specified file PARAMETER DESCRIPTION obj Altair chart to be saved TYPE: alt . Chart filename Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str Source code in edsteva/viz/utils.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def save_html ( obj : alt . Chart , filename : str ): \"\"\"Save chart in the specified file Parameters ---------- obj : alt.Chart Altair chart to be saved filename : str Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" if not isinstance ( filename , Path ): filename = Path ( filename ) os . makedirs ( filename . parent , exist_ok = True ) if hasattr ( obj , \"save\" ): obj . save ( filename ) else : with open ( filename , \"w\" ) as f : f . write ( obj ) logger . info ( \"The chart has been saved in {} \" , filename )","title":"save_html()"},{"location":"reference/viz/dashboards/","text":"edsteva.viz.dashboards","title":"`edsteva.viz.dashboards`"},{"location":"reference/viz/dashboards/#edstevavizdashboards","text":"","title":"edsteva.viz.dashboards"},{"location":"reference/viz/dashboards/estimates_dashboard/","text":"edsteva.viz.dashboards.estimates_dashboard estimates_dashboard estimates_dashboard ( probe : BaseProbe , fitted_model : BaseModel , care_site_level : str = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ], save_path : str = None , ) Displays an interactive chart with: On the top, the aggregated normalized completeness predictor \\(\\frac{c(\\Delta t)}{c_0}\\) over normalized time \\(\\Delta t = t - t_0\\) . It represents the overall deviation from the Model. On the bottom, interactive filters including all the columns in the Probe (such as time, care site, number of visits...etc.) and all the estimates (coefficients and metrics) in the Model . Is is possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: CARE_SITE_LEVEL_NAMES['Hospital'] save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/viz/dashboards/estimates_dashboard.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 def estimates_dashboard ( probe : BaseProbe , fitted_model : BaseModel , care_site_level : str = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ], save_path : str = None , ): r \"\"\"Displays an interactive chart with: - On the top, the aggregated normalized completeness predictor $\\frac{c(\\Delta t)}{c_0}$ over normalized time $\\Delta t = t - t_0$. It represents the overall deviation from the Model. - On the bottom, interactive filters including all the columns in the [Probe][probe] (such as time, care site, number of visits...etc.) and all the estimates (coefficients and metrics) in the [Model][model]. Is is possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$ fitted_model : BaseModel Model fitted to the probe care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" alt . data_transformers . disable_max_rows () predictor = probe . predictor . copy () estimates = fitted_model . estimates . copy () predictor = predictor . merge ( estimates , on = probe . _index ) def month_diff ( x , y ): end = x . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) start = y . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) return end - start predictor [ \"date\" ] = predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"normalized_date\" ] = month_diff ( predictor [ \"date\" ], predictor [ \"t_0\" ]) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( str ) predictor [ \"normalized_date\" ] = predictor [ \"normalized_date\" ] . astype ( int ) predictor [ \"normalized_c\" ] = predictor [ \"c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] == 0 ), 1 ) predictor [ \"normalized_c\" ] = predictor [ \"normalized_c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] > 0 ), predictor [ \"c\" ] / predictor [ \"c_0\" ], ) predictor [ \"legend_error_band\" ] = \"Standard deviation\" predictor [ \"legend_model\" ] = type ( fitted_model ) . __name__ predictor [ \"model\" ] = predictor [ \"c_0\" ] . where ( predictor [ \"normalized_date\" ] < 0 , 1 ) predictor [ \"model\" ] = predictor [ \"model\" ] . where ( predictor [ \"normalized_date\" ] >= 0 , 0 ) predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : predictor = predictor [ predictor . date < predictor . t_1 ] index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) time = \"date\" _predictor = \"c\" c_0_min_slider = alt . binding_range ( min = 0 , max = round_it ( predictor . c_0 . max (), 2 ), step = scale_it ( predictor . c_0 . max ()) / 100 , name = \"c\u2080 min: \" , ) c_0_min_selection = alt . selection_single ( name = \"c_0_min\" , fields = [ \"c_0_min\" ], bind = c_0_min_slider , init = { \"c_0_min\" : 0 }, ) t_0_slider = alt . binding ( input = \"t_0\" , name = \"t\u2080 max: \" , ) t_0_selection = alt . selection_single ( name = \"t_0\" , fields = [ \"t_0\" ], bind = t_0_slider , init = { \"t_0\" : predictor . t_0 . astype ( str ) . max ()}, ) if fitted_model . name == \"RectangleFunction\" : t_1_slider = alt . binding ( input = \"t_1\" , name = \"t\u2081 min: \" , ) t_1_selection = alt . selection_single ( name = \"t_1\" , fields = [ \"t_1\" ], bind = t_1_slider , init = { \"t_1\" : predictor . t_1 . astype ( str ) . min ()}, ) error_max_slider = alt . binding_range ( min = 0 , max = round_it ( predictor . error . max (), 2 ), step = scale_it ( predictor . error . max ()) / 100 , name = \"error max: \" , ) error_max_selection = alt . selection_single ( name = \"error_max\" , fields = [ \"error_max\" ], bind = error_max_slider , init = { \"error_max\" : round_it ( predictor . error . max (), 2 )}, ) care_site_selection = alt . selection_multi ( fields = [ \"care_site_short_name\" ]) care_site_color = alt . condition ( care_site_selection , alt . Color ( \"care_site_short_name:N\" , legend = None , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) base_chart = alt . Chart ( predictor ) . transform_joinaggregate ( mean_c_0 = \"mean(c_0)\" , mean_error = \"mean(error)\" , groupby = [ \"care_site_short_name\" ] + index , ) time_selection = alt . selection_interval ( encodings = [ \"x\" ]) time_line = ( base_chart . mark_line () . encode ( x = alt . X ( \"normalized_date:Q\" , title = \"\u0394t = (t - t\u2080) months\" , scale = alt . Scale ( nice = False ), ), y = alt . Y ( \"mean(c):Q\" , title = \"c(\u0394t)\" , ), ) . add_selection ( time_selection ) ) . properties ( width = 800 , height = 50 ) predictor_hist = ( base_chart . mark_bar () . encode ( y = alt . Y ( \"care_site_short_name:N\" , title = \"Care site short name\" , sort = \"-x\" , ), color = care_site_color , ) . add_selection ( care_site_selection ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) ) . properties ( width = 300 ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : predictor_hist = predictor_hist . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) c_0_hist = predictor_hist . encode ( x = alt . X ( \"min(mean_c_0):Q\" , title = \"Min(c\u2080)\" , ), tooltip = alt . Tooltip ( \"min(mean_c_0):Q\" , format = \".2\" ), ) error_hist = predictor_hist . encode ( x = alt . X ( \"max(mean_error):Q\" , title = \"Max(error)\" , ), tooltip = alt . Tooltip ( \"max(mean_error):Q\" , format = \".2\" ), ) index_variables_hists = [] index_variables_selections = [] for index_variable in index : index_variable_selection = alt . selection_multi ( fields = [ index_variable ]) index_variables_selections . append ( index_variable_selection ) index_variable_color = alt . condition ( index_variable_selection , alt . Color ( \" {} :N\" . format ( index_variable ), legend = None , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) index_variable_hist = ( base_chart . mark_bar () . encode ( y = alt . Y ( \"mean(c):Q\" , title = \"c\" , ), x = alt . X ( \" {} :N\" . format ( index_variable ), title = index_variable , sort = \"-y\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), color = index_variable_color , ) . add_selection ( index_variable_selection ) . transform_filter ( care_site_selection ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) ) . properties ( title = \"Average completeness per {} \" . format ( index_variable )) # RectangleModel if fitted_model . name == \"RectangleFunction\" : index_variable_hist = index_variable_hist . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) index_variables_hists . append ( index_variable_hist ) extra_predictors = predictor . columns . difference ( index + [ time ] + [ _predictor ] + [ \"care_site_short_name\" , \"care_site_id\" , \"model\" , \"legend_error_band\" , \"legend_model\" , \"normalized_date\" , \"normalized_c\" , \"c_0\" , \"t_0\" , \"t_1\" , \"error\" , ] ) extra_predictors_hists = [] for extra_predictor in extra_predictors : extra_predictor_hist = predictor_hist . encode ( x = alt . X ( \"sum( {} ):Q\" . format ( extra_predictor ), title = \" {} \" . format ( extra_predictor ), axis = alt . Axis ( format = \"s\" ), ), tooltip = alt . Tooltip ( \"sum( {} ):Q\" . format ( extra_predictor ), format = \",\" ), ) . properties ( title = \"Total {} per care site\" . format ( extra_predictor )) extra_predictors_hists . append ( extra_predictor_hist ) index . append ( \"care_site_short_name\" ) index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : \"stay_type\" }, ) probe_line = ( base_chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( x = alt . X ( \"normalized_date:Q\" , title = \"\u0394t = (t - t\u2080) months\" , scale = alt . Scale ( nice = False ), ), color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) . transform_filter ( care_site_selection ) . transform_filter ( index_selection ) ) . properties ( width = 900 , height = 300 ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : probe_line = probe_line . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) mean_line = probe_line . mark_line () . encode ( y = alt . Y ( \"mean(normalized_c):Q\" , ) ) error_line = probe_line . mark_errorband ( extent = \"stdev\" ) . encode ( y = alt . Y ( \"normalized_c:Q\" , title = \"c(\u0394t) / c\u2080\" , ), stroke = alt . Stroke ( \"legend_error_band\" , title = \"Error band\" , legend = alt . Legend ( symbolType = \"square\" , orient = \"top\" ), ), ) model_line = ( alt . Chart ( predictor ) . mark_line ( color = \"black\" , interpolate = \"step-after\" , strokeDash = [ 5 , 5 ]) . encode ( x = alt . X ( \"normalized_date:Q\" , scale = alt . Scale ( nice = False ), ), y = \"model:Q\" , strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"Model line\" , legend = alt . Legend ( orient = \"top\" , symbolDash = [ 2 , 2 ]), ), ) ) top_chart = mean_line + error_line + model_line top_chart = top_chart . add_selection ( index_selection ) . transform_filter ( time_selection ) for index_variable_selection in index_variables_selections : top_chart = top_chart . transform_filter ( index_variable_selection ) c_0_hist = c_0_hist . transform_filter ( index_variable_selection ) error_hist = error_hist . transform_filter ( index_variable_selection ) for idx in range ( len ( extra_predictors_hists )): extra_predictors_hists [ idx ] = extra_predictors_hists [ idx ] . transform_filter ( index_variable_selection ) for idx in range ( len ( index_variables_hists )): if idx != index_variables_selections . index ( index_variable_selection ): index_variables_hists [ idx ] = index_variables_hists [ idx ] . transform_filter ( index_variable_selection ) index_variables_hists = reduce ( lambda index_variable_hist_1 , index_variable_hist_2 : index_variable_hist_1 & index_variable_hist_2 , index_variables_hists , ) extra_predictors_hists = reduce ( lambda extra_predictor_hist_1 , extra_predictor_hist_2 : extra_predictor_hist_1 & extra_predictor_hist_2 , extra_predictors_hists , ) chart = ( ( ( alt . vconcat ( alt . vconcat ( top_chart , time_line , spacing = 130 ), ( index_variables_hists | c_0_hist | error_hist & extra_predictors_hists ), spacing = 10 , ) ) . resolve_scale ( color = \"independent\" ) . add_selection ( error_max_selection ) . add_selection ( c_0_min_selection ) . add_selection ( t_0_selection ) ) . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ) ) if fitted_model . name == \"RectangleFunction\" : chart = chart . add_selection ( t_1_selection ) vis_threshold = \"id\" + uuid . uuid4 () . hex new_sliders_threshold_id = \"id\" + uuid . uuid4 () . hex old_sliders_threshold_id = \"id\" + uuid . uuid4 () . hex new_index_threshold_id = \"id\" + uuid . uuid4 () . hex old_index_threshold_id = \"id\" + uuid . uuid4 () . hex html_chart = f \"\"\" <!DOCTYPE html> <html> <head> <script src=\"https://cdn.jsdelivr.net/npm/vega@ { alt . VEGA_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@ { alt . VEGALITE_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@ { alt . VEGAEMBED_VERSION } \"></script> </head> <body> <div class=\"container\"> <div class=\"row\"> <div> <div id= { vis_threshold } ></div> </div> <div style=\"position:absolute;left:920px;top:520px;width: -webkit-fill-available;\"> <div id= { new_sliders_threshold_id } > <div id= { old_sliders_threshold_id } ></div> </div> </div> <div style=\"position:absolute;left:45px;top:390px;width: -webkit-fill-available;\"> <div id= { new_index_threshold_id } > <div id= { old_index_threshold_id } ></div> </div> <hr/> <h1 style=\"text-align:center\"> Interactive filters </h1> </div> </div> </div> <script type=\"text/javascript\"> vegaEmbed('# { vis_threshold } ', { chart . to_json ( indent = None ) } ).then(function(result) {{ const sliders = document.getElementsByClassName('vega-bindings'); const newestimate = document.getElementById(' { new_sliders_threshold_id } '); const oldestimate = document.getElementById(' { old_sliders_threshold_id } '); const newparent = document.getElementById(' { new_index_threshold_id } '); const oldchild = document.getElementById(' { old_index_threshold_id } '); for (var i = 0; i < sliders.length; i++) {{ if (sliders[i].parentElement.parentElement.id == ' { vis_threshold } ') {{ var estimate_slider = sliders[i] var index_slider = estimate_slider.querySelectorAll(\".vega-bind\") }} }} newestimate.replaceChild(estimate_slider, oldestimate); for (var i = 0; i < index_slider.length; i++) {{ if (index_slider[i].firstChild.innerHTML == \"Plot average completeness per: \") {{ var index_color = index_slider[i] }} }} newparent.replaceChild(index_color, oldchild); }} ).catch(console.error); </script> </body> </html> \"\"\" display ( HTML ( html_chart )) if save_path : save_html ( obj = html_chart , filename = save_path , )","title":"estimates_dashboard"},{"location":"reference/viz/dashboards/estimates_dashboard/#edstevavizdashboardsestimates_dashboard","text":"","title":"edsteva.viz.dashboards.estimates_dashboard"},{"location":"reference/viz/dashboards/estimates_dashboard/#edsteva.viz.dashboards.estimates_dashboard.estimates_dashboard","text":"estimates_dashboard ( probe : BaseProbe , fitted_model : BaseModel , care_site_level : str = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ], save_path : str = None , ) Displays an interactive chart with: On the top, the aggregated normalized completeness predictor \\(\\frac{c(\\Delta t)}{c_0}\\) over normalized time \\(\\Delta t = t - t_0\\) . It represents the overall deviation from the Model. On the bottom, interactive filters including all the columns in the Probe (such as time, care site, number of visits...etc.) and all the estimates (coefficients and metrics) in the Model . Is is possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: CARE_SITE_LEVEL_NAMES['Hospital'] save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/viz/dashboards/estimates_dashboard.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 def estimates_dashboard ( probe : BaseProbe , fitted_model : BaseModel , care_site_level : str = CARE_SITE_LEVEL_NAMES [ \"Hospital\" ], save_path : str = None , ): r \"\"\"Displays an interactive chart with: - On the top, the aggregated normalized completeness predictor $\\frac{c(\\Delta t)}{c_0}$ over normalized time $\\Delta t = t - t_0$. It represents the overall deviation from the Model. - On the bottom, interactive filters including all the columns in the [Probe][probe] (such as time, care site, number of visits...etc.) and all the estimates (coefficients and metrics) in the [Model][model]. Is is possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$ fitted_model : BaseModel Model fitted to the probe care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" alt . data_transformers . disable_max_rows () predictor = probe . predictor . copy () estimates = fitted_model . estimates . copy () predictor = predictor . merge ( estimates , on = probe . _index ) def month_diff ( x , y ): end = x . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) start = y . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) return end - start predictor [ \"date\" ] = predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"normalized_date\" ] = month_diff ( predictor [ \"date\" ], predictor [ \"t_0\" ]) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( str ) predictor [ \"normalized_date\" ] = predictor [ \"normalized_date\" ] . astype ( int ) predictor [ \"normalized_c\" ] = predictor [ \"c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] == 0 ), 1 ) predictor [ \"normalized_c\" ] = predictor [ \"normalized_c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] > 0 ), predictor [ \"c\" ] / predictor [ \"c_0\" ], ) predictor [ \"legend_error_band\" ] = \"Standard deviation\" predictor [ \"legend_model\" ] = type ( fitted_model ) . __name__ predictor [ \"model\" ] = predictor [ \"c_0\" ] . where ( predictor [ \"normalized_date\" ] < 0 , 1 ) predictor [ \"model\" ] = predictor [ \"model\" ] . where ( predictor [ \"normalized_date\" ] >= 0 , 0 ) predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : predictor = predictor [ predictor . date < predictor . t_1 ] index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) time = \"date\" _predictor = \"c\" c_0_min_slider = alt . binding_range ( min = 0 , max = round_it ( predictor . c_0 . max (), 2 ), step = scale_it ( predictor . c_0 . max ()) / 100 , name = \"c\u2080 min: \" , ) c_0_min_selection = alt . selection_single ( name = \"c_0_min\" , fields = [ \"c_0_min\" ], bind = c_0_min_slider , init = { \"c_0_min\" : 0 }, ) t_0_slider = alt . binding ( input = \"t_0\" , name = \"t\u2080 max: \" , ) t_0_selection = alt . selection_single ( name = \"t_0\" , fields = [ \"t_0\" ], bind = t_0_slider , init = { \"t_0\" : predictor . t_0 . astype ( str ) . max ()}, ) if fitted_model . name == \"RectangleFunction\" : t_1_slider = alt . binding ( input = \"t_1\" , name = \"t\u2081 min: \" , ) t_1_selection = alt . selection_single ( name = \"t_1\" , fields = [ \"t_1\" ], bind = t_1_slider , init = { \"t_1\" : predictor . t_1 . astype ( str ) . min ()}, ) error_max_slider = alt . binding_range ( min = 0 , max = round_it ( predictor . error . max (), 2 ), step = scale_it ( predictor . error . max ()) / 100 , name = \"error max: \" , ) error_max_selection = alt . selection_single ( name = \"error_max\" , fields = [ \"error_max\" ], bind = error_max_slider , init = { \"error_max\" : round_it ( predictor . error . max (), 2 )}, ) care_site_selection = alt . selection_multi ( fields = [ \"care_site_short_name\" ]) care_site_color = alt . condition ( care_site_selection , alt . Color ( \"care_site_short_name:N\" , legend = None , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) base_chart = alt . Chart ( predictor ) . transform_joinaggregate ( mean_c_0 = \"mean(c_0)\" , mean_error = \"mean(error)\" , groupby = [ \"care_site_short_name\" ] + index , ) time_selection = alt . selection_interval ( encodings = [ \"x\" ]) time_line = ( base_chart . mark_line () . encode ( x = alt . X ( \"normalized_date:Q\" , title = \"\u0394t = (t - t\u2080) months\" , scale = alt . Scale ( nice = False ), ), y = alt . Y ( \"mean(c):Q\" , title = \"c(\u0394t)\" , ), ) . add_selection ( time_selection ) ) . properties ( width = 800 , height = 50 ) predictor_hist = ( base_chart . mark_bar () . encode ( y = alt . Y ( \"care_site_short_name:N\" , title = \"Care site short name\" , sort = \"-x\" , ), color = care_site_color , ) . add_selection ( care_site_selection ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) ) . properties ( width = 300 ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : predictor_hist = predictor_hist . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) c_0_hist = predictor_hist . encode ( x = alt . X ( \"min(mean_c_0):Q\" , title = \"Min(c\u2080)\" , ), tooltip = alt . Tooltip ( \"min(mean_c_0):Q\" , format = \".2\" ), ) error_hist = predictor_hist . encode ( x = alt . X ( \"max(mean_error):Q\" , title = \"Max(error)\" , ), tooltip = alt . Tooltip ( \"max(mean_error):Q\" , format = \".2\" ), ) index_variables_hists = [] index_variables_selections = [] for index_variable in index : index_variable_selection = alt . selection_multi ( fields = [ index_variable ]) index_variables_selections . append ( index_variable_selection ) index_variable_color = alt . condition ( index_variable_selection , alt . Color ( \" {} :N\" . format ( index_variable ), legend = None , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) index_variable_hist = ( base_chart . mark_bar () . encode ( y = alt . Y ( \"mean(c):Q\" , title = \"c\" , ), x = alt . X ( \" {} :N\" . format ( index_variable ), title = index_variable , sort = \"-y\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), color = index_variable_color , ) . add_selection ( index_variable_selection ) . transform_filter ( care_site_selection ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) ) . properties ( title = \"Average completeness per {} \" . format ( index_variable )) # RectangleModel if fitted_model . name == \"RectangleFunction\" : index_variable_hist = index_variable_hist . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) index_variables_hists . append ( index_variable_hist ) extra_predictors = predictor . columns . difference ( index + [ time ] + [ _predictor ] + [ \"care_site_short_name\" , \"care_site_id\" , \"model\" , \"legend_error_band\" , \"legend_model\" , \"normalized_date\" , \"normalized_c\" , \"c_0\" , \"t_0\" , \"t_1\" , \"error\" , ] ) extra_predictors_hists = [] for extra_predictor in extra_predictors : extra_predictor_hist = predictor_hist . encode ( x = alt . X ( \"sum( {} ):Q\" . format ( extra_predictor ), title = \" {} \" . format ( extra_predictor ), axis = alt . Axis ( format = \"s\" ), ), tooltip = alt . Tooltip ( \"sum( {} ):Q\" . format ( extra_predictor ), format = \",\" ), ) . properties ( title = \"Total {} per care site\" . format ( extra_predictor )) extra_predictors_hists . append ( extra_predictor_hist ) index . append ( \"care_site_short_name\" ) index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : \"stay_type\" }, ) probe_line = ( base_chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( x = alt . X ( \"normalized_date:Q\" , title = \"\u0394t = (t - t\u2080) months\" , scale = alt . Scale ( nice = False ), ), color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) . transform_filter ( care_site_selection ) . transform_filter ( index_selection ) ) . properties ( width = 900 , height = 300 ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : probe_line = probe_line . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) mean_line = probe_line . mark_line () . encode ( y = alt . Y ( \"mean(normalized_c):Q\" , ) ) error_line = probe_line . mark_errorband ( extent = \"stdev\" ) . encode ( y = alt . Y ( \"normalized_c:Q\" , title = \"c(\u0394t) / c\u2080\" , ), stroke = alt . Stroke ( \"legend_error_band\" , title = \"Error band\" , legend = alt . Legend ( symbolType = \"square\" , orient = \"top\" ), ), ) model_line = ( alt . Chart ( predictor ) . mark_line ( color = \"black\" , interpolate = \"step-after\" , strokeDash = [ 5 , 5 ]) . encode ( x = alt . X ( \"normalized_date:Q\" , scale = alt . Scale ( nice = False ), ), y = \"model:Q\" , strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"Model line\" , legend = alt . Legend ( orient = \"top\" , symbolDash = [ 2 , 2 ]), ), ) ) top_chart = mean_line + error_line + model_line top_chart = top_chart . add_selection ( index_selection ) . transform_filter ( time_selection ) for index_variable_selection in index_variables_selections : top_chart = top_chart . transform_filter ( index_variable_selection ) c_0_hist = c_0_hist . transform_filter ( index_variable_selection ) error_hist = error_hist . transform_filter ( index_variable_selection ) for idx in range ( len ( extra_predictors_hists )): extra_predictors_hists [ idx ] = extra_predictors_hists [ idx ] . transform_filter ( index_variable_selection ) for idx in range ( len ( index_variables_hists )): if idx != index_variables_selections . index ( index_variable_selection ): index_variables_hists [ idx ] = index_variables_hists [ idx ] . transform_filter ( index_variable_selection ) index_variables_hists = reduce ( lambda index_variable_hist_1 , index_variable_hist_2 : index_variable_hist_1 & index_variable_hist_2 , index_variables_hists , ) extra_predictors_hists = reduce ( lambda extra_predictor_hist_1 , extra_predictor_hist_2 : extra_predictor_hist_1 & extra_predictor_hist_2 , extra_predictors_hists , ) chart = ( ( ( alt . vconcat ( alt . vconcat ( top_chart , time_line , spacing = 130 ), ( index_variables_hists | c_0_hist | error_hist & extra_predictors_hists ), spacing = 10 , ) ) . resolve_scale ( color = \"independent\" ) . add_selection ( error_max_selection ) . add_selection ( c_0_min_selection ) . add_selection ( t_0_selection ) ) . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ) ) if fitted_model . name == \"RectangleFunction\" : chart = chart . add_selection ( t_1_selection ) vis_threshold = \"id\" + uuid . uuid4 () . hex new_sliders_threshold_id = \"id\" + uuid . uuid4 () . hex old_sliders_threshold_id = \"id\" + uuid . uuid4 () . hex new_index_threshold_id = \"id\" + uuid . uuid4 () . hex old_index_threshold_id = \"id\" + uuid . uuid4 () . hex html_chart = f \"\"\" <!DOCTYPE html> <html> <head> <script src=\"https://cdn.jsdelivr.net/npm/vega@ { alt . VEGA_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@ { alt . VEGALITE_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@ { alt . VEGAEMBED_VERSION } \"></script> </head> <body> <div class=\"container\"> <div class=\"row\"> <div> <div id= { vis_threshold } ></div> </div> <div style=\"position:absolute;left:920px;top:520px;width: -webkit-fill-available;\"> <div id= { new_sliders_threshold_id } > <div id= { old_sliders_threshold_id } ></div> </div> </div> <div style=\"position:absolute;left:45px;top:390px;width: -webkit-fill-available;\"> <div id= { new_index_threshold_id } > <div id= { old_index_threshold_id } ></div> </div> <hr/> <h1 style=\"text-align:center\"> Interactive filters </h1> </div> </div> </div> <script type=\"text/javascript\"> vegaEmbed('# { vis_threshold } ', { chart . to_json ( indent = None ) } ).then(function(result) {{ const sliders = document.getElementsByClassName('vega-bindings'); const newestimate = document.getElementById(' { new_sliders_threshold_id } '); const oldestimate = document.getElementById(' { old_sliders_threshold_id } '); const newparent = document.getElementById(' { new_index_threshold_id } '); const oldchild = document.getElementById(' { old_index_threshold_id } '); for (var i = 0; i < sliders.length; i++) {{ if (sliders[i].parentElement.parentElement.id == ' { vis_threshold } ') {{ var estimate_slider = sliders[i] var index_slider = estimate_slider.querySelectorAll(\".vega-bind\") }} }} newestimate.replaceChild(estimate_slider, oldestimate); for (var i = 0; i < index_slider.length; i++) {{ if (index_slider[i].firstChild.innerHTML == \"Plot average completeness per: \") {{ var index_color = index_slider[i] }} }} newparent.replaceChild(index_color, oldchild); }} ).catch(console.error); </script> </body> </html> \"\"\" display ( HTML ( html_chart )) if save_path : save_html ( obj = html_chart , filename = save_path , )","title":"estimates_dashboard()"},{"location":"reference/viz/dashboards/predictor_dashboard/","text":"edsteva.viz.dashboards.predictor_dashboard","title":"`edsteva.viz.dashboards.predictor_dashboard`"},{"location":"reference/viz/dashboards/predictor_dashboard/#edstevavizdashboardspredictor_dashboard","text":"","title":"edsteva.viz.dashboards.predictor_dashboard"},{"location":"reference/viz/dashboards/predictor_dashboard/fitted_probe/","text":"edsteva.viz.dashboards.predictor_dashboard.fitted_probe fitted_probe_dashboard fitted_probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ) Script to be used by predictor_dashboard() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe with its prediction \\(\\hat{c}(t)\\) TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True Source code in edsteva/viz/dashboards/predictor_dashboard/fitted_probe.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def fitted_probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ): r \"\"\"Script to be used by [``predictor_dashboard()``][edsteva.viz.dashboards.predictor_dashboard.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe with its prediction $\\hat{c}(t)$ index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. \"\"\" alt . data_transformers . disable_max_rows () predictor [ \"legend_predictor\" ] = \"Predictor c(t)\" predictor [ \"legend_model\" ] = \"Model f(t)\" time_selection = alt . selection_interval ( encodings = [ \"x\" ]) time_line = ( alt . Chart ( predictor ) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 ), ), y = alt . Y ( \"mean(c):Q\" , title = \"c(t)\" , ), ) . add_selection ( time_selection ) ) . properties ( width = 900 , height = 50 ) care_site_selection = alt . selection_multi ( fields = [ \"care_site_short_name\" ]) care_site_color = alt . condition ( care_site_selection , alt . Color ( \"care_site_short_name:N\" , legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) predictor_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"care_site_short_name:N\" , title = \"Care site short name\" , sort = \"-x\" , ), color = care_site_color , ) . transform_filter ( time_selection ) . add_selection ( care_site_selection ) ) . properties ( width = 300 ) care_site_hist = predictor_hist . encode ( x = alt . X ( \"mean(c):Q\" , title = \"c\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), ) . properties ( title = \"Average completeness per care site\" ) index_variables_hists = [] index_variables_selections = [] for index_variable in index : index_variable_selection = alt . selection_multi ( fields = [ index_variable ]) index_variables_selections . append ( index_variable_selection ) index_variable_color = alt . condition ( index_variable_selection , alt . Color ( \" {} :N\" . format ( index_variable ), legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) index_variable_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"mean(c):Q\" , title = \"c\" , ), x = alt . X ( \" {} :N\" . format ( index_variable ), title = index_variable , sort = \"-y\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), color = index_variable_color , ) . add_selection ( index_variable_selection ) . transform_filter ( time_selection ) . transform_filter ( care_site_selection ) ) . properties ( title = \"Average completeness per {} \" . format ( index_variable )) index_variables_hists . append ( index_variable_hist ) extra_predictors = predictor . columns . difference ( index + [ \"date\" , \"care_site_short_name\" , \"care_site_id\" , \"c\" , \"c_hat\" , \"legend_predictor\" , \"legend_model\" , ] ) extra_predictors_hists = [] for extra_predictor in extra_predictors : extra_predictor_hist = predictor_hist . encode ( x = alt . X ( \"sum( {} ):Q\" . format ( extra_predictor ), title = \" {} \" . format ( extra_predictor ), axis = alt . Axis ( format = \"s\" ), ), tooltip = alt . Tooltip ( \"sum( {} ):Q\" . format ( extra_predictor ), format = \",\" ), ) . properties ( title = \"Total {} per care site\" . format ( extra_predictor )) extra_predictors_hists . append ( extra_predictor_hist ) index . append ( \"care_site_short_name\" ) index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : \"stay_type\" }, ) base_line = ( alt . Chart ( predictor ) . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( index_selection ) ) . properties ( width = 900 , height = 300 ) probe_line = base_line . mark_line () . encode ( y = alt . Y ( \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), strokeDash = alt . StrokeDash ( \"legend_predictor\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , orient = \"left\" , ), ), ) model_line = base_line . mark_line ( interpolate = \"step-after\" , strokeDash = [ 5 , 5 ] ) . encode ( y = alt . Y ( \"mean(c_hat):Q\" , ), strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , symbolDash = [ 2 , 2 ], orient = \"left\" , ), ), ) fitted_probe = ( probe_line + model_line ) . add_selection ( index_selection ) fitted_probe = fitted_probe . transform_filter ( care_site_selection ) fitted_probe = fitted_probe . transform_filter ( time_selection ) for index_variable_selection in index_variables_selections : fitted_probe = fitted_probe . transform_filter ( index_variable_selection ) care_site_hist = care_site_hist . transform_filter ( index_variable_selection ) for extra_predictor_hist in extra_predictors_hists : extra_predictor_hist = extra_predictor_hist . transform_filter ( index_variable_selection ) for idx in range ( len ( index_variables_hists )): if idx != index_variables_selections . index ( index_variable_selection ): index_variables_hists [ idx ] = index_variables_hists [ idx ] . transform_filter ( index_variable_selection ) index_variables_hists = reduce ( lambda index_variable_hist_1 , index_variable_hist_2 : index_variable_hist_1 & index_variable_hist_2 , index_variables_hists , ) extra_predictors_hists = reduce ( lambda extra_predictor_hist_1 , extra_predictor_hist_2 : extra_predictor_hist_1 & extra_predictor_hist_2 , extra_predictors_hists , ) chart = ( alt . vconcat ( alt . vconcat ( fitted_probe , time_line , spacing = 130 ), ( index_variables_hists | care_site_hist | extra_predictors_hists ), spacing = 10 , ) ) . resolve_scale ( color = \"independent\" ) return chart","title":"fitted_probe"},{"location":"reference/viz/dashboards/predictor_dashboard/fitted_probe/#edstevavizdashboardspredictor_dashboardfitted_probe","text":"","title":"edsteva.viz.dashboards.predictor_dashboard.fitted_probe"},{"location":"reference/viz/dashboards/predictor_dashboard/fitted_probe/#edsteva.viz.dashboards.predictor_dashboard.fitted_probe.fitted_probe_dashboard","text":"fitted_probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ) Script to be used by predictor_dashboard() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe with its prediction \\(\\hat{c}(t)\\) TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True Source code in edsteva/viz/dashboards/predictor_dashboard/fitted_probe.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def fitted_probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ): r \"\"\"Script to be used by [``predictor_dashboard()``][edsteva.viz.dashboards.predictor_dashboard.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe with its prediction $\\hat{c}(t)$ index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. \"\"\" alt . data_transformers . disable_max_rows () predictor [ \"legend_predictor\" ] = \"Predictor c(t)\" predictor [ \"legend_model\" ] = \"Model f(t)\" time_selection = alt . selection_interval ( encodings = [ \"x\" ]) time_line = ( alt . Chart ( predictor ) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 ), ), y = alt . Y ( \"mean(c):Q\" , title = \"c(t)\" , ), ) . add_selection ( time_selection ) ) . properties ( width = 900 , height = 50 ) care_site_selection = alt . selection_multi ( fields = [ \"care_site_short_name\" ]) care_site_color = alt . condition ( care_site_selection , alt . Color ( \"care_site_short_name:N\" , legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) predictor_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"care_site_short_name:N\" , title = \"Care site short name\" , sort = \"-x\" , ), color = care_site_color , ) . transform_filter ( time_selection ) . add_selection ( care_site_selection ) ) . properties ( width = 300 ) care_site_hist = predictor_hist . encode ( x = alt . X ( \"mean(c):Q\" , title = \"c\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), ) . properties ( title = \"Average completeness per care site\" ) index_variables_hists = [] index_variables_selections = [] for index_variable in index : index_variable_selection = alt . selection_multi ( fields = [ index_variable ]) index_variables_selections . append ( index_variable_selection ) index_variable_color = alt . condition ( index_variable_selection , alt . Color ( \" {} :N\" . format ( index_variable ), legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) index_variable_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"mean(c):Q\" , title = \"c\" , ), x = alt . X ( \" {} :N\" . format ( index_variable ), title = index_variable , sort = \"-y\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), color = index_variable_color , ) . add_selection ( index_variable_selection ) . transform_filter ( time_selection ) . transform_filter ( care_site_selection ) ) . properties ( title = \"Average completeness per {} \" . format ( index_variable )) index_variables_hists . append ( index_variable_hist ) extra_predictors = predictor . columns . difference ( index + [ \"date\" , \"care_site_short_name\" , \"care_site_id\" , \"c\" , \"c_hat\" , \"legend_predictor\" , \"legend_model\" , ] ) extra_predictors_hists = [] for extra_predictor in extra_predictors : extra_predictor_hist = predictor_hist . encode ( x = alt . X ( \"sum( {} ):Q\" . format ( extra_predictor ), title = \" {} \" . format ( extra_predictor ), axis = alt . Axis ( format = \"s\" ), ), tooltip = alt . Tooltip ( \"sum( {} ):Q\" . format ( extra_predictor ), format = \",\" ), ) . properties ( title = \"Total {} per care site\" . format ( extra_predictor )) extra_predictors_hists . append ( extra_predictor_hist ) index . append ( \"care_site_short_name\" ) index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : \"stay_type\" }, ) base_line = ( alt . Chart ( predictor ) . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( index_selection ) ) . properties ( width = 900 , height = 300 ) probe_line = base_line . mark_line () . encode ( y = alt . Y ( \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), strokeDash = alt . StrokeDash ( \"legend_predictor\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , orient = \"left\" , ), ), ) model_line = base_line . mark_line ( interpolate = \"step-after\" , strokeDash = [ 5 , 5 ] ) . encode ( y = alt . Y ( \"mean(c_hat):Q\" , ), strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , symbolDash = [ 2 , 2 ], orient = \"left\" , ), ), ) fitted_probe = ( probe_line + model_line ) . add_selection ( index_selection ) fitted_probe = fitted_probe . transform_filter ( care_site_selection ) fitted_probe = fitted_probe . transform_filter ( time_selection ) for index_variable_selection in index_variables_selections : fitted_probe = fitted_probe . transform_filter ( index_variable_selection ) care_site_hist = care_site_hist . transform_filter ( index_variable_selection ) for extra_predictor_hist in extra_predictors_hists : extra_predictor_hist = extra_predictor_hist . transform_filter ( index_variable_selection ) for idx in range ( len ( index_variables_hists )): if idx != index_variables_selections . index ( index_variable_selection ): index_variables_hists [ idx ] = index_variables_hists [ idx ] . transform_filter ( index_variable_selection ) index_variables_hists = reduce ( lambda index_variable_hist_1 , index_variable_hist_2 : index_variable_hist_1 & index_variable_hist_2 , index_variables_hists , ) extra_predictors_hists = reduce ( lambda extra_predictor_hist_1 , extra_predictor_hist_2 : extra_predictor_hist_1 & extra_predictor_hist_2 , extra_predictors_hists , ) chart = ( alt . vconcat ( alt . vconcat ( fitted_probe , time_line , spacing = 130 ), ( index_variables_hists | care_site_hist | extra_predictors_hists ), spacing = 10 , ) ) . resolve_scale ( color = \"independent\" ) return chart","title":"fitted_probe_dashboard()"},{"location":"reference/viz/dashboards/predictor_dashboard/probe/","text":"edsteva.viz.dashboards.predictor_dashboard.probe probe_dashboard probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ) Script to be used by predictor_dashboard() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit show the number of visit instead of the completeness predictor \\(c(t)\\) TYPE: bool DEFAULT: False Source code in edsteva/viz/dashboards/predictor_dashboard/probe.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ): \"\"\"Script to be used by [``predictor_dashboard()``][edsteva.viz.dashboards.predictor_dashboard.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional show the number of visit instead of the completeness predictor $c(t)$ \"\"\" time = \"date\" _predictor = \"c\" alt . data_transformers . disable_max_rows () time_selection = alt . selection_interval ( encodings = [ \"x\" ]) time_line = ( alt . Chart ( predictor ) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 ), ), y = alt . Y ( \"mean(c):Q\" , title = \"c(t)\" , ), ) . add_selection ( time_selection ) ) . properties ( width = 900 , height = 50 ) care_site_selection = alt . selection_multi ( fields = [ \"care_site_short_name\" ]) care_site_color = alt . condition ( care_site_selection , alt . Color ( \"care_site_short_name:N\" , legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) predictor_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"care_site_short_name:N\" , title = \"Care site short name\" , sort = \"-x\" , ), color = care_site_color , ) . transform_filter ( time_selection ) . add_selection ( care_site_selection ) ) . properties ( width = 300 ) care_site_hist = predictor_hist . encode ( x = alt . X ( \"mean(c):Q\" , title = \"c\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), ) . properties ( title = \"Average completeness per care site\" ) index_variables_hists = [] index_variables_selections = [] for index_variable in index : index_variable_selection = alt . selection_multi ( fields = [ index_variable ]) index_variables_selections . append ( index_variable_selection ) index_variable_color = alt . condition ( index_variable_selection , alt . Color ( \" {} :N\" . format ( index_variable ), legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) index_variable_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"mean(c):Q\" , title = \"c\" , ), x = alt . X ( \" {} :N\" . format ( index_variable ), title = index_variable , sort = \"-y\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), color = index_variable_color , ) . add_selection ( index_variable_selection ) . transform_filter ( time_selection ) . transform_filter ( care_site_selection ) ) . properties ( title = \"Average completeness per {} \" . format ( index_variable )) index_variables_hists . append ( index_variable_hist ) extra_predictors = predictor . columns . difference ( index + [ time ] + [ _predictor ] + [ \"care_site_short_name\" , \"care_site_id\" , \"c_hat\" ] ) extra_predictors_hists = [] for extra_predictor in extra_predictors : extra_predictor_hist = predictor_hist . encode ( x = alt . X ( \"sum( {} ):Q\" . format ( extra_predictor ), title = \" {} \" . format ( extra_predictor ), axis = alt . Axis ( format = \"s\" ), ), tooltip = alt . Tooltip ( \"sum( {} ):Q\" . format ( extra_predictor ), format = \",\" ), ) . properties ( title = \"Total {} per care site\" . format ( extra_predictor )) extra_predictors_hists . append ( extra_predictor_hist ) index . append ( \"care_site_short_name\" ) index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : \"stay_type\" }, ) probe_line = ( alt . Chart ( predictor ) . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), y = alt . Y ( \"sum(n_visit):Q\" if show_n_visit else \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, title = None , ), ) . add_selection ( index_selection ) . transform_filter ( index_selection ) . transform_filter ( care_site_selection ) . transform_filter ( time_selection ) ) . properties ( width = 900 , height = 300 ) for index_variable_selection in index_variables_selections : probe_line = probe_line . transform_filter ( index_variable_selection ) care_site_hist = care_site_hist . transform_filter ( index_variable_selection ) for idx in range ( len ( extra_predictors_hists )): extra_predictors_hists [ idx ] = extra_predictors_hists [ idx ] . transform_filter ( index_variable_selection ) for idx in range ( len ( index_variables_hists )): if idx != index_variables_selections . index ( index_variable_selection ): index_variables_hists [ idx ] = index_variables_hists [ idx ] . transform_filter ( index_variable_selection ) index_variables_hists = reduce ( lambda index_variable_hist_1 , index_variable_hist_2 : index_variable_hist_1 & index_variable_hist_2 , index_variables_hists , ) extra_predictors_hists = reduce ( lambda extra_predictor_hist_1 , extra_predictor_hist_2 : extra_predictor_hist_1 & extra_predictor_hist_2 , extra_predictors_hists , ) chart = ( alt . vconcat ( alt . vconcat ( probe_line , time_line , spacing = 130 ), ( index_variables_hists | care_site_hist | extra_predictors_hists ), spacing = 10 , ) ) . resolve_scale ( color = \"independent\" ) return chart","title":"probe"},{"location":"reference/viz/dashboards/predictor_dashboard/probe/#edstevavizdashboardspredictor_dashboardprobe","text":"","title":"edsteva.viz.dashboards.predictor_dashboard.probe"},{"location":"reference/viz/dashboards/predictor_dashboard/probe/#edsteva.viz.dashboards.predictor_dashboard.probe.probe_dashboard","text":"probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ) Script to be used by predictor_dashboard() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit show the number of visit instead of the completeness predictor \\(c(t)\\) TYPE: bool DEFAULT: False Source code in edsteva/viz/dashboards/predictor_dashboard/probe.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def probe_dashboard ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ): \"\"\"Script to be used by [``predictor_dashboard()``][edsteva.viz.dashboards.predictor_dashboard.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional show the number of visit instead of the completeness predictor $c(t)$ \"\"\" time = \"date\" _predictor = \"c\" alt . data_transformers . disable_max_rows () time_selection = alt . selection_interval ( encodings = [ \"x\" ]) time_line = ( alt . Chart ( predictor ) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 ), ), y = alt . Y ( \"mean(c):Q\" , title = \"c(t)\" , ), ) . add_selection ( time_selection ) ) . properties ( width = 900 , height = 50 ) care_site_selection = alt . selection_multi ( fields = [ \"care_site_short_name\" ]) care_site_color = alt . condition ( care_site_selection , alt . Color ( \"care_site_short_name:N\" , legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) predictor_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"care_site_short_name:N\" , title = \"Care site short name\" , sort = \"-x\" , ), color = care_site_color , ) . transform_filter ( time_selection ) . add_selection ( care_site_selection ) ) . properties ( width = 300 ) care_site_hist = predictor_hist . encode ( x = alt . X ( \"mean(c):Q\" , title = \"c\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), ) . properties ( title = \"Average completeness per care site\" ) index_variables_hists = [] index_variables_selections = [] for index_variable in index : index_variable_selection = alt . selection_multi ( fields = [ index_variable ]) index_variables_selections . append ( index_variable_selection ) index_variable_color = alt . condition ( index_variable_selection , alt . Color ( \" {} :N\" . format ( index_variable ), legend = None , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), alt . value ( \"lightgray\" ), ) index_variable_hist = ( alt . Chart ( predictor ) . mark_bar () . encode ( y = alt . Y ( \"mean(c):Q\" , title = \"c\" , ), x = alt . X ( \" {} :N\" . format ( index_variable ), title = index_variable , sort = \"-y\" , ), tooltip = alt . Tooltip ( \"mean(c):Q\" , format = \".2\" ), color = index_variable_color , ) . add_selection ( index_variable_selection ) . transform_filter ( time_selection ) . transform_filter ( care_site_selection ) ) . properties ( title = \"Average completeness per {} \" . format ( index_variable )) index_variables_hists . append ( index_variable_hist ) extra_predictors = predictor . columns . difference ( index + [ time ] + [ _predictor ] + [ \"care_site_short_name\" , \"care_site_id\" , \"c_hat\" ] ) extra_predictors_hists = [] for extra_predictor in extra_predictors : extra_predictor_hist = predictor_hist . encode ( x = alt . X ( \"sum( {} ):Q\" . format ( extra_predictor ), title = \" {} \" . format ( extra_predictor ), axis = alt . Axis ( format = \"s\" ), ), tooltip = alt . Tooltip ( \"sum( {} ):Q\" . format ( extra_predictor ), format = \",\" ), ) . properties ( title = \"Total {} per care site\" . format ( extra_predictor )) extra_predictors_hists . append ( extra_predictor_hist ) index . append ( \"care_site_short_name\" ) index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : \"stay_type\" }, ) probe_line = ( alt . Chart ( predictor ) . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), y = alt . Y ( \"sum(n_visit):Q\" if show_n_visit else \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, title = None , ), ) . add_selection ( index_selection ) . transform_filter ( index_selection ) . transform_filter ( care_site_selection ) . transform_filter ( time_selection ) ) . properties ( width = 900 , height = 300 ) for index_variable_selection in index_variables_selections : probe_line = probe_line . transform_filter ( index_variable_selection ) care_site_hist = care_site_hist . transform_filter ( index_variable_selection ) for idx in range ( len ( extra_predictors_hists )): extra_predictors_hists [ idx ] = extra_predictors_hists [ idx ] . transform_filter ( index_variable_selection ) for idx in range ( len ( index_variables_hists )): if idx != index_variables_selections . index ( index_variable_selection ): index_variables_hists [ idx ] = index_variables_hists [ idx ] . transform_filter ( index_variable_selection ) index_variables_hists = reduce ( lambda index_variable_hist_1 , index_variable_hist_2 : index_variable_hist_1 & index_variable_hist_2 , index_variables_hists , ) extra_predictors_hists = reduce ( lambda extra_predictor_hist_1 , extra_predictor_hist_2 : extra_predictor_hist_1 & extra_predictor_hist_2 , extra_predictors_hists , ) chart = ( alt . vconcat ( alt . vconcat ( probe_line , time_line , spacing = 130 ), ( index_variables_hists | care_site_hist | extra_predictors_hists ), spacing = 10 , ) ) . resolve_scale ( color = \"independent\" ) return chart","title":"probe_dashboard()"},{"location":"reference/viz/dashboards/predictor_dashboard/wrapper/","text":"edsteva.viz.dashboards.predictor_dashboard.wrapper predictor_dashboard predictor_dashboard ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , ) Displays an interactive chart with: On the top, the aggregated average completeness predictor \\(c(t)\\) over time \\(t\\) with the fitted model \\(\\hat{c}(t)\\) if specified. On the bottom, interactive filters including all the concepts in the Probe (such as time, care site, number of visits...etc.) Is is possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel , optional DEFAULT: None care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: None save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit show the number of visit instead of the completeness predictor \\(c(t)\\) TYPE: bool DEFAULT: False Source code in edsteva/viz/dashboards/predictor_dashboard/wrapper.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def predictor_dashboard ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , ): r \"\"\"Displays an interactive chart with: - On the top, the aggregated average completeness predictor $c(t)$ over time $t$ with the fitted model $\\hat{c}(t)$ if specified. - On the bottom, interactive filters including all the concepts in the [Probe][probe] (such as time, care site, number of visits...etc.) Is is possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$ fitted_model : BaseModel, optional Model fitted to the probe care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional show the number of visit instead of the completeness predictor $c(t)$ \"\"\" index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) if fitted_model : predictor = fitted_model . predict ( probe ) else : predictor = probe . predictor predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , ) if fitted_model : chart = fitted_probe_dashboard ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , ) else : chart = probe_dashboard ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , show_n_visit = show_n_visit , ) chart = chart . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ) vis_probe = \"id\" + uuid . uuid4 () . hex new_index_probe_id = \"id\" + uuid . uuid4 () . hex old_index_probe_id = \"id\" + uuid . uuid4 () . hex html_chart = f \"\"\" <!DOCTYPE html> <html> <head> <script src=\"https://cdn.jsdelivr.net/npm/vega@ { alt . VEGA_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@ { alt . VEGALITE_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@ { alt . VEGAEMBED_VERSION } \"></script> </head> <body> <div class=\"container\"> <div class=\"row\"> <div> <div id= { vis_probe } ></div> </div> <div style=\"position:absolute;left:45px;top:380px;width: -webkit-fill-available;\"> <div id= { new_index_probe_id } > <div id= { old_index_probe_id } ></div> </div> <hr/> <h1 style=\"text-align:center\"> Interactive filters </h1> </div> </div> </div> <script type=\"text/javascript\"> vegaEmbed('# { vis_probe } ', { chart . to_json ( indent = None ) } ).then(function(result) {{ const sliders = document.getElementsByClassName('vega-bindings'); const newparent = document.getElementById(' { new_index_probe_id } '); const oldchild = document.getElementById(' { old_index_probe_id } '); for (var i = 0; i < sliders.length; i++) {{ if (sliders[i].parentElement.parentElement.id == ' { vis_probe } ') {{ var index_slider = sliders[i] }} }} newparent.replaceChild(index_slider, oldchild); }} ).catch(console.error); </script> </body> </html> \"\"\" display ( HTML ( html_chart )) if save_path : save_html ( obj = html_chart , filename = save_path , )","title":"wrapper"},{"location":"reference/viz/dashboards/predictor_dashboard/wrapper/#edstevavizdashboardspredictor_dashboardwrapper","text":"","title":"edsteva.viz.dashboards.predictor_dashboard.wrapper"},{"location":"reference/viz/dashboards/predictor_dashboard/wrapper/#edsteva.viz.dashboards.predictor_dashboard.wrapper.predictor_dashboard","text":"predictor_dashboard ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , ) Displays an interactive chart with: On the top, the aggregated average completeness predictor \\(c(t)\\) over time \\(t\\) with the fitted model \\(\\hat{c}(t)\\) if specified. On the bottom, interactive filters including all the concepts in the Probe (such as time, care site, number of visits...etc.) Is is possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel , optional DEFAULT: None care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: None save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit show the number of visit instead of the completeness predictor \\(c(t)\\) TYPE: bool DEFAULT: False Source code in edsteva/viz/dashboards/predictor_dashboard/wrapper.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def predictor_dashboard ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , ): r \"\"\"Displays an interactive chart with: - On the top, the aggregated average completeness predictor $c(t)$ over time $t$ with the fitted model $\\hat{c}(t)$ if specified. - On the bottom, interactive filters including all the concepts in the [Probe][probe] (such as time, care site, number of visits...etc.) Is is possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$ fitted_model : BaseModel, optional Model fitted to the probe care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional show the number of visit instead of the completeness predictor $c(t)$ \"\"\" index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) if fitted_model : predictor = fitted_model . predict ( probe ) else : predictor = probe . predictor predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , ) if fitted_model : chart = fitted_probe_dashboard ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , ) else : chart = probe_dashboard ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , show_n_visit = show_n_visit , ) chart = chart . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ) vis_probe = \"id\" + uuid . uuid4 () . hex new_index_probe_id = \"id\" + uuid . uuid4 () . hex old_index_probe_id = \"id\" + uuid . uuid4 () . hex html_chart = f \"\"\" <!DOCTYPE html> <html> <head> <script src=\"https://cdn.jsdelivr.net/npm/vega@ { alt . VEGA_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@ { alt . VEGALITE_VERSION } \"></script> <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@ { alt . VEGAEMBED_VERSION } \"></script> </head> <body> <div class=\"container\"> <div class=\"row\"> <div> <div id= { vis_probe } ></div> </div> <div style=\"position:absolute;left:45px;top:380px;width: -webkit-fill-available;\"> <div id= { new_index_probe_id } > <div id= { old_index_probe_id } ></div> </div> <hr/> <h1 style=\"text-align:center\"> Interactive filters </h1> </div> </div> </div> <script type=\"text/javascript\"> vegaEmbed('# { vis_probe } ', { chart . to_json ( indent = None ) } ).then(function(result) {{ const sliders = document.getElementsByClassName('vega-bindings'); const newparent = document.getElementById(' { new_index_probe_id } '); const oldchild = document.getElementById(' { old_index_probe_id } '); for (var i = 0; i < sliders.length; i++) {{ if (sliders[i].parentElement.parentElement.id == ' { vis_probe } ') {{ var index_slider = sliders[i] }} }} newparent.replaceChild(index_slider, oldchild); }} ).catch(console.error); </script> </body> </html> \"\"\" display ( HTML ( html_chart )) if save_path : save_html ( obj = html_chart , filename = save_path , )","title":"predictor_dashboard()"},{"location":"reference/viz/plots/","text":"edsteva.viz.plots","title":"`edsteva.viz.plots`"},{"location":"reference/viz/plots/#edstevavizplots","text":"","title":"edsteva.viz.plots"},{"location":"reference/viz/plots/estimates_densities/","text":"edsteva.viz.plots.estimates_densities plot_estimates_densities plot_estimates_densities ( fitted_model : BaseModel , save_path : str = None , ** kwargs ) Displays the density plot with the associated box plot of each estimate and metric computed in the input model. It can help you to set the thresholds. PARAMETER DESCRIPTION fitted_model Model with estimates of interest EXAMPLE : StepFunction Model with \\((\\hat{t_0}, \\hat{c_0})\\) TYPE: BaseModel save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/viz/plots/estimates_densities.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def plot_estimates_densities ( fitted_model : BaseModel , save_path : str = None , ** kwargs , ): r \"\"\"Displays the density plot with the associated box plot of each estimate and metric computed in the input model. It can help you to set the thresholds. Parameters ---------- fitted_model : BaseModel Model with estimates of interest **EXAMPLE**: StepFunction Model with $(\\hat{t_0}, \\hat{c_0})$ save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" alt . data_transformers . disable_max_rows () estimates = fitted_model . estimates . copy () indexes = list ( estimates . columns . difference ( fitted_model . _coefs + fitted_model . _metrics ) ) indexes . remove ( \"care_site_id\" ) quantitative_estimates = [] time_estimates = [] for estimate in fitted_model . _coefs + fitted_model . _metrics : if estimates [ estimate ] . dtype == float or estimates [ estimate ] . dtype == int : max_value = estimates [ estimate ] . max () min_value = estimates [ estimate ] . min () estimates [ estimate ] = round ( estimates [ estimate ], 3 ) estimate_density = ( alt . vconcat ( ( ( alt . Chart ( estimates ) . transform_density ( estimate , as_ = [ estimate , \"Density\" ], groupby = indexes , extent = [ min_value , max_value ], ** kwargs , ) . mark_area () . encode ( x = alt . X ( \" {} :Q\" . format ( estimate ), title = None , ), y = \"Density:Q\" , ) ) + alt . Chart ( estimates ) . mark_rule ( color = \"red\" ) . encode ( x = \"median( {} ):Q\" . format ( estimate )) ) . properties ( width = 800 , height = 300 ), ( alt . Chart ( estimates ) . mark_tick () . encode ( x = alt . X ( \" {} :Q\" . format ( estimate ), axis = None )) ), spacing = 0 , ) & ( alt . Chart ( estimates ) . mark_boxplot () . encode ( x = \" {} :Q\" . format ( estimate ), ) ) ) . resolve_scale ( x = \"shared\" ) quantitative_estimates . append ( estimate_density ) else : estimates [ estimate ] = estimates [ estimate ] estimates [ estimate ] = estimates [ estimate ] . astype ( \"datetime64[s]\" ) estimate_density = ( ( alt . Chart ( estimates ) . transform_timeunit ( estimate = \"yearmonth( {} )\" . format ( estimate )) . mark_bar ( size = 10 ) . encode ( x = alt . X ( \" {} :T\" . format ( estimate ), axis = alt . Axis ( tickCount = \"month\" , format = \"%Y, %b\" , labelAngle =- 90 , ), title = estimate , ), y = alt . Y ( \"count( {} ):Q\" . format ( estimate ), axis = alt . Axis ( tickMinStep = 1 ), ), ) ) + alt . Chart ( estimates ) . mark_rule ( color = \"red\" ) . encode ( x = \"median( {} ):T\" . format ( estimate )) ) . properties ( width = 800 , height = 300 ) time_estimates . append ( estimate_density ) estimates_densities = time_estimates + quantitative_estimates estimates_densities = reduce ( lambda estimate_density_1 , estimate_density_2 : estimate_density_1 & estimate_density_2 , estimates_densities , ) for index in indexes : index_selection = alt . selection_single ( fields = [ index ], bind = alt . binding_select ( name = index , options = estimates [ index ] . unique ()), init = { index : estimates [ index ] . unique ()[ 0 ]}, ) estimates_densities = estimates_densities . add_selection ( index_selection ) estimates_densities = estimates_densities . transform_filter ( index_selection ) if save_path : save_html ( obj = estimates_densities . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ), filename = save_path , ) return estimates_densities","title":"estimates_densities"},{"location":"reference/viz/plots/estimates_densities/#edstevavizplotsestimates_densities","text":"","title":"edsteva.viz.plots.estimates_densities"},{"location":"reference/viz/plots/estimates_densities/#edsteva.viz.plots.estimates_densities.plot_estimates_densities","text":"plot_estimates_densities ( fitted_model : BaseModel , save_path : str = None , ** kwargs ) Displays the density plot with the associated box plot of each estimate and metric computed in the input model. It can help you to set the thresholds. PARAMETER DESCRIPTION fitted_model Model with estimates of interest EXAMPLE : StepFunction Model with \\((\\hat{t_0}, \\hat{c_0})\\) TYPE: BaseModel save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None Source code in edsteva/viz/plots/estimates_densities.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def plot_estimates_densities ( fitted_model : BaseModel , save_path : str = None , ** kwargs , ): r \"\"\"Displays the density plot with the associated box plot of each estimate and metric computed in the input model. It can help you to set the thresholds. Parameters ---------- fitted_model : BaseModel Model with estimates of interest **EXAMPLE**: StepFunction Model with $(\\hat{t_0}, \\hat{c_0})$ save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` \"\"\" alt . data_transformers . disable_max_rows () estimates = fitted_model . estimates . copy () indexes = list ( estimates . columns . difference ( fitted_model . _coefs + fitted_model . _metrics ) ) indexes . remove ( \"care_site_id\" ) quantitative_estimates = [] time_estimates = [] for estimate in fitted_model . _coefs + fitted_model . _metrics : if estimates [ estimate ] . dtype == float or estimates [ estimate ] . dtype == int : max_value = estimates [ estimate ] . max () min_value = estimates [ estimate ] . min () estimates [ estimate ] = round ( estimates [ estimate ], 3 ) estimate_density = ( alt . vconcat ( ( ( alt . Chart ( estimates ) . transform_density ( estimate , as_ = [ estimate , \"Density\" ], groupby = indexes , extent = [ min_value , max_value ], ** kwargs , ) . mark_area () . encode ( x = alt . X ( \" {} :Q\" . format ( estimate ), title = None , ), y = \"Density:Q\" , ) ) + alt . Chart ( estimates ) . mark_rule ( color = \"red\" ) . encode ( x = \"median( {} ):Q\" . format ( estimate )) ) . properties ( width = 800 , height = 300 ), ( alt . Chart ( estimates ) . mark_tick () . encode ( x = alt . X ( \" {} :Q\" . format ( estimate ), axis = None )) ), spacing = 0 , ) & ( alt . Chart ( estimates ) . mark_boxplot () . encode ( x = \" {} :Q\" . format ( estimate ), ) ) ) . resolve_scale ( x = \"shared\" ) quantitative_estimates . append ( estimate_density ) else : estimates [ estimate ] = estimates [ estimate ] estimates [ estimate ] = estimates [ estimate ] . astype ( \"datetime64[s]\" ) estimate_density = ( ( alt . Chart ( estimates ) . transform_timeunit ( estimate = \"yearmonth( {} )\" . format ( estimate )) . mark_bar ( size = 10 ) . encode ( x = alt . X ( \" {} :T\" . format ( estimate ), axis = alt . Axis ( tickCount = \"month\" , format = \"%Y, %b\" , labelAngle =- 90 , ), title = estimate , ), y = alt . Y ( \"count( {} ):Q\" . format ( estimate ), axis = alt . Axis ( tickMinStep = 1 ), ), ) ) + alt . Chart ( estimates ) . mark_rule ( color = \"red\" ) . encode ( x = \"median( {} ):T\" . format ( estimate )) ) . properties ( width = 800 , height = 300 ) time_estimates . append ( estimate_density ) estimates_densities = time_estimates + quantitative_estimates estimates_densities = reduce ( lambda estimate_density_1 , estimate_density_2 : estimate_density_1 & estimate_density_2 , estimates_densities , ) for index in indexes : index_selection = alt . selection_single ( fields = [ index ], bind = alt . binding_select ( name = index , options = estimates [ index ] . unique ()), init = { index : estimates [ index ] . unique ()[ 0 ]}, ) estimates_densities = estimates_densities . add_selection ( index_selection ) estimates_densities = estimates_densities . transform_filter ( index_selection ) if save_path : save_html ( obj = estimates_densities . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ), filename = save_path , ) return estimates_densities","title":"plot_estimates_densities()"},{"location":"reference/viz/plots/normalized_probe/","text":"edsteva.viz.plots.normalized_probe plot_normalized_probe plot_normalized_probe ( probe : BaseProbe , fitted_model : BaseModel , t_0_max : datetime = None , t_1_min : datetime = None , error_max : float = None , c_0_min : float = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : Union [ datetime , str ] = None , end_date : Union [ datetime , str ] = None , care_site_short_name : List [ int ] = None , t_min : int = None , t_max : int = None , save_path : str = None , x_axis_title : str = \"\u0394t = (t - t\u2080) months\" , y_axis_title : str = \"c(\u0394t) / c\u2080\" , show_per_care_site : bool = False , ** kwargs ) Displays a chart with the aggregated normalized completeness predictor \\(\\frac{c(\\Delta t)}{c_0}\\) over normalized time \\(\\Delta t = t - t_0\\) . It represents the overall deviation from the Model. Is is possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel t_0_max Initial value for the \\(t_0\\) threshold. If None, it will be set as the maximum possible \\(t_0\\) value. EXAMPLE : \"2022-01\" , datetime(2020, 2, 1) TYPE: datetime , optional DEFAULT: None error_max Initial value for the \\(error\\) threshold. If None, it will be set as the maximum possible error value. EXAMPLE : 0.02 , 0.25 TYPE: float , optional DEFAULT: None c_0_min Initial value for the \\(c_0\\) threshold. If None, it will be set as 0. EXAMPLE : 0.1 , 0.8 TYPE: float , optional DEFAULT: None stay_type EXAMPLE : \"All\" or [\"All\", \"Urg\"] TYPE: List [ str ], optional DEFAULT: None care_site_id EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: None stay_type EXAMPLE : \"All\" or [\"All\", \"Urg\"] TYPE: List [ str ], optional DEFAULT: None care_site_id EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_short_name EXAMPLE : \"HOSPITAL XXXX\" TYPE: List [ int ], optional DEFAULT: None save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None x_axis_title Label name for the x axis TYPE: str DEFAULT: '\u0394t = (t - t\u2080) months' y_axis_title Label name for the y axis TYPE: str DEFAULT: 'c(\u0394t) / c\u2080' show_per_care_site If True, the average completeness predictor \\(c(t)\\) is computed for each care site independently. If False, it is computed over all care sites. TYPE: bool DEFAULT: False Source code in edsteva/viz/plots/normalized_probe.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def plot_normalized_probe ( probe : BaseProbe , fitted_model : BaseModel , t_0_max : datetime = None , t_1_min : datetime = None , error_max : float = None , c_0_min : float = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : Union [ datetime , str ] = None , end_date : Union [ datetime , str ] = None , care_site_short_name : List [ int ] = None , t_min : int = None , t_max : int = None , save_path : str = None , x_axis_title : str = \"\u0394t = (t - t\u2080) months\" , y_axis_title : str = \"c(\u0394t) / c\u2080\" , show_per_care_site : bool = False , ** kwargs , ): r \"\"\"Displays a chart with the aggregated normalized completeness predictor $\\frac{c(\\Delta t)}{c_0}$ over normalized time $\\Delta t = t - t_0$. It represents the overall deviation from the Model. Is is possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$ fitted_model : BaseModel Model fitted to the probe t_0_max : datetime, optional Initial value for the $t_0$ threshold. If None, it will be set as the maximum possible $t_0$ value. **EXAMPLE**: `\"2022-01\"`, `datetime(2020, 2, 1)` error_max : float, optional Initial value for the $error$ threshold. If None, it will be set as the maximum possible error value. **EXAMPLE**: `0.02`, `0.25` c_0_min : float, optional Initial value for the $c_0$ threshold. If None, it will be set as 0. **EXAMPLE**: `0.1`, `0.8` stay_type : List[str], optional **EXAMPLE**: `\"All\"` or `[\"All\", \"Urg\"]` care_site_id : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` stay_type : List[str], optional **EXAMPLE**: `\"All\"` or `[\"All\", \"Urg\"]` care_site_id : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_short_name : List[int], optional **EXAMPLE**: `\"HOSPITAL XXXX\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` x_axis_title: str, optional, Label name for the x axis y_axis_title: str, optional, Label name for the y axis show_per_care_site: bool, optional If True, the average completeness predictor $c(t)$ is computed for each care site independently. If False, it is computed over all care sites. \"\"\" predictor = probe . predictor . copy () estimates = fitted_model . estimates . copy () predictor = predictor . merge ( estimates , on = probe . _index ) def month_diff ( x , y ): end = x . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) start = y . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) return end - start predictor [ \"date\" ] = predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"normalized_date\" ] = month_diff ( predictor [ \"date\" ], predictor [ \"t_0\" ]) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( str ) predictor [ \"normalized_date\" ] = predictor [ \"normalized_date\" ] . astype ( int ) if t_min : predictor = predictor [ predictor . normalized_date >= t_min ] if t_max : predictor = predictor [ predictor . normalized_date <= t_max ] predictor [ \"normalized_c\" ] = predictor [ \"c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] == 0 ), 1 ) predictor [ \"normalized_c\" ] = predictor [ \"normalized_c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] > 0 ), predictor [ \"c\" ] / predictor [ \"c_0\" ], ) predictor [ \"legend_error_band\" ] = \"Standard deviation\" predictor [ \"legend_model\" ] = type ( fitted_model ) . __name__ predictor [ \"model\" ] = predictor [ \"c_0\" ] . where ( predictor [ \"normalized_date\" ] < 0 , 1 ) predictor [ \"model\" ] = predictor [ \"model\" ] . where ( predictor [ \"normalized_date\" ] >= 0 , 0 ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : predictor = predictor [ predictor . date < predictor . t_1 ] index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) if show_per_care_site : index = index + [ \"care_site_short_name\" ] predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , stay_type = stay_type , care_site_id = care_site_id , care_site_short_name = care_site_short_name , start_date = start_date , end_date = end_date , ** kwargs , ) index = [ variable for variable in index if len ( predictor [ variable ] . unique ()) >= 2 ] if c_0_min : c_0_min = round_it ( float ( c_0_min ), 2 ) if c_0_min > round_it ( predictor . c_0 . max (), 2 ): c_0_min = round_it ( predictor . c_0 . max (), 2 ) elif c_0_min < round_it ( predictor . c_0 . min (), 2 ): c_0_min = round_it ( predictor . c_0 . min (), 2 ) else : c_0_min = round_it ( predictor . c_0 . min (), 2 ) if error_max : error_max = round_it ( float ( error_max ), 2 ) if error_max > predictor . error . max (): error_max = round_it ( predictor . error . max (), 2 ) elif error_max < round_it ( predictor . error . min (), 2 ): error_max = round_it ( predictor . error . min (), 2 ) else : error_max = round_it ( predictor . error . max (), 2 ) t_0_max = ( str ( pd . to_datetime ( t_0_max )) if t_0_max else predictor . t_0 . astype ( str ) . max () ) c_0_min_slider = alt . binding_range ( min = 0 , max = round_it ( predictor . c_0 . max (), 2 ), step = 1 / 100 , name = \"c\u2080 min: \" , ) c_0_min_selection = alt . selection_single ( name = \"c_0_min\" , fields = [ \"c_0_min\" ], bind = c_0_min_slider , init = { \"c_0_min\" : c_0_min }, ) t_0_slider = alt . binding ( input = \"t_0\" , name = \"t\u2080 max: \" , ) t_0_selection = alt . selection_single ( name = \"t_0\" , fields = [ \"t_0\" ], bind = t_0_slider , init = { \"t_0\" : t_0_max }, ) if fitted_model . name == \"RectangleFunction\" : t_1_min = ( str ( pd . to_datetime ( t_1_min )) if t_1_min else predictor . t_1 . astype ( str ) . min () ) t_1_slider = alt . binding ( input = \"t_1\" , name = \"t\u2081 min: \" , ) t_1_selection = alt . selection_single ( name = \"t_1\" , fields = [ \"t_1\" ], bind = t_1_slider , init = { \"t_1\" : t_1_min }, ) error_max_slider = alt . binding_range ( min = round_it ( predictor . error . min (), 2 ), max = round_it ( predictor . error . max (), 2 ), step = scale_it ( predictor . error . max ()) / 100 , name = \"error max: \" , ) error_max_selection = alt . selection_single ( name = \"error_max\" , fields = [ \"error_max\" ], bind = error_max_slider , init = { \"error_max\" : error_max }, ) alt . data_transformers . disable_max_rows () base_chart = ( alt . Chart ( predictor ) . encode ( x = alt . X ( \"normalized_date:Q\" , title = x_axis_title , scale = alt . Scale ( nice = False ), ), ) ) . properties ( width = 900 , height = 300 ) transform_chart = ( base_chart . transform_joinaggregate ( mean_c_0 = \"mean(c_0)\" , mean_error = \"mean(error)\" , groupby = [ \"care_site_short_name\" ] + index , ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : transform_chart = transform_chart . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) mean_line = transform_chart . mark_line () . encode ( y = alt . Y ( \"mean(normalized_c):Q\" , ) ) error_line = transform_chart . mark_errorband ( extent = \"stdev\" ) . encode ( y = alt . Y ( \"normalized_c:Q\" , title = y_axis_title , ), stroke = alt . Stroke ( \"legend_error_band\" , title = \"Error band\" , legend = alt . Legend ( symbolType = \"square\" , orient = \"top\" ), ), ) model_line = base_chart . mark_line ( color = \"black\" , interpolate = \"step-after\" , strokeDash = [ 5 , 5 ] ) . encode ( y = \"model:Q\" , strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"Model line\" , legend = alt . Legend ( orient = \"top\" , symbolDash = [ 2 , 2 ]), ), ) chart = mean_line + error_line + model_line if len ( index ) >= 2 : index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : index [ 0 ]}, ) chart = ( chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( index_selection ) . add_selection ( index_selection ) ) elif len ( index ) == 1 : chart = chart . encode ( color = alt . Color ( \" {} :N\" . format ( index [ 0 ]), sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, ), ) chart = ( chart . add_selection ( error_max_selection ) . add_selection ( c_0_min_selection ) . add_selection ( t_0_selection ) ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : chart = chart . add_selection ( t_1_selection ) if save_path : save_html ( obj = chart . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ), filename = save_path , ) return chart","title":"normalized_probe"},{"location":"reference/viz/plots/normalized_probe/#edstevavizplotsnormalized_probe","text":"","title":"edsteva.viz.plots.normalized_probe"},{"location":"reference/viz/plots/normalized_probe/#edsteva.viz.plots.normalized_probe.plot_normalized_probe","text":"plot_normalized_probe ( probe : BaseProbe , fitted_model : BaseModel , t_0_max : datetime = None , t_1_min : datetime = None , error_max : float = None , c_0_min : float = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : Union [ datetime , str ] = None , end_date : Union [ datetime , str ] = None , care_site_short_name : List [ int ] = None , t_min : int = None , t_max : int = None , save_path : str = None , x_axis_title : str = \"\u0394t = (t - t\u2080) months\" , y_axis_title : str = \"c(\u0394t) / c\u2080\" , show_per_care_site : bool = False , ** kwargs ) Displays a chart with the aggregated normalized completeness predictor \\(\\frac{c(\\Delta t)}{c_0}\\) over normalized time \\(\\Delta t = t - t_0\\) . It represents the overall deviation from the Model. Is is possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel t_0_max Initial value for the \\(t_0\\) threshold. If None, it will be set as the maximum possible \\(t_0\\) value. EXAMPLE : \"2022-01\" , datetime(2020, 2, 1) TYPE: datetime , optional DEFAULT: None error_max Initial value for the \\(error\\) threshold. If None, it will be set as the maximum possible error value. EXAMPLE : 0.02 , 0.25 TYPE: float , optional DEFAULT: None c_0_min Initial value for the \\(c_0\\) threshold. If None, it will be set as 0. EXAMPLE : 0.1 , 0.8 TYPE: float , optional DEFAULT: None stay_type EXAMPLE : \"All\" or [\"All\", \"Urg\"] TYPE: List [ str ], optional DEFAULT: None care_site_id EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: None stay_type EXAMPLE : \"All\" or [\"All\", \"Urg\"] TYPE: List [ str ], optional DEFAULT: None care_site_id EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_short_name EXAMPLE : \"HOSPITAL XXXX\" TYPE: List [ int ], optional DEFAULT: None save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None x_axis_title Label name for the x axis TYPE: str DEFAULT: '\u0394t = (t - t\u2080) months' y_axis_title Label name for the y axis TYPE: str DEFAULT: 'c(\u0394t) / c\u2080' show_per_care_site If True, the average completeness predictor \\(c(t)\\) is computed for each care site independently. If False, it is computed over all care sites. TYPE: bool DEFAULT: False Source code in edsteva/viz/plots/normalized_probe.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def plot_normalized_probe ( probe : BaseProbe , fitted_model : BaseModel , t_0_max : datetime = None , t_1_min : datetime = None , error_max : float = None , c_0_min : float = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : Union [ datetime , str ] = None , end_date : Union [ datetime , str ] = None , care_site_short_name : List [ int ] = None , t_min : int = None , t_max : int = None , save_path : str = None , x_axis_title : str = \"\u0394t = (t - t\u2080) months\" , y_axis_title : str = \"c(\u0394t) / c\u2080\" , show_per_care_site : bool = False , ** kwargs , ): r \"\"\"Displays a chart with the aggregated normalized completeness predictor $\\frac{c(\\Delta t)}{c_0}$ over normalized time $\\Delta t = t - t_0$. It represents the overall deviation from the Model. Is is possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$ fitted_model : BaseModel Model fitted to the probe t_0_max : datetime, optional Initial value for the $t_0$ threshold. If None, it will be set as the maximum possible $t_0$ value. **EXAMPLE**: `\"2022-01\"`, `datetime(2020, 2, 1)` error_max : float, optional Initial value for the $error$ threshold. If None, it will be set as the maximum possible error value. **EXAMPLE**: `0.02`, `0.25` c_0_min : float, optional Initial value for the $c_0$ threshold. If None, it will be set as 0. **EXAMPLE**: `0.1`, `0.8` stay_type : List[str], optional **EXAMPLE**: `\"All\"` or `[\"All\", \"Urg\"]` care_site_id : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` stay_type : List[str], optional **EXAMPLE**: `\"All\"` or `[\"All\", \"Urg\"]` care_site_id : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_short_name : List[int], optional **EXAMPLE**: `\"HOSPITAL XXXX\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` x_axis_title: str, optional, Label name for the x axis y_axis_title: str, optional, Label name for the y axis show_per_care_site: bool, optional If True, the average completeness predictor $c(t)$ is computed for each care site independently. If False, it is computed over all care sites. \"\"\" predictor = probe . predictor . copy () estimates = fitted_model . estimates . copy () predictor = predictor . merge ( estimates , on = probe . _index ) def month_diff ( x , y ): end = x . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) start = y . dt . to_period ( \"M\" ) . view ( dtype = \"int64\" ) return end - start predictor [ \"date\" ] = predictor [ \"date\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( \"datetime64[ns]\" ) predictor [ \"normalized_date\" ] = month_diff ( predictor [ \"date\" ], predictor [ \"t_0\" ]) predictor [ \"t_0\" ] = predictor [ \"t_0\" ] . astype ( str ) predictor [ \"normalized_date\" ] = predictor [ \"normalized_date\" ] . astype ( int ) if t_min : predictor = predictor [ predictor . normalized_date >= t_min ] if t_max : predictor = predictor [ predictor . normalized_date <= t_max ] predictor [ \"normalized_c\" ] = predictor [ \"c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] == 0 ), 1 ) predictor [ \"normalized_c\" ] = predictor [ \"normalized_c\" ] . mask ( ( predictor [ \"normalized_date\" ] >= 0 ) & ( predictor [ \"c_0\" ] > 0 ), predictor [ \"c\" ] / predictor [ \"c_0\" ], ) predictor [ \"legend_error_band\" ] = \"Standard deviation\" predictor [ \"legend_model\" ] = type ( fitted_model ) . __name__ predictor [ \"model\" ] = predictor [ \"c_0\" ] . where ( predictor [ \"normalized_date\" ] < 0 , 1 ) predictor [ \"model\" ] = predictor [ \"model\" ] . where ( predictor [ \"normalized_date\" ] >= 0 , 0 ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : predictor = predictor [ predictor . date < predictor . t_1 ] index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) if show_per_care_site : index = index + [ \"care_site_short_name\" ] predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , stay_type = stay_type , care_site_id = care_site_id , care_site_short_name = care_site_short_name , start_date = start_date , end_date = end_date , ** kwargs , ) index = [ variable for variable in index if len ( predictor [ variable ] . unique ()) >= 2 ] if c_0_min : c_0_min = round_it ( float ( c_0_min ), 2 ) if c_0_min > round_it ( predictor . c_0 . max (), 2 ): c_0_min = round_it ( predictor . c_0 . max (), 2 ) elif c_0_min < round_it ( predictor . c_0 . min (), 2 ): c_0_min = round_it ( predictor . c_0 . min (), 2 ) else : c_0_min = round_it ( predictor . c_0 . min (), 2 ) if error_max : error_max = round_it ( float ( error_max ), 2 ) if error_max > predictor . error . max (): error_max = round_it ( predictor . error . max (), 2 ) elif error_max < round_it ( predictor . error . min (), 2 ): error_max = round_it ( predictor . error . min (), 2 ) else : error_max = round_it ( predictor . error . max (), 2 ) t_0_max = ( str ( pd . to_datetime ( t_0_max )) if t_0_max else predictor . t_0 . astype ( str ) . max () ) c_0_min_slider = alt . binding_range ( min = 0 , max = round_it ( predictor . c_0 . max (), 2 ), step = 1 / 100 , name = \"c\u2080 min: \" , ) c_0_min_selection = alt . selection_single ( name = \"c_0_min\" , fields = [ \"c_0_min\" ], bind = c_0_min_slider , init = { \"c_0_min\" : c_0_min }, ) t_0_slider = alt . binding ( input = \"t_0\" , name = \"t\u2080 max: \" , ) t_0_selection = alt . selection_single ( name = \"t_0\" , fields = [ \"t_0\" ], bind = t_0_slider , init = { \"t_0\" : t_0_max }, ) if fitted_model . name == \"RectangleFunction\" : t_1_min = ( str ( pd . to_datetime ( t_1_min )) if t_1_min else predictor . t_1 . astype ( str ) . min () ) t_1_slider = alt . binding ( input = \"t_1\" , name = \"t\u2081 min: \" , ) t_1_selection = alt . selection_single ( name = \"t_1\" , fields = [ \"t_1\" ], bind = t_1_slider , init = { \"t_1\" : t_1_min }, ) error_max_slider = alt . binding_range ( min = round_it ( predictor . error . min (), 2 ), max = round_it ( predictor . error . max (), 2 ), step = scale_it ( predictor . error . max ()) / 100 , name = \"error max: \" , ) error_max_selection = alt . selection_single ( name = \"error_max\" , fields = [ \"error_max\" ], bind = error_max_slider , init = { \"error_max\" : error_max }, ) alt . data_transformers . disable_max_rows () base_chart = ( alt . Chart ( predictor ) . encode ( x = alt . X ( \"normalized_date:Q\" , title = x_axis_title , scale = alt . Scale ( nice = False ), ), ) ) . properties ( width = 900 , height = 300 ) transform_chart = ( base_chart . transform_joinaggregate ( mean_c_0 = \"mean(c_0)\" , mean_error = \"mean(error)\" , groupby = [ \"care_site_short_name\" ] + index , ) . transform_filter ( alt . datum . mean_c_0 >= c_0_min_selection . c_0_min ) . transform_filter ( alt . datum . mean_error <= error_max_selection . error_max ) . transform_filter ( alt . datum . t_0 <= t_0_selection . t_0 ) ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : transform_chart = transform_chart . transform_filter ( alt . datum . t_1 >= t_1_selection . t_1 ) mean_line = transform_chart . mark_line () . encode ( y = alt . Y ( \"mean(normalized_c):Q\" , ) ) error_line = transform_chart . mark_errorband ( extent = \"stdev\" ) . encode ( y = alt . Y ( \"normalized_c:Q\" , title = y_axis_title , ), stroke = alt . Stroke ( \"legend_error_band\" , title = \"Error band\" , legend = alt . Legend ( symbolType = \"square\" , orient = \"top\" ), ), ) model_line = base_chart . mark_line ( color = \"black\" , interpolate = \"step-after\" , strokeDash = [ 5 , 5 ] ) . encode ( y = \"model:Q\" , strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"Model line\" , legend = alt . Legend ( orient = \"top\" , symbolDash = [ 2 , 2 ]), ), ) chart = mean_line + error_line + model_line if len ( index ) >= 2 : index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : index [ 0 ]}, ) chart = ( chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( index_selection ) . add_selection ( index_selection ) ) elif len ( index ) == 1 : chart = chart . encode ( color = alt . Color ( \" {} :N\" . format ( index [ 0 ]), sort = { \"field\" : \"c_0\" , \"op\" : \"min\" , \"order\" : \"descending\" }, ), ) chart = ( chart . add_selection ( error_max_selection ) . add_selection ( c_0_min_selection ) . add_selection ( t_0_selection ) ) # RectangleModel if fitted_model . name == \"RectangleFunction\" : chart = chart . add_selection ( t_1_selection ) if save_path : save_html ( obj = chart . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ), filename = save_path , ) return chart","title":"plot_normalized_probe()"},{"location":"reference/viz/plots/plot_probe/","text":"edsteva.viz.plots.plot_probe","title":"`edsteva.viz.plots.plot_probe`"},{"location":"reference/viz/plots/plot_probe/#edstevavizplotsplot_probe","text":"","title":"edsteva.viz.plots.plot_probe"},{"location":"reference/viz/plots/plot_probe/fitted_probe/","text":"edsteva.viz.plots.plot_probe.fitted_probe fitted_probe_line fitted_probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ) Script to be used by plot_probe() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe with its prediction \\(\\hat{c}(t)\\) TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True Source code in edsteva/viz/plots/plot_probe/fitted_probe.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def fitted_probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ): r \"\"\"Script to be used by [``plot_probe()``][edsteva.viz.plots.plot_probe.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe with its prediction $\\hat{c}(t)$ index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. \"\"\" predictor [ \"legend_predictor\" ] = \"Predictor c(t)\" predictor [ \"legend_model\" ] = \"Model f(t)\" base_chart = ( alt . Chart ( predictor ) . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), ) ) . properties ( width = 800 , height = 300 ) probe_line = base_chart . mark_line () . encode ( y = alt . Y ( \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), strokeDash = alt . StrokeDash ( \"legend_predictor\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , orient = \"left\" , ), ), ) model_line = base_chart . mark_line ( interpolate = \"step-after\" , strokeDash = [ 5 , 5 ] ) . encode ( y = alt . Y ( \"mean(c_hat):Q\" , ), strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , symbolDash = [ 2 , 2 ], orient = \"left\" , ), ), ) chart = probe_line + model_line if len ( index ) >= 2 : index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : index [ 0 ]}, ) chart = ( chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( index_selection ) . add_selection ( index_selection ) ) elif len ( index ) == 1 : chart = chart . encode ( color = alt . Color ( \" {} :N\" . format ( index [ 0 ]), sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), ) return chart","title":"fitted_probe"},{"location":"reference/viz/plots/plot_probe/fitted_probe/#edstevavizplotsplot_probefitted_probe","text":"","title":"edsteva.viz.plots.plot_probe.fitted_probe"},{"location":"reference/viz/plots/plot_probe/fitted_probe/#edsteva.viz.plots.plot_probe.fitted_probe.fitted_probe_line","text":"fitted_probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ) Script to be used by plot_probe() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe with its prediction \\(\\hat{c}(t)\\) TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True Source code in edsteva/viz/plots/plot_probe/fitted_probe.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def fitted_probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , ): r \"\"\"Script to be used by [``plot_probe()``][edsteva.viz.plots.plot_probe.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe with its prediction $\\hat{c}(t)$ index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. \"\"\" predictor [ \"legend_predictor\" ] = \"Predictor c(t)\" predictor [ \"legend_model\" ] = \"Model f(t)\" base_chart = ( alt . Chart ( predictor ) . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), ) ) . properties ( width = 800 , height = 300 ) probe_line = base_chart . mark_line () . encode ( y = alt . Y ( \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), strokeDash = alt . StrokeDash ( \"legend_predictor\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , orient = \"left\" , ), ), ) model_line = base_chart . mark_line ( interpolate = \"step-after\" , strokeDash = [ 5 , 5 ] ) . encode ( y = alt . Y ( \"mean(c_hat):Q\" , ), strokeWidth = alt . StrokeWidth ( \"legend_model\" , title = \"\" , legend = alt . Legend ( symbolType = \"stroke\" , symbolStrokeColor = \"steelblue\" , labelFontSize = 11 , labelFontStyle = \"bold\" , symbolDash = [ 2 , 2 ], orient = \"left\" , ), ), ) chart = probe_line + model_line if len ( index ) >= 2 : index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : index [ 0 ]}, ) chart = ( chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, title = None , ), ) . transform_filter ( index_selection ) . add_selection ( index_selection ) ) elif len ( index ) == 1 : chart = chart . encode ( color = alt . Color ( \" {} :N\" . format ( index [ 0 ]), sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), ) return chart","title":"fitted_probe_line()"},{"location":"reference/viz/plots/plot_probe/probe/","text":"edsteva.viz.plots.plot_probe.probe probe_line probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ) Script to be used by plot_probe() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit show the number of visit instead of the completeness predictor \\(c(t)\\) TYPE: bool DEFAULT: False Source code in edsteva/viz/plots/plot_probe/probe.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ): \"\"\"Script to be used by [``plot_probe()``][edsteva.viz.plots.plot_probe.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional show the number of visit instead of the completeness predictor $c(t)$ \"\"\" chart = ( alt . Chart ( predictor ) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), y = alt . Y ( \"sum(n_visit):Q\" if show_n_visit else \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), ) ) . properties ( width = 800 , height = 300 ) if len ( index ) >= 2 : index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : index [ 0 ]}, ) chart = ( chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), ) . add_selection ( index_selection ) . transform_filter ( index_selection ) ) . properties ( width = 800 , height = 300 ) elif len ( index ) == 1 : chart = ( chart . encode ( color = alt . Color ( \" {} :N\" . format ( index [ 0 ]), sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), ) ) . properties ( width = 800 , height = 300 ) return chart","title":"probe"},{"location":"reference/viz/plots/plot_probe/probe/#edstevavizplotsplot_probeprobe","text":"","title":"edsteva.viz.plots.plot_probe.probe"},{"location":"reference/viz/plots/plot_probe/probe/#edsteva.viz.plots.plot_probe.probe.probe_line","text":"probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ) Script to be used by plot_probe() PARAMETER DESCRIPTION predictor \\(c(t)\\) computed in the Probe TYPE: pd . DataFrame index Variable from which data is grouped TYPE: List [ str ] x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit show the number of visit instead of the completeness predictor \\(c(t)\\) TYPE: bool DEFAULT: False Source code in edsteva/viz/plots/plot_probe/probe.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def probe_line ( predictor : pd . DataFrame , index : List [ str ], x_axis_title : str = \"Time (Month Year)\" , y_axis_title : str = \"Completeness predictor c(t)\" , x_grid : bool = True , y_grid : bool = True , show_n_visit : bool = False , ): \"\"\"Script to be used by [``plot_probe()``][edsteva.viz.plots.plot_probe.wrapper] Parameters ---------- predictor : pd.DataFrame $c(t)$ computed in the Probe index : List[str] Variable from which data is grouped x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional show the number of visit instead of the completeness predictor $c(t)$ \"\"\" chart = ( alt . Chart ( predictor ) . mark_line () . encode ( x = alt . X ( f 'yearmonth( { \"date\" } ):T' , title = x_axis_title , axis = alt . Axis ( tickCount = \"month\" , labelAngle =- 90 , grid = x_grid ), ), y = alt . Y ( \"sum(n_visit):Q\" if show_n_visit else \"mean(c):Q\" , title = y_axis_title , axis = alt . Axis ( grid = y_grid ), ), ) ) . properties ( width = 800 , height = 300 ) if len ( index ) >= 2 : index_selection = alt . selection_single ( fields = [ \"index\" ], bind = alt . binding_radio ( name = \"Plot average completeness per: \" , options = index ), init = { \"index\" : index [ 0 ]}, ) chart = ( chart . transform_fold ( index , as_ = [ \"index\" , \"value\" ]) . encode ( color = alt . Color ( \"value:N\" , sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), ) . add_selection ( index_selection ) . transform_filter ( index_selection ) ) . properties ( width = 800 , height = 300 ) elif len ( index ) == 1 : chart = ( chart . encode ( color = alt . Color ( \" {} :N\" . format ( index [ 0 ]), sort = { \"field\" : \"c\" , \"op\" : \"mean\" , \"order\" : \"descending\" }, ), ) ) . properties ( width = 800 , height = 300 ) return chart","title":"probe_line()"},{"location":"reference/viz/plots/plot_probe/wrapper/","text":"edsteva.viz.plots.plot_probe.wrapper plot_probe plot_probe ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : datetime = None , end_date : datetime = None , care_site_short_name : List [ int ] = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , show_per_care_site : bool = True , ** kwargs ) Displays a chart with the average completeness predictor \\(c(t)\\) over time \\(t\\) with the fitted model \\(\\hat{c}(t)\\) if specified. The chart is exportable in png or svg format and easy to integrate into a report. Is also possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) . TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel , optional DEFAULT: None care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: None stay_type EXAMPLE : \"All\" or [\"All\", \"Urg\"] TYPE: List [ str ], optional DEFAULT: None care_site_id EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_short_name EXAMPLE : \"HOSPITAL XXXX\" TYPE: List [ int ], optional DEFAULT: None save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit If True, compute the sum of the number of visit instead of the mean of the completeness predictor \\(c(t)\\) . TYPE: bool DEFAULT: False show_per_care_site If True, the average completeness predictor \\(c(t)\\) is computed for each care site independently. If False, it is computed over all care sites. TYPE: bool DEFAULT: True Source code in edsteva/viz/plots/plot_probe/wrapper.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def plot_probe ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : datetime = None , end_date : datetime = None , care_site_short_name : List [ int ] = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , show_per_care_site : bool = True , ** kwargs , ): r \"\"\" Displays a chart with the average completeness predictor $c(t)$ over time $t$ with the fitted model $\\hat{c}(t)$ if specified. The chart is exportable in png or svg format and easy to integrate into a report. Is also possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$. fitted_model : BaseModel, optional Model fitted to the probe care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` stay_type : List[str], optional **EXAMPLE**: `\"All\"` or `[\"All\", \"Urg\"]` care_site_id : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_short_name : List[int], optional **EXAMPLE**: `\"HOSPITAL XXXX\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional If True, compute the sum of the number of visit instead of the mean of the completeness predictor $c(t)$. show_per_care_site: bool, optional If True, the average completeness predictor $c(t)$ is computed for each care site independently. If False, it is computed over all care sites. \"\"\" index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) if show_per_care_site : index = index + [ \"care_site_short_name\" ] if fitted_model : predictor = fitted_model . predict ( probe ) else : predictor = probe . predictor predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , stay_type = stay_type , care_site_id = care_site_id , care_site_short_name = care_site_short_name , start_date = start_date , end_date = end_date , ** kwargs , ) index = [ variable for variable in index if len ( predictor [ variable ] . unique ()) >= 2 ] alt . data_transformers . disable_max_rows () if fitted_model : chart = fitted_probe_line ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , ) else : chart = probe_line ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , show_n_visit = show_n_visit , ) if save_path : save_html ( obj = chart . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ), filename = save_path , ) return chart","title":"wrapper"},{"location":"reference/viz/plots/plot_probe/wrapper/#edstevavizplotsplot_probewrapper","text":"","title":"edsteva.viz.plots.plot_probe.wrapper"},{"location":"reference/viz/plots/plot_probe/wrapper/#edsteva.viz.plots.plot_probe.wrapper.plot_probe","text":"plot_probe ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : datetime = None , end_date : datetime = None , care_site_short_name : List [ int ] = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , show_per_care_site : bool = True , ** kwargs ) Displays a chart with the average completeness predictor \\(c(t)\\) over time \\(t\\) with the fitted model \\(\\hat{c}(t)\\) if specified. The chart is exportable in png or svg format and easy to integrate into a report. Is also possible to save the chart in HTML with the \"save_path\" optional input. PARAMETER DESCRIPTION probe Class describing the completeness predictor \\(c(t)\\) . TYPE: BaseProbe fitted_model Model fitted to the probe TYPE: BaseModel , optional DEFAULT: None care_site_level EXAMPLE : \"Hospital\" , \"H\u00f4pital\" or \"UF\" TYPE: str , optional DEFAULT: None stay_type EXAMPLE : \"All\" or [\"All\", \"Urg\"] TYPE: List [ str ], optional DEFAULT: None care_site_id EXAMPLE : [8312056386, 8312027648] TYPE: List [ int ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2021-07-01\" TYPE: datetime , optional DEFAULT: None care_site_short_name EXAMPLE : \"HOSPITAL XXXX\" TYPE: List [ int ], optional DEFAULT: None save_path Folder path where to save the chart in HTML format. EXAMPLE : \"my_folder/my_file.html\" TYPE: str , optional DEFAULT: None x_axis_title Label name for the x axis. TYPE: str DEFAULT: 'Time (Month Year)' x_grid If False, remove the grid for the x axis. TYPE: bool DEFAULT: True y_axis_title Label name for the y axis. TYPE: str DEFAULT: 'Completeness predictor c(t)' y_grid If False, remove the grid for the y axis. TYPE: bool DEFAULT: True show_n_visit If True, compute the sum of the number of visit instead of the mean of the completeness predictor \\(c(t)\\) . TYPE: bool DEFAULT: False show_per_care_site If True, the average completeness predictor \\(c(t)\\) is computed for each care site independently. If False, it is computed over all care sites. TYPE: bool DEFAULT: True Source code in edsteva/viz/plots/plot_probe/wrapper.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def plot_probe ( probe : BaseProbe , fitted_model : BaseModel = None , care_site_level : str = None , stay_type : List [ str ] = None , care_site_id : List [ int ] = None , start_date : datetime = None , end_date : datetime = None , care_site_short_name : List [ int ] = None , save_path : str = None , x_axis_title : str = \"Time (Month Year)\" , x_grid : bool = True , y_axis_title : str = \"Completeness predictor c(t)\" , y_grid : bool = True , show_n_visit : bool = False , show_per_care_site : bool = True , ** kwargs , ): r \"\"\" Displays a chart with the average completeness predictor $c(t)$ over time $t$ with the fitted model $\\hat{c}(t)$ if specified. The chart is exportable in png or svg format and easy to integrate into a report. Is also possible to save the chart in HTML with the \"save_path\" optional input. Parameters ---------- probe : BaseProbe Class describing the completeness predictor $c(t)$. fitted_model : BaseModel, optional Model fitted to the probe care_site_level : str, optional **EXAMPLE**: `\"Hospital\"`, `\"H\u00f4pital\"` or `\"UF\"` stay_type : List[str], optional **EXAMPLE**: `\"All\"` or `[\"All\", \"Urg\"]` care_site_id : List[int], optional **EXAMPLE**: `[8312056386, 8312027648]` start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2021-07-01\"` care_site_short_name : List[int], optional **EXAMPLE**: `\"HOSPITAL XXXX\"` save_path : str, optional Folder path where to save the chart in HTML format. **EXAMPLE**: `\"my_folder/my_file.html\"` x_axis_title: str, optional, Label name for the x axis. x_grid: bool, optional, If False, remove the grid for the x axis. y_axis_title: str, optional, Label name for the y axis. y_grid: bool, optional, If False, remove the grid for the y axis. show_n_visit: bool, optional If True, compute the sum of the number of visit instead of the mean of the completeness predictor $c(t)$. show_per_care_site: bool, optional If True, the average completeness predictor $c(t)$ is computed for each care site independently. If False, it is computed over all care sites. \"\"\" index = list ( set ( probe . _index ) . difference ([ \"care_site_level\" , \"care_site_id\" ])) if show_per_care_site : index = index + [ \"care_site_short_name\" ] if fitted_model : predictor = fitted_model . predict ( probe ) else : predictor = probe . predictor predictor = filter_predictor ( predictor = predictor , care_site_level = care_site_level , stay_type = stay_type , care_site_id = care_site_id , care_site_short_name = care_site_short_name , start_date = start_date , end_date = end_date , ** kwargs , ) index = [ variable for variable in index if len ( predictor [ variable ] . unique ()) >= 2 ] alt . data_transformers . disable_max_rows () if fitted_model : chart = fitted_probe_line ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , ) else : chart = probe_line ( predictor = predictor , index = index , x_axis_title = x_axis_title , y_axis_title = y_axis_title , x_grid = x_grid , y_grid = y_grid , show_n_visit = show_n_visit , ) if save_path : save_html ( obj = chart . configure_axis ( labelFontSize = 11 , titleFontSize = 12 ) . configure_legend ( labelFontSize = 11 ), filename = save_path , ) return chart","title":"plot_probe()"}]}